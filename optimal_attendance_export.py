#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€é©å‹¤æ€ ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›æ©Ÿèƒ½
jinjerå½¢å¼CSVï¼ˆ194åˆ—ï¼‰ã‚’å‡ºåŠ›ã™ã‚‹
"""

import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import io
from typing import List, Dict, Any, Optional, Tuple
import calendar
import re
from src import normalize_name, parse_date_any, parse_minute_of_day
import os
import glob
from pathlib import Path
import base64
import gzip


def find_default_attendance_csv() -> Optional[Path]:
    """ãƒªãƒã‚¸ãƒˆãƒª/ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã‹ã‚‰å‹¤æ€ å±¥æ­´.csvã‚’æ¢ç´¢"""
    candidates = [
        Path(__file__).resolve().parent / "input" / "å‹¤æ€ å±¥æ­´.csv",
        Path.cwd() / "input" / "å‹¤æ€ å±¥æ­´.csv",
    ]
    seen = set()
    for candidate in candidates:
        try:
            resolved = candidate.resolve()
        except FileNotFoundError:
            resolved = candidate
        if resolved in seen:
            continue
        seen.add(resolved)
        if resolved.exists():
            return resolved
    return None


BUILTIN_ATTENDANCE_CSV_BASE64 = (
    "H4sIAHPL5WgC/+1dzYskSRW/C/4Pc9ShYCNeRH71yWXX0VmG7WFFFGZ1BRHdRR2WERkRhLYQd6q6q7q6"
    "e7rmsDMHvYjigI4eFgZcQVcUGT9Qwcsggst4EA8yl2UPVr78isiu6oqsyOqezvn14fEiMjvzkfmriBe/"
    "fO/F4f3JZu/8+DPDd954fPH53vmDF7dH++/1zk9+vTPub/aHW1f6X5/1jweTu0Xz8H6v/+n+5f6n+t+Z"
    "NYe70+3PpWdcHT6Yvrz3aHRnZ9ybPDQaw+vbXygasnfzzeGzVcs8RtYxso4p65iyjmnrmLaOBdaxwDJ8"
    "sGmeWTtk/tvsivvvbb0qNg4/svco1eXGeJBpNOsbXv/Q7icnfxteH1/9sJodMds6Oz66M/hLertZT5Cd"
    "YfRsvcYPT/JTk6wT68S6Yl2xrlnXrAesB6yHrIesR6xHrMesx6wnrCfZvUR2M8EPK3sp2cvIXkL28LOH"
    "nj3s7CFnDzd7qAHrIesh6xHrEesx6zHrCetJdi+R3Uz0dj8+OpSDF0ZfzbTxxelbrFHZR9w32Bxflb2d"
    "X+2/L1kn1ol1xbpiXbOuWQ9YD1gPWQ9Zj1iPWI9Zj1lPWE+ye4nsZoJf+/D6jd8e3pemvnVh9J+bv7N7"
    "UpvTV2r3pvYf7Z3enr5k9JCpH7k2zb02zb025ddOf7ujO/2X+lv9/f4Xe4PfTG+P/nzweOfLveGDw5+M"
    "/zf8OYP5e+l5cmP7u8MHMwjuXZ/907vjX47fmN7+9u+nlw7v8Un35Mb00uzwZGvy8JjD6X+nHdNLctaZ"
    "n2917N3fuSZnMgVaqhPrxLpiXbGuWdesB6wHrIesh6xHrEesx6zHrCesJ9m9RHYzwY9j+O7NH158fkOa"
    "DTIbymxosxGYjdBsRGYjNhuJdVPbBMsGaRkhLSukZYa07JCWIdKyRFqmSMsWsmwh+3lYtpBlC1m2kGUL"
    "WbaQZQtZtpBli7JsUZYtyn45li3KskVZtijLFmXZoixblGWLtmzRli3askXbSLFs0ZYt2rJFW7ZoyxZt"
    "2RKIXvbb7KU/Tf59cyv9kbMyenv3T4Ov7D1Kf8m9yc6tF/Z/zCPB6O1STYdaViY7o88P32E1nb32/1E0"
    "t141J7zRT2c/0foJxdXKg4PN8mK7Vw7/zcrNa5MfFSfuHsy5av0E66q7L+/+Ynzv4MLOHwxDq07botoB"
    "w5p0Trv1g+n3M7vTVjpvb13Zf3/wd9Ze4/muOmXyMD9wJe3PT1l0lcvPDV/cvzv3Opef2/vn6Fv1K33w"
    "A5N/Db95bueV0eu9a2KGXhIUPEPPyN5sCjsY3b3x39Efv3T1a736n9wQokfpDyNIhUzSdot/6f0vPHvp"
    "Ex+dI+t/i86DbCplkL7GTIpKjUsRlUcWitnfIlDRElBlUFIsMz1kI/jWLUPsOIAtBhsg0pZM0vfLglHD"
    "WoUzWR5YBWdqCc7CEsp8W5lBPQLKOid5LFElmKqmWg6wZSjTLqNZxABjTJPCHNkBGdnzYGQLP0gFSyAl"
    "UjRVXlcxL6rWwYVx63RlNRMK2wGL/UEWNgPZ+kYwgOy0XTD2gETpggnbBfMBWQSQQS4CWVC+d+Xl58er"
    "gIyXFpKqhSXJFgHnCrdF0ANg2vPTilee+2kW0iIv4CVOREbbnNhRdPXg56+PC1MVG5URBMrmKAIvCEnh"
    "TFKshZnA8vG0GAnmIUTJSFjCz7OX0pmSKIgvuF1YQDaGGYGmgFw7yhSWkJDr5imkBsogFxMVhdB+KAtW"
    "QVmx6ii/w+cfxdtmLVbhLOYjEihqbfUpKgc9w4Gwl5v1dlNEhqAwQGF4UhjRSnEWiObpnqRy/hJlYGCt"
    "b3WYxUthhkCLDkoePcISIaEtPDGVgMKAXDeFQQKLS8h1UxgkgTLIRSirPoP7xVQTrR5sAV+/U6tKWb3a"
    "bEVXQ5b0RJpyoiZ0niCC7KOOA6uaJ/2ixUg3oCtKzgIjGLyxRigLkBaCtJB200IoBFsBuXa2AjH7kOuf"
    "H2OgDHIRymQpYkeUTW9tPz43fXPwsd43hCDX8hVxFgMrOBOkKjoQ585+OXES9yM3BEHYztAjl6+VBvRI"
    "wP/vRsCOKEIcDLWaNskLVcoFVWUhi3IsA6zOuGSWU5e8V9XUTQIr5mNKu9R4Yv7LGLN4OdvymAVPrAuR"
    "FvNB5kSH1SLDMHABU8dhKnSZDONq4QhIoZjTEkgt5774IgUrkc2F4Rr8d8yFT17ZgFrfyiCLm3wawndt"
    "BEcvh1TiMhW2TjEgjeNMpnEs4ErF0nWgyumLPNK+JBkw98HBckWZXIVtgIfVxZVh8Y5FOabU+lZHmVOJ"
    "6cSgSbE07IKsKsoJu8Bc0AKmFDAFTLWMKe3+lTDCV8IOYaooKCHs+hKBPysq3ah2qhwrDFSd3UqhotqV"
    "3+LQiWtHnYizTTBQPjKUBAPZBIP2w1DklsCP2IWOwcrOOqyasgUHKl45eAHJPHCpXFGWNIheQJVecFRO"
    "AaLCPXwhQvgCQvlcMCXd+YQYfEKHMGXVptS28MQUOU5+CnwCkgxdQaUapkdgLdgRTBU+TY4pZWPKh1+g"
    "BmS6PLLFbLv4wmqwu+F7S0s7rJlpwNjVQcc9bMAwBHCyMCG6gCpqRFsBVKCtHEDlXoYZHwLP7Bcbq76H"
    "tIVzeZmtV84N/5rBR7tWZMgTSxU+JKNk9zz8OJUftYPw4HujrMLxoHIko9Sa8t4Bqs4NUw2oqBAUAWKF"
    "lyMqaIIoTHwIa1mKqNB9P50i8zBY03gFuvy0qU0SmcipTRL1vlVRtoyDojIXOqh/8gPGnoKtDE0XXq0O"
    "s7hJ4AuCqboc+KI2ZC5EOmGtiqjEpXZoDmrKS4UCUp0os1Dut2tsvWuWWfDhP4U7gRVjZdjp5CyzlrbH"
    "1Cele14NyAaU6XCBFDWoMwu6AZSoC6YahHwe4RtQ+6VjMAtEJnKYBaLetzLMtHt1PdBaXSdPSWQiJ09J"
    "1PtWhlnTIFCsDju7WwmlaGIRbmgPSDlt72VmmQJT3WAcwmptJiq1UcLNMbCKXOo6RlaZbLjy2InwOEjF"
    "TRkHZG9hv6TjIZW4QyoGpBBq7BAqKjwIB+QDYiXoCDPpXhcUMOs6r8WLQCp5LRL1vpVh5kbJyzxDEKtD"
    "fI92gpVqklABVHUGVUE1UIhKrVAVeaFKO1FZuqoZg5SKbsAqyrNkyiC/yI50SHwiHShAWgXSKlpNq6Cw"
    "CZWFQAdQWcshFSHQAXLtgQ4U+wQ68IIUMOvM1oJaZCLfWlCLep8jzKYXbryVwixwLdgQlp/B8fUQFUVd"
    "EEVLM8JUPg0bIxhVzEde/ypcSzKP60i2HJHA0JPn/h/Bolqa5JO6hCn4UJDmLM+POn+DpWK3jEyM0AdO"
    "GnACnNqDU+Die0XwvTrnzVsFjcgWXvNd6O7No3Qtqog4ICpCPivyWX3yWY8gKm4465EEojpbyqFKZ/VA"
    "VOIeoSWtoOX2C4WAKe1QIcmjZKlwRxq+JiJt2gVSDcJL4WBhtwkXSBG+6cDDavWbjkNxBxALIBYaQUpj"
    "lMIo1e4oFeD7DL7PtPd9RobuIVgnFcCwWggDcHjWZscITAOYhnaZhhizI2bHFmfHBOELCF9oNXzBoaRD"
    "CSnQDMjWcYFUgwB3MpIpYtSLhL/eAGbkPnKZS0XADKNZE5ippowEQh26WwCweNV5AUAl6n0rw0y7sw8B"
    "/DBAygFSgTuksAkr9qp3gVQIAgIERLsERNRktQhIIWzUCVVg3sG8r8a8Hwy3f3Zu97PT26PXU0hFrmUb"
    "RFXW6GgNGgW6oVsFJIPiBRcFJAO7gGTghTVywRplPl0J8BhIeyo2RzGTwvyGNOUMM31M5aNWIYdQmydm"
    "kOP9dzIpcvebTPQ57GR+LPp0M/TNK+/W9h7BQN9TM8UGTuhruzQzVgcnujpQ1fwkKtWYQAPPGTR0XxRE"
    "2NG8S3NjuUm9qAIZzLlReeGqaQ0I4KobuOKMDFnlZchaiobUngNWDBYD8lgXS5jbYvj5WAloDMj10xhL"
    "a0SAxwCPsT4eQ0p3Z40M0K1j8x8MdKe+8TAVteBFVRbe3HjYE2zUYEtrwpKzO1yGyDd1KhaWmWrMouS5"
    "5pTKfW2ARWenJsgyuEZUqjlBevpnTkx/DGA9HfssioXt5sgKQGdAnhSdIUPwGZAnwGdE4DMgT4/PiBGY"
    "AXlqgRkyacBwzBv0wKd1avdjErksdj8me/djv8GOhDt5G4JO69Ju7aqoGyeqEnLmbu1+bBrJpkRtvlxe"
    "R5wQhrFT/yxgp6DUs1G8s1OIwIRAnhQTQgpMCOT6mRDSYEIgT40JISQJIEnAP0mAwkZLAQFGo8uDVsyv"
    "Nq6cs9jmz2Y48HTOGiQPRPjc3iVKwx6tMtWkNDzHsRhxHIjjWD2O4/+imNjiaFQBAA=="
)


def _read_attendance_csv(path: Path) -> Optional[pd.DataFrame]:
    """è¤‡æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã—ã¦å‹¤æ€ CSVã‚’èª­ã¿è¾¼ã‚€"""
    encodings = ['utf-8-sig', 'cp932', 'utf-8', 'shift_jis']
    for encoding in encodings:
        try:
            return pd.read_csv(path, encoding=encoding)
        except UnicodeDecodeError:
            continue
    return None


def build_builtin_attendance_dataframe() -> pd.DataFrame:
    """çµ„ã¿è¾¼ã¿ã®å‹¤æ€ ãƒ‡ãƒ¼ã‚¿CSVã‚’èª­ã¿è¾¼ã‚“ã§DataFrameã‚’è¿”ã™"""
    raw_bytes = gzip.decompress(base64.b64decode(BUILTIN_ATTENDANCE_CSV_BASE64))
    buffer = io.BytesIO(raw_bytes)
    return pd.read_csv(buffer, encoding='cp932')


def get_builtin_attendance_csv_bytes(encoding: str = 'shift_jis') -> bytes:
    """çµ„ã¿è¾¼ã¿å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡å®šã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã®CSVãƒã‚¤ãƒˆåˆ—ã¨ã—ã¦å–å¾—"""
    raw_bytes = gzip.decompress(base64.b64decode(BUILTIN_ATTENDANCE_CSV_BASE64))
    if encoding.lower() in ('cp932', 'shift_jis', 'shift-jis'):  # å…ƒãƒ‡ãƒ¼ã‚¿ã¯CP932
        return raw_bytes
    df = pd.read_csv(io.BytesIO(raw_bytes), encoding='cp932')
    buffer = io.StringIO()
    df.to_csv(buffer, index=False)
    return buffer.getvalue().encode(encoding, errors='ignore')

def create_jinjer_headers() -> List[str]:
    """jinjerå½¢å¼CSVã®ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆ194åˆ—ï¼‰ã‚’ç”Ÿæˆ"""
    headers = []
    
    # åŸºæœ¬æƒ…å ±ï¼ˆ5åˆ—ï¼‰
    headers.extend([
        'åå‰', '*å¾“æ¥­å“¡ID', '*å¹´æœˆæ—¥', '*æ‰“åˆ»ã‚°ãƒ«ãƒ¼ãƒ—ID', 'æ‰€å±ã‚°ãƒ«ãƒ¼ãƒ—å'
    ])
    
    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æƒ…å ±ï¼ˆ15åˆ—ï¼‰
    headers.extend([
        'ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«é››å½¢ID', 'å‡ºå‹¤äºˆå®šæ™‚åˆ»', 'é€€å‹¤äºˆå®šæ™‚åˆ»',
        'ä¼‘æ†©äºˆå®šæ™‚åˆ»1', 'å¾©å¸°äºˆå®šæ™‚åˆ»1', 'ä¼‘æ†©äºˆå®šæ™‚åˆ»2', 'å¾©å¸°äºˆå®šæ™‚åˆ»2',
        'ä¼‘æ†©äºˆå®šæ™‚åˆ»3', 'å¾©å¸°äºˆå®šæ™‚åˆ»3', 'ä¼‘æ†©äºˆå®šæ™‚åˆ»4', 'å¾©å¸°äºˆå®šæ™‚åˆ»4',
        'ä¼‘æ†©äºˆå®šæ™‚åˆ»5', 'å¾©å¸°äºˆå®šæ™‚åˆ»5',
        'ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å¤–ä¼‘æ†©äºˆå®šæ™‚åˆ»', 'ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å¤–å¾©å¸°äºˆå®šæ™‚åˆ»'
    ])
    
    # ä¼‘æ—¥è¨­å®šï¼ˆ1åˆ—ï¼‰
    headers.extend([
        'ä¼‘æ—¥ï¼ˆ0:æ³•å®šä¼‘æ—¥1:æ‰€å®šä¼‘æ—¥2:æ³•ä¼‘(æŒ¯æ›¿ä¼‘å‡º)3:æ‰€ä¼‘(æŒ¯æ›¿ä¼‘å‡º)4:æ³•ä¼‘(æ™‚é–“å¤–ä¼‘å‡º)5:æ‰€ä¼‘(æ™‚é–“å¤–ä¼‘å‡º)ï¼‰'
    ])
    
    # å®Ÿéš›ã®å‡ºé€€å‹¤æ™‚åˆ»ï¼ˆ20åˆ—ï¼‰- æœ€å¤§10ã‚·ãƒ•ãƒˆå¯¾å¿œ
    headers.extend([
        'å‡ºå‹¤1', 'é€€å‹¤1', 'å‡ºå‹¤2', 'é€€å‹¤2', 'å‡ºå‹¤3', 'é€€å‹¤3', 'å‡ºå‹¤4', 'é€€å‹¤4', 'å‡ºå‹¤5', 'é€€å‹¤5',
        'å‡ºå‹¤6', 'é€€å‹¤6', 'å‡ºå‹¤7', 'é€€å‹¤7', 'å‡ºå‹¤8', 'é€€å‹¤8', 'å‡ºå‹¤9', 'é€€å‹¤9', 'å‡ºå‹¤10', 'é€€å‹¤10'
    ])
    
    # å®Ÿéš›ã®ä¼‘æ†©æ™‚åˆ»ï¼ˆ20åˆ—ï¼‰- æœ€å¤§10å›ä¼‘æ†©å¯¾å¿œ
    headers.extend([
        'ä¼‘æ†©1', 'å¾©å¸°1', 'ä¼‘æ†©2', 'å¾©å¸°2', 'ä¼‘æ†©3', 'å¾©å¸°3', 'ä¼‘æ†©4', 'å¾©å¸°4', 'ä¼‘æ†©5', 'å¾©å¸°5',
        'ä¼‘æ†©6', 'å¾©å¸°6', 'ä¼‘æ†©7', 'å¾©å¸°7', 'ä¼‘æ†©8', 'å¾©å¸°8', 'ä¼‘æ†©9', 'å¾©å¸°9', 'ä¼‘æ†©10', 'å¾©å¸°10'
    ])
    
    # é£Ÿäº‹æ™‚é–“ï¼ˆ4åˆ—ï¼‰
    headers.extend([
        'é£Ÿäº‹1é–‹å§‹', 'é£Ÿäº‹1çµ‚äº†', 'é£Ÿäº‹2é–‹å§‹', 'é£Ÿäº‹2çµ‚äº†'
    ])
    
    # å¤–å‡ºãƒ»å†å…¥ï¼ˆ20åˆ—ï¼‰- æœ€å¤§10å›å¤–å‡ºå¯¾å¿œ
    headers.extend([
        'å¤–å‡º1', 'å†å…¥1', 'å¤–å‡º2', 'å†å…¥2', 'å¤–å‡º3', 'å†å…¥3', 'å¤–å‡º4', 'å†å…¥4', 'å¤–å‡º5', 'å†å…¥5',
        'å¤–å‡º6', 'å†å…¥6', 'å¤–å‡º7', 'å†å…¥7', 'å¤–å‡º8', 'å†å…¥8', 'å¤–å‡º9', 'å†å…¥9', 'å¤–å‡º10', 'å†å…¥10'
    ])
    
    # ä¼‘æ—¥ä¼‘æš‡ï¼ˆ10åˆ—ï¼‰
    headers.extend([
        'ä¼‘æ—¥ä¼‘æš‡å1', 'ä¼‘æ—¥ä¼‘æš‡å1ï¼šç¨®åˆ¥', 'ä¼‘æ—¥ä¼‘æš‡å1ï¼šé–‹å§‹æ™‚é–“', 'ä¼‘æ—¥ä¼‘æš‡å1ï¼šçµ‚äº†æ™‚é–“', 'ä¼‘æ—¥ä¼‘æš‡å1ï¼šç†ç”±',
        'ä¼‘æ—¥ä¼‘æš‡å2', 'ä¼‘æ—¥ä¼‘æš‡å2ï¼šç¨®åˆ¥', 'ä¼‘æ—¥ä¼‘æš‡å2ï¼šé–‹å§‹æ™‚é–“', 'ä¼‘æ—¥ä¼‘æš‡å2ï¼šçµ‚äº†æ™‚é–“', 'ä¼‘æ—¥ä¼‘æš‡å2ï¼šç†ç”±'
    ])
    
    # ç®¡ç†æƒ…å ±ï¼ˆ7åˆ—ï¼‰
    headers.extend([
        'æ‰“åˆ»æ™‚ã‚³ãƒ¡ãƒ³ãƒˆ', 'ç®¡ç†è€…å‚™è€ƒ',
        'å‹¤å‹™çŠ¶æ³ï¼ˆ0:æœªæ‰“åˆ»1:æ¬ å‹¤ï¼‰', 'é…åˆ»å–æ¶ˆå‡¦ç†ã®æœ‰ç„¡ï¼ˆ0:ç„¡1:æœ‰ï¼‰', 'æ—©é€€å–æ¶ˆå‡¦ç†ã®æœ‰ç„¡ï¼ˆ0:ç„¡1:æœ‰ï¼‰',
        'é…åˆ»ï¼ˆ0:æœ‰1:ç„¡ï¼‰', 'æ—©é€€ï¼ˆ0:æœ‰1:ç„¡ï¼‰'
    ])
    
    # ç›´è¡Œãƒ»ç›´å¸°ï¼ˆ20åˆ—ï¼‰- æœ€å¤§10ã‚·ãƒ•ãƒˆå¯¾å¿œ
    headers.extend([
        'ç›´è¡Œ1', 'ç›´å¸°1', 'ç›´è¡Œ2', 'ç›´å¸°2', 'ç›´è¡Œ3', 'ç›´å¸°3', 'ç›´è¡Œ4', 'ç›´å¸°4', 'ç›´è¡Œ5', 'ç›´å¸°5',
        'ç›´è¡Œ6', 'ç›´å¸°6', 'ç›´è¡Œ7', 'ç›´å¸°7', 'ç›´è¡Œ8', 'ç›´å¸°8', 'ç›´è¡Œ9', 'ç›´å¸°9', 'ç›´è¡Œ10', 'ç›´å¸°10'
    ])
    
    # æ‰“åˆ»åŒºåˆ†IDï¼ˆ50åˆ—ï¼‰
    for i in range(1, 51):
        headers.append(f'æ‰“åˆ»åŒºåˆ†ID:{i}')
    
    # å‹¤å‹™çŠ¶æ³ãƒ•ãƒ©ã‚°ï¼ˆ5åˆ—ï¼‰
    headers.extend(['æœªæ‰“åˆ»', 'æ¬ å‹¤', 'ä¼‘æ—¥æ‰“åˆ»', 'ä¼‘æš‡æ‰“åˆ»', 'å®Ÿç¸¾ç¢ºå®šçŠ¶æ³'])
    
    # åŠ´åƒæ™‚é–“è¨ˆç®—ï¼ˆ13åˆ—ï¼‰
    headers.extend([
        'ç·åŠ´åƒæ™‚é–“', 'å®ŸåŠ´åƒæ™‚é–“', 'ä¼‘æ†©æ™‚é–“', 'ç·æ®‹æ¥­æ™‚é–“',
        'æ³•å®šå†…æ®‹æ¥­æ™‚é–“ï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è»¸ï¼‰', 'æ³•å®šå†…æ®‹æ¥­æ™‚é–“ï¼ˆåŠ´åƒæ™‚é–“è»¸ï¼‰', 'æ³•å®šå¤–æ®‹æ¥­æ™‚é–“', 'æ·±å¤œæ™‚é–“',
        'ä¸è¶³åŠ´åƒæ™‚é–“æ•°ï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è»¸ï¼‰', 'ä¸è¶³åŠ´åƒæ™‚é–“æ•°ï¼ˆåŠ´åƒæ™‚é–“è»¸ï¼‰',
        'ç”³è«‹æ‰¿èªæ¸ˆç·æ®‹æ¥­æ™‚é–“', 'ç”³è«‹æ‰¿èªæ¸ˆæ³•å®šå†…æ®‹æ¥­æ™‚é–“', 'ç”³è«‹æ‰¿èªæ¸ˆæ³•å®šå¤–æ®‹æ¥­æ™‚é–“'
    ])
    
    # ä¹–é›¢æ™‚é–“ï¼ˆ4åˆ—ï¼‰
    headers.extend([
        'å‡ºå‹¤ä¹–é›¢æ™‚é–“ï¼ˆå‡ºå‹¤æ™‚åˆ»ãƒ¼å…¥é¤¨æ™‚åˆ»ï¼‰', 'é€€å‹¤ä¹–é›¢æ™‚é–“ï¼ˆé€€é¤¨æ™‚åˆ»ãƒ¼é€€å‹¤æ™‚åˆ»ï¼‰',
        'å‡ºå‹¤ä¹–é›¢æ™‚é–“ï¼ˆå‡ºå‹¤æ™‚åˆ»ãƒ¼PCèµ·å‹•æ™‚åˆ»ï¼‰', 'é€€å‹¤ä¹–é›¢æ™‚é–“ï¼ˆPCåœæ­¢æ™‚åˆ»ãƒ¼é€€å‹¤æ™‚åˆ»ï¼‰'
    ])
    
    return headers


def dataframe_to_jinjer_csv_bytes(
    df: pd.DataFrame,
    encoding: str = 'shift_jis',
    column_order: Optional[List[str]] = None
) -> bytes:
    """
    DataFrameã‚’jinjerå½¢å¼ã®ãƒ˜ãƒƒãƒ€ãƒ¼é †ã«ä¸¦ã¹æ›¿ãˆã¦CSVãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›ã™ã‚‹ã€‚
    æŒ‡å®šãƒ˜ãƒƒãƒ€ãƒ¼ã«å«ã¾ã‚Œãªã„åˆ—ã¯å‰Šé™¤ã—ã€æ¬ æã¯ç©ºæ–‡å­—ã§åŸ‹ã‚ã‚‹ã€‚
    
    Args:
        df: CSVã«å¤‰æ›ã™ã‚‹DataFrameã€‚
        encoding: å‡ºåŠ›æ™‚ã«ä½¿ç”¨ã™ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€‚
        column_order: åˆ—é †ã‚’å›ºå®šã—ãŸã„å ´åˆã«ä½¿ç”¨ã™ã‚‹åˆ—åãƒªã‚¹ãƒˆã€‚
                      æœªæŒ‡å®šã®å ´åˆã¯jinjeræ¨™æº–ãƒ˜ãƒƒãƒ€ãƒ¼é †ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
    """
    normalized_df = df.copy()
    # å‡ºåŠ›å¯¾è±¡å¤–ã®åˆ—ã‚’é™¤å¤–
    drop_targets = [col for col in EXCLUDED_OUTPUT_COLUMNS if col in normalized_df.columns]
    if drop_targets:
        normalized_df = normalized_df.drop(columns=drop_targets)
    
    headers = column_order or create_jinjer_headers()
    headers = [col for col in headers if col not in EXCLUDED_OUTPUT_COLUMNS]
    normalized_df = normalized_df.reindex(columns=headers, fill_value='')
    
    # æŒ‡å®šã®åˆ—ã¯å¼·åˆ¶çš„ã«ãƒ–ãƒ©ãƒ³ã‚¯ã«ã™ã‚‹
    for col in FORCED_EMPTY_COLUMNS:
        if col in normalized_df.columns:
            normalized_df[col] = ''
    
    normalized_df = normalized_df.fillna('')
    
    buffer = io.StringIO()
    normalized_df.to_csv(buffer, index=False)
    return buffer.getvalue().encode(encoding, errors='ignore')

def time_to_minutes(time_str: str, is_end_time: bool = False) -> int:
    """æ™‚é–“ã‚’åˆ†ã«å¤‰æ›ï¼ˆ24æ™‚é–“å¯¾å¿œï¼‰"""
    if not time_str or time_str == '':
        return 0
    
    if time_str == '24:00' or (time_str == '0:00' and is_end_time):
        return 24 * 60  # 1440åˆ†
    
    try:
        parts = time_str.split(':')
        hours = int(parts[0])
        minutes = int(parts[1]) if len(parts) > 1 else 0
        return hours * 60 + minutes
    except:
        return 0

def minutes_to_time(minutes: int) -> str:
    """åˆ†ã‚’æ™‚:åˆ†å½¢å¼ã«å¤‰æ›"""
    if minutes >= 24 * 60:
        return '24:00'
    hours = minutes // 60
    mins = minutes % 60
    return f"{hours}:{mins:02d}"

def format_time_for_csv(time_str: str) -> str:
    """CSVå‡ºåŠ›ç”¨ã®æ™‚é–“ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if not time_str or time_str == '':
        return ''
    return time_str

# ä¼‘æ†©æ™‚é–“ã®åˆ—ãƒšã‚¢ï¼ˆå®Ÿç¸¾å´10æ ï¼‰
BREAK_COLUMN_PAIRS: List[Tuple[str, str]] = [(f"ä¼‘æ†©{i}", f"å¾©å¸°{i}") for i in range(1, 11)]
FULL_WIDTH_DIGIT_MAP = str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™", "0123456789")
COLUMN_REMOVE_CHARS = [' ', 'ã€€', '"', "'", 'â€œ', 'â€']
EXCLUDED_OUTPUT_COLUMNS = frozenset()
HOLIDAY_COLUMNS = [
    'ä¼‘æ—¥ä¼‘æš‡å1', 'ä¼‘æ—¥ä¼‘æš‡å1ï¼šç¨®åˆ¥', 'ä¼‘æ—¥ä¼‘æš‡å1ï¼šé–‹å§‹æ™‚é–“', 'ä¼‘æ—¥ä¼‘æš‡å1ï¼šçµ‚äº†æ™‚é–“', 'ä¼‘æ—¥ä¼‘æš‡å1ï¼šç†ç”±',
    'ä¼‘æ—¥ä¼‘æš‡å2', 'ä¼‘æ—¥ä¼‘æš‡å2ï¼šç¨®åˆ¥', 'ä¼‘æ—¥ä¼‘æš‡å2ï¼šé–‹å§‹æ™‚é–“', 'ä¼‘æ—¥ä¼‘æš‡å2ï¼šçµ‚äº†æ™‚é–“', 'ä¼‘æ—¥ä¼‘æš‡å2ï¼šç†ç”±'
]
FORCED_EMPTY_COLUMNS = tuple(
    ['ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«é››å½¢ID']
    + HOLIDAY_COLUMNS
    + [f'æ‰“åˆ»åŒºåˆ†ID:{i}' for i in range(1, 51)]
    + [f'ç›´è¡Œ{i}' for i in range(1, 11)]
    + [f'ç›´å¸°{i}' for i in range(1, 11)]
)
FORCED_EMPTY_SET = set(FORCED_EMPTY_COLUMNS)


def build_forced_empty_indices(headers: List[str]) -> List[int]:
    """ãƒ˜ãƒƒãƒ€ãƒ¼ãƒªã‚¹ãƒˆã‹ã‚‰å¼·åˆ¶ãƒ–ãƒ©ãƒ³ã‚¯å¯¾è±¡åˆ—ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§ã‚’å–å¾—"""
    return [idx for idx, name in enumerate(headers) if name in FORCED_EMPTY_SET]


def enforce_forced_empty_fields(row: List[Any], forced_indices: List[int]) -> None:
    """æŒ‡å®šã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å€¤ã‚’å¼·åˆ¶çš„ã«ãƒ–ãƒ©ãƒ³ã‚¯ã¸æ›´æ–°"""
    for idx in forced_indices:
        if idx < len(row):
            row[idx] = ''


def normalize_column_name(name: Any) -> str:
    """åˆ—åã‚’æ¯”è¼ƒç”¨ã«æ­£è¦åŒ–ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ãƒ»å¼•ç”¨ç¬¦é™¤å»ã€å…¨è§’æ•°å­—â†’åŠè§’ï¼‰"""
    if name is None:
        return ''
    normalized = str(name)
    for ch in COLUMN_REMOVE_CHARS:
        normalized = normalized.replace(ch, '')
    normalized = normalized.translate(FULL_WIDTH_DIGIT_MAP)
    return normalized


def normalize_break_header(header: str) -> str:
    """ä¼‘æ†©/å¾©å¸°ã‚«ãƒ©ãƒ åã®ã‚¹ãƒšãƒ¼ã‚¹ãƒ»å…¨è§’æ•°å­—ã‚’æ­£è¦åŒ–"""
    if header is None:
        return ''
    return normalize_column_name(header)


def resolve_column(df: pd.DataFrame, target_name: str, fallback_suffix: str = '') -> Optional[str]:
    """æ­£è¦åŒ–ã—ãŸåˆ—åã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã‚’æ¢ç´¢"""
    normalized_map = {normalize_column_name(col): col for col in df.columns}
    normalized_target = normalize_column_name(target_name)
    if normalized_target in normalized_map:
        return normalized_map[normalized_target]
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ä¸€è‡´ãªã©ã§æ¢ç´¢
    if fallback_suffix:
        normalized_suffix = normalize_column_name(fallback_suffix)
        for norm, col in normalized_map.items():
            if norm.endswith(normalized_suffix):
                return col
    else:
        for norm, col in normalized_map.items():
            if normalized_target and normalized_target in norm:
                return col
    return None


def get_break_column_pairs(df: pd.DataFrame) -> List[Tuple[str, str]]:
    """DataFrameå†…ã®ä¼‘æ†©/å¾©å¸°ã‚«ãƒ©ãƒ ã‚’æ¤œå‡ºã—ã€å®Ÿéš›ã®åˆ—åãƒšã‚¢ã‚’è¿”ã™"""
    normalized_map: Dict[str, str] = {}
    for col in df.columns:
        normalized_map.setdefault(normalize_break_header(col), col)
    
    detected_pairs: List[Tuple[str, str]] = []
    for i in range(1, 11):
        start_key = f"ä¼‘æ†©{i}"
        end_key = f"å¾©å¸°{i}"
        start_col = normalized_map.get(start_key)
        end_col = normalized_map.get(end_key)
        if start_col and end_col:
            detected_pairs.append((start_col, end_col))
    
    if detected_pairs:
        return detected_pairs
    return BREAK_COLUMN_PAIRS.copy()


def extract_month_string(date_value: Any) -> str:
    """æ§˜ã€…ãªæ—¥ä»˜è¡¨è¨˜ã‹ã‚‰ 'YYYY-MM' å½¢å¼ã®æœˆæ–‡å­—åˆ—ã‚’å–å¾—"""
    if pd.isna(date_value):
        return ''
    date_str = str(date_value).strip()
    if not date_str:
        return ''
    
    # æ—¢ã«ISOå½¢å¼ã§ã‚ã‚Œã°å…ˆé ­7æ–‡å­—ã‚’ä½¿ç”¨
    if re.match(r'^\d{4}-\d{2}-\d{2}$', date_str):
        return date_str[:7]
    
    # YYYY-M-D ã®ã‚ˆã†ãªå½¢å¼ã¯ã‚¼ãƒ­åŸ‹ã‚ã—ã¦å¯¾å¿œ
    iso_parts = re.match(r'^(\d{4})-(\d{1,2})-(\d{1,2})$', date_str)
    if iso_parts:
        year, month = iso_parts.group(1), int(iso_parts.group(2))
        return f"{year}-{month:02d}"
    
    # ã‚¹ãƒ©ãƒƒã‚·ãƒ¥åŒºåˆ‡ã‚Šãªã©ã‚‚è¨±å®¹
    slash_parts = re.match(r'^(\d{4})/(\d{1,2})/(\d{1,2})$', date_str)
    if slash_parts:
        year, month = int(slash_parts.group(1)), int(slash_parts.group(2))
        return f"{year:04d}-{month:02d}"
    
    # YYYYå¹´MMæœˆ ã®ã¿ãŒè¨˜è¼‰ã•ã‚ŒãŸã‚±ãƒ¼ã‚¹
    jp_month = re.match(r'^(\d{4})å¹´\s*(\d{1,2})æœˆ$', date_str)
    if jp_month:
        year, month = int(jp_month.group(1)), int(jp_month.group(2))
        if 1 <= month <= 12:
            return f"{year:04d}-{month:02d}"
    
    # YYYYMMæœˆ / YYYYMM ã¨ã„ã£ãŸåœ§ç¸®è¡¨è¨˜
    compact_month = re.match(r'^(\d{4})(\d{2})æœˆ?$', date_str)
    if compact_month:
        year, month = int(compact_month.group(1)), int(compact_month.group(2))
        if 1 <= month <= 12:
            return f"{year:04d}-{month:02d}"
    
    # YYYY/MM ã‚„ YYYY-MM ã®å ´åˆ
    compact_slash = re.match(r'^(\d{4})/(\d{1,2})$', date_str)
    if compact_slash:
        year, month = int(compact_slash.group(1)), int(compact_slash.group(2))
        if 1 <= month <= 12:
            return f"{year:04d}-{month:02d}"
    
    compact_dash = re.match(r'^(\d{4})-(\d{1,2})$', date_str)
    if compact_dash:
        year, month = int(compact_dash.group(1)), int(compact_dash.group(2))
        if 1 <= month <= 12:
            return f"{year:04d}-{month:02d}"
    
    try:
        parsed = parse_date_any(date_str)
        return parsed.strftime("%Y-%m")
    except Exception:
        pass
    
    try:
        parsed = pd.to_datetime(date_str, errors='coerce')
        if pd.notna(parsed):
            return f"{parsed.year:04d}-{parsed.month:02d}"
    except Exception:
        pass
    
    return ''


def build_employee_month_mask(
    df: pd.DataFrame,
    selected_employees: List[str],
    target_month: str
) -> pd.Series:
    """é¸æŠå¾“æ¥­å“¡ã¨å¯¾è±¡æœˆã«åˆè‡´ã™ã‚‹è¡Œã®ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ"""
    name_col = resolve_column(df, 'åå‰', fallback_suffix='åå‰')
    date_col = resolve_column(df, '*å¹´æœˆæ—¥', fallback_suffix='å¹´æœˆæ—¥')
    if not name_col or not date_col:
        return pd.Series([False] * len(df), index=df.index)
    
    normalized_names = df[name_col].astype(str).str.strip()
    normalized_months = df[date_col].apply(extract_month_string)
    employee_set = {normalize_column_name(emp).strip() for emp in selected_employees}
    normalized_employee_names = normalized_names.apply(normalize_column_name)
    
    return normalized_employee_names.isin(employee_set) & (normalized_months == target_month)


def build_month_mask(df: pd.DataFrame, target_month: str) -> pd.Series:
    """å¯¾è±¡æœˆã«åˆè‡´ã™ã‚‹è¡Œã®ã¿Trueã¨ãªã‚‹ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆ"""
    date_col = resolve_column(df, '*å¹´æœˆæ—¥', fallback_suffix='å¹´æœˆæ—¥')
    if not date_col:
        return pd.Series([True] * len(df), index=df.index)
    normalized_months = df[date_col].apply(extract_month_string)
    return normalized_months == target_month


def get_unique_employee_names(df: pd.DataFrame) -> List[str]:
    """å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¸€æ„ãªå¾“æ¥­å“¡åä¸€è¦§ã‚’å–å¾—"""
    name_col = resolve_column(df, 'åå‰', fallback_suffix='åå‰')
    if not name_col:
        return []
    names = df[name_col].astype(str).str.strip()
    non_empty = names[names != '']
    unique_names = pd.Series(non_empty).drop_duplicates().tolist()
    unique_names = [name for name in unique_names if name]
    return sorted(unique_names)


def minutes_to_extended_time(minutes: Optional[int]) -> str:
    """åˆ†ã‚’0åŸ‹ã‚ãªã—ã®æ™‚åˆ»æ–‡å­—åˆ—ã«å¤‰æ›ï¼ˆ24æ™‚è¶…ã‚‚ãã®ã¾ã¾ä¿æŒï¼‰"""
    if minutes is None:
        return ''
    hours, mins = divmod(minutes, 60)
    return f"{hours}:{mins:02d}"


def round_to_nearest_half_hour(minutes: int) -> int:
    """åˆ†å˜ä½ã®å€¤ã‚’30åˆ†åˆ»ã¿ã«å››æ¨äº”å…¥ï¼ˆ15åˆ†ä»¥ä¸Šã§åˆ‡ã‚Šä¸Šã’ï¼‰"""
    return ((minutes + 15) // 30) * 30


def auto_round_break_times(
    attendance_df: pd.DataFrame
) -> Tuple[pd.DataFrame, int, int]:
    """
    å‹¤æ€ CSVå…¨ä½“ã®ä¼‘æ†©æ™‚é–“ã‚’30åˆ†å˜ä½ã«è¿‘ã¥ã‘ã‚‹ã€‚
    
    - é–‹å§‹ãƒ»çµ‚äº†ã¨ã‚‚ã«30åˆ†å˜ä½ã§ã‚ã‚Œã°å¤‰æ›´ã—ãªã„
    - ãã‚Œä»¥å¤–ã¯ã€é–‹å§‹ã‚’30åˆ†å˜ä½ã«å››æ¨äº”å…¥ã—ã€å…ƒã®ä¼‘æ†©æ™‚é–“ï¼ˆåˆ†ï¼‰ã¯ç¶­æŒã—ãŸã¾ã¾çµ‚äº†ã‚’èª¿æ•´
    
    Returns:
        (è£œæ­£å¾ŒDataFrame, è£œæ­£ã•ã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰æ•°, è£œæ­£ã—ãŸä¼‘æ†©æ æ•°)
    """
    df = attendance_df.copy()
    break_pairs = get_break_column_pairs(df)
    
    if not break_pairs:
        return df, 0, 0
    
    updated_rows = 0
    updated_slots = 0
    
    for idx in df.index:
        row_modified = False
        for start_col, end_col in break_pairs:
            start_raw = df.at[idx, start_col] if start_col in df.columns else None
            end_raw = df.at[idx, end_col] if end_col in df.columns else None
            
            start_minutes = parse_minute_of_day(start_raw)
            end_minutes = parse_minute_of_day(end_raw)
            
            if start_minutes is None or end_minutes is None:
                continue
            if end_minutes <= start_minutes:
                continue
            
            duration = end_minutes - start_minutes
            if duration <= 0:
                continue
            
            if start_minutes % 30 == 0 and end_minutes % 30 == 0:
                continue  # æ—¢ã«30åˆ†å˜ä½
            
            rounded_start = max(0, round_to_nearest_half_hour(start_minutes))
            new_end_minutes = rounded_start + duration
            
            new_start = minutes_to_extended_time(rounded_start)
            new_end = minutes_to_extended_time(new_end_minutes)
            
            if new_start != str(start_raw) or new_end != str(end_raw):
                df.at[idx, start_col] = new_start
                df.at[idx, end_col] = new_end
                row_modified = True
                updated_slots += 1
        
        if row_modified:
            updated_rows += 1
    
    return df, updated_rows, updated_slots


def bulk_override_break_times(
    attendance_df: pd.DataFrame,
    selected_employees: List[str],
    target_month: str,
    new_start: str,
    new_end: str
) -> Tuple[pd.DataFrame, int, int]:
    """
    å¯¾è±¡å¾“æ¥­å“¡ãƒ»å¯¾è±¡æœˆã®ä¼‘æ†©æ™‚é–“ï¼ˆä¼‘æ†©1/å¾©å¸°1ã®ã¿ï¼‰ã‚’æŒ‡å®šæ™‚åˆ»ã«ä¸€æ‹¬ç½®æ›ã™ã‚‹ã€‚
    
    Returns:
        (ç½®æ›å¾ŒDataFrame, æ›´æ–°ã•ã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰æ•°, æ›´æ–°ã—ãŸä¼‘æ†©æ æ•°)
    """
    df = attendance_df.copy()
    mask = build_employee_month_mask(df, selected_employees, target_month)
    
    if not mask.any():
        return df, 0, 0
    
    break_pairs = get_break_column_pairs(df)
    updated_rows = 0
    updated_slots = 0
    
    for idx in df[mask].index:
        row_modified = False
        if not break_pairs:
            break
        start_col, end_col = break_pairs[0]
        if start_col not in df.columns or end_col not in df.columns:
            continue
        
        start_val = df.at[idx, start_col]
        end_val = df.at[idx, end_col]
        
        value_exists = False
        if isinstance(start_val, str) and start_val.strip():
            value_exists = True
        elif isinstance(end_val, str) and end_val.strip():
            value_exists = True
        elif not isinstance(start_val, str) and not pd.isna(start_val):
            value_exists = True
        elif not isinstance(end_val, str) and not pd.isna(end_val):
            value_exists = True
        
        if value_exists:
            df.at[idx, start_col] = new_start
            df.at[idx, end_col] = new_end
            row_modified = True
            updated_slots += 1
        
        if row_modified:
            updated_rows += 1
    
    return df, updated_rows, updated_slots

def merge_overlapping_shifts(shifts: List[Dict]) -> List[Dict]:
    """1æ™‚é–“åŠãƒ«ãƒ¼ãƒ«é©ç”¨ï¼šã‚·ãƒ•ãƒˆã‚’çµåˆã—ã¦æœ€é©ãªå‹¤å‹™æ™‚é–“ã‚’ç®—å‡º
    
    1æ™‚é–“åŠï¼ˆ90åˆ†ï¼‰æœªæº€ã®é–“éš”ã§åŒºåˆ‡ã‚‰ã‚ŒãŸã‚·ãƒ•ãƒˆã‚’çµåˆã—ã€
    æœ€å°å‡ºå‹¤å›æ•°ã‹ã¤æœ€å°å‡ºå‹¤æ™‚é–“ã‚’å®Ÿç¾ã™ã‚‹ã€‚
    
    ä¾‹: 0:00-0:30, 1:00-2:00, 4:00-5:00, 7:00-8:00, 8:00-9:00
    â†’ 0:00-5:00, 7:00-9:00 (2å›å‡ºå‹¤ã€5æ™‚é–“+2æ™‚é–“=7æ™‚é–“)
    â†’ 0:00-9:00 (1å›å‡ºå‹¤ã€9æ™‚é–“) ã‚ˆã‚Š2æ™‚é–“çŸ­ç¸®
    """
    if not shifts or len(shifts) <= 1:
        return shifts
    
    # æ™‚é–“é †ã«ã‚½ãƒ¼ãƒˆ
    sorted_shifts = sorted(shifts, key=lambda x: time_to_minutes(x.get('work_start', '0:00')))
    merged = []
    
    for shift in sorted_shifts:
        if not shift.get('work_start') or not shift.get('work_end'):
            continue
            
        current_start = time_to_minutes(shift['work_start'], False)
        current_end = time_to_minutes(shift['work_end'], True)
        
        # æœ€å¾Œã«è¿½åŠ ã•ã‚ŒãŸã‚·ãƒ•ãƒˆã¨é‡è¤‡ãƒ»é€£ç¶šãƒã‚§ãƒƒã‚¯
        if merged:
            last_shift = merged[-1]
            last_end = time_to_minutes(last_shift['work_end'], True)
            
            # 1æ™‚é–“åŠï¼ˆ90åˆ†ï¼‰æœªæº€ã®é–“éš”ã¯é€£ç¶šã¨ã¿ãªã™
            if current_start - last_end < 90:
                # çµåˆï¼šçµ‚äº†æ™‚é–“ã‚’å»¶é•·
                new_end_time = max(last_end, current_end)
                last_shift['work_end'] = minutes_to_time(new_end_time)
                continue
        
        # æ–°ã—ã„ã‚·ãƒ•ãƒˆã¨ã—ã¦è¿½åŠ 
        merged.append({
            'work_start': minutes_to_time(current_start),
            'work_end': minutes_to_time(current_end)
        })
    
    return merged

def load_employee_id_mapping(attendance_file_path: str = 'input/å‹¤æ€ å±¥æ­´.csv') -> Dict[str, str]:
    """å‹¤æ€ CSVã‹ã‚‰å¾“æ¥­å“¡åã¨å¾“æ¥­å“¡IDã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ"""
    df: Optional[pd.DataFrame] = None

    path = Path(attendance_file_path)
    if path.exists():
        df = _read_attendance_csv(path)

    if df is None:
        fallback = find_default_attendance_csv()
        if fallback and fallback.exists():
            df = _read_attendance_csv(fallback)

    if df is None:
        df = build_builtin_attendance_dataframe()

    mapping: Dict[str, str] = {}
    for _, row in df.iterrows():
        name = str(row.get('åå‰', '')).strip()
        emp_id = str(row.get('*å¾“æ¥­å“¡ID', '')).strip()
        
        if name and emp_id and name != 'nan' and emp_id != 'nan':
            normalized_name = normalize_name(name)
            if normalized_name:
                mapping[normalized_name] = emp_id
                mapping[name] = emp_id
    
    return mapping

def convert_japanese_date_to_iso(japanese_date: str) -> str:
    """å’Œæš¦æ—¥ä»˜ã‚’è¥¿æš¦ISOå½¢å¼ã«å¤‰æ›
    
    ä¾‹: 'ä»¤å’Œ07å¹´06æœˆ01æ—¥ (æ—¥)' -> '2025-06-01'
    """
    if not japanese_date or japanese_date.strip() == '':
        return ''
    
    try:
        # å’Œæš¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒãƒƒãƒ
        pattern = r'ä»¤å’Œ(\d+)å¹´(\d+)æœˆ(\d+)æ—¥'
        match = re.search(pattern, japanese_date)
        
        if match:
            reiwa_year = int(match.group(1))
            month = int(match.group(2))
            day = int(match.group(3))
            
            # ä»¤å’Œå…ƒå¹´ã¯2019å¹´ã€ä»¤å’Œ2å¹´ã¯2020å¹´...
            western_year = 2018 + reiwa_year
            
            return f"{western_year:04d}-{month:02d}-{day:02d}"
        
        # æ—¢ã«è¥¿æš¦å½¢å¼ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
        if re.match(r'\d{4}-\d{2}-\d{2}', japanese_date):
            return japanese_date
        
        # ãã®ä»–ã®å½¢å¼ã¯ç©ºæ–‡å­—ã‚’è¿”ã™
        return ''
        
    except Exception as e:
        print(f"æ—¥ä»˜å¤‰æ›ã‚¨ãƒ©ãƒ¼: {japanese_date} -> {str(e)}")
        return ''

def get_employee_id(employee_name: str, attendance_file_path: str = 'input/å‹¤æ€ å±¥æ­´.csv') -> str:
    """å‹¤æ€ CSVã‹ã‚‰å¾“æ¥­å“¡IDã‚’æ­£ã—ãå–å¾—"""
    # å‹¤æ€ CSVã‹ã‚‰å¾“æ¥­å“¡IDãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å–å¾—
    mapping = load_employee_id_mapping(attendance_file_path)
    
    if not mapping:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®å›ºå®šãƒãƒƒãƒ”ãƒ³ã‚°
        employee_ids = {
            'åˆ©å…‰ æ¢¨çµµ': 'EMP001',
            'å¤§å®® æµ©å­': 'EMP002',
            'æ—©å´ å‹éŸ³': 'EMP003',
            'æ—©å´ ç´çµµ': 'EMP004',
            'è©åŸ çœŸç†å­': 'EMP005'
        }
        return employee_ids.get(employee_name, f'EMP{hash(employee_name) % 1000:03d}')
    
    # ã¾ãšå…ƒã®åå‰ã§æ¤œç´¢
    if employee_name in mapping:
        return mapping[employee_name]
    
    # æ­£è¦åŒ–ã—ãŸåå‰ã§æ¤œç´¢
    normalized_name = normalize_name(employee_name)
    if normalized_name in mapping:
        return mapping[normalized_name]
    
    # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€å‹¤æ€ CSVã«å­˜åœ¨ã™ã‚‹å¾“æ¥­å“¡åã‚’è¡¨ç¤ºã—ã¦ã‚¨ãƒ©ãƒ¼
    available_names = [name for name in mapping.keys() if not name.startswith('EMP')]
    print(f"è­¦å‘Š: å¾“æ¥­å“¡ '{employee_name}' (æ­£è¦åŒ–å¾Œ: '{normalized_name}') ãŒå‹¤æ€ CSVã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    print(f"åˆ©ç”¨å¯èƒ½ãªå¾“æ¥­å“¡å: {available_names[:10]}...")  # æœ€åˆã®10åã‚’è¡¨ç¤º
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã®IDç”Ÿæˆ
    return f'EMP{hash(employee_name) % 1000:03d}'

def load_service_data_from_session() -> pd.DataFrame:
    """Streamlitã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€è¤‡æ•°CSVã‚’çµ±åˆ"""
    service_data = []
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    if 'service_data_list' in st.session_state and st.session_state.service_data_list:
        for service_df in st.session_state.service_data_list:
            if service_df is not None and not service_df.empty:
                # ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                for _, row in service_df.iterrows():
                    # æ§˜ã€…ãªã‚«ãƒ©ãƒ åã«å¯¾å¿œ
                    employee = ''
                    date = ''
                    start_time = ''
                    end_time = ''
                    
                    # æ‹…å½“è€…åã®å–å¾—ï¼ˆè¤‡æ•°ã®ã‚«ãƒ©ãƒ åã«å¯¾å¿œï¼‰
                    for col in ['æ‹…å½“æ‰€å“¡', 'æ‹…å½“è€…', 'è·å“¡å', 'å¾“æ¥­å“¡å', 'åå‰']:
                        if col in row and str(row[col]).strip():
                            employee = str(row[col]).strip()
                            break
                    
                    # æ—¥ä»˜ã®å–å¾—ï¼ˆè¤‡æ•°ã®ã‚«ãƒ©ãƒ åã«å¯¾å¿œï¼‰
                    for col in ['æ—¥ä»˜', 'ã‚µãƒ¼ãƒ“ã‚¹æä¾›æ—¥', 'å®Ÿæ–½æ—¥', 'å¹´æœˆæ—¥']:
                        if col in row and str(row[col]).strip():
                            date = str(row[col]).strip()
                            break
                    
                    # é–‹å§‹æ™‚é–“ã®å–å¾—ï¼ˆè¤‡æ•°ã®ã‚«ãƒ©ãƒ åã«å¯¾å¿œï¼‰
                    for col in ['é–‹å§‹æ™‚é–“', 'ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹æ™‚é–“', 'é–‹å§‹', 'é–‹å§‹æ™‚åˆ»']:
                        if col in row and str(row[col]).strip():
                            start_time = str(row[col]).strip()
                            break
                    
                    # çµ‚äº†æ™‚é–“ã®å–å¾—ï¼ˆè¤‡æ•°ã®ã‚«ãƒ©ãƒ åã«å¯¾å¿œï¼‰
                    for col in ['çµ‚äº†æ™‚é–“', 'ã‚µãƒ¼ãƒ“ã‚¹çµ‚äº†æ™‚é–“', 'çµ‚äº†', 'çµ‚äº†æ™‚åˆ»']:
                        if col in row and str(row[col]).strip():
                            end_time = str(row[col]).strip()
                            break
                    
                    if employee and date and start_time and end_time:
                        # å¾“æ¥­å“¡åã‚’æ­£è¦åŒ–
                        normalized_employee = normalize_name(employee)
                        
                        # æ—¥ä»˜ã‚’è¥¿æš¦å½¢å¼ã«å¤‰æ›
                        iso_date = convert_japanese_date_to_iso(date)
                        if not iso_date:
                            iso_date = date  # å¤‰æ›ã§ããªã„å ´åˆã¯å…ƒã®æ—¥ä»˜ã‚’ä½¿ç”¨
                        
                        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±: æ—¥ä»˜å¤‰æ›
                        if hasattr(st, 'session_state') and date != iso_date:
                            st.write(f"    ğŸ“… æ—¥ä»˜å¤‰æ›: '{date}' -> '{iso_date}'")
                        
                        service_data.append({
                            'employee': employee,  # å…ƒã®åå‰
                            'employee_normalized': normalized_employee,  # æ­£è¦åŒ–ã—ãŸåå‰
                            'date': iso_date,  # è¥¿æš¦å½¢å¼ã«å¤‰æ›ã—ãŸæ—¥ä»˜
                            'original_date': date,  # å…ƒã®æ—¥ä»˜å½¢å¼
                            'start_time': start_time,
                            'end_time': end_time,
                            'service_content': str(row.get('ã‚µãƒ¼ãƒ“ã‚¹å†…å®¹', row.get('å†…å®¹', ''))).strip(),
                            'implementation_time': str(row.get('å®Ÿæ–½æ™‚é–“', row.get('æ™‚é–“', ''))).strip()
                        })
    
    # çµ±åˆã•ã‚ŒãŸDataFrameã‚’è¿”ã™
    result_df = pd.DataFrame(service_data)
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
    if hasattr(st, 'session_state'):
        st.info(f"ğŸ” ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ç¢ºèª:")
        st.write(f"  - service_data_listå­˜åœ¨: {'service_data_list' in st.session_state}")
        if 'service_data_list' in st.session_state:
            st.write(f"  - service_data_listã®é•·ã•: {len(st.session_state.service_data_list) if st.session_state.service_data_list else 0}")
        
        st.info(f"ğŸ” çµ±åˆã•ã‚ŒãŸã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿: {len(result_df)}è¡Œ")
        if not result_df.empty:
            unique_employees = result_df['employee'].nunique()
            unique_dates = result_df['date'].nunique()
            st.write(f"  å¾“æ¥­å“¡æ•°: {unique_employees}, æ—¥ä»˜æ•°: {unique_dates}")
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
            st.write("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€åˆã®5è¡Œï¼‰:")
            st.dataframe(result_df.head())
        else:
            st.warning("âš ï¸ ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
    
    return result_df

def load_service_data_from_input_dir(workdir: str = None) -> pd.DataFrame:
    """inputãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ç›´æ¥ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾CSVã‚’èª­ã¿è¾¼ã¿"""
    service_data = []
    
    # inputãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’æ±ºå®š
    if workdir and os.path.exists(workdir):
        input_dir = os.path.join(workdir, "input")
    else:
        input_dir = "input"
    
    if not os.path.exists(input_dir):
        return pd.DataFrame(service_data)
    
    # inputãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ï¼ˆå‹¤æ€ å±¥æ­´ä»¥å¤–ï¼‰
    csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]
    service_files = [f for f in csv_files if 'å‹¤æ€ ' not in f and 'attendance' not in f.lower()]
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    if hasattr(st, 'session_state'):
        st.info(f"ğŸ” inputãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {input_dir}")
        st.write(f"  å…¨CSVãƒ•ã‚¡ã‚¤ãƒ«: {csv_files}")
        st.write(f"  ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ•ã‚¡ã‚¤ãƒ«: {service_files}")
    
    for service_file in service_files:
        file_path = os.path.join(input_dir, service_file)
        try:
            # è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
            df = None
            for encoding in ['utf-8-sig', 'cp932', 'utf-8', 'shift_jis']:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue
            
            if df is None:
                continue
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
            if hasattr(st, 'session_state'):
                st.success(f"âœ… {service_file}ã‚’èª­ã¿è¾¼ã¿: {len(df)}è¡Œ")
                st.write(f"  ã‚«ãƒ©ãƒ : {df.columns.tolist()}")
            
            # ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            for _, row in df.iterrows():
                # æ§˜ã€…ãªã‚«ãƒ©ãƒ åã«å¯¾å¿œ
                employee = ''
                date = ''
                start_time = ''
                end_time = ''
                
                # æ‹…å½“è€…åã®å–å¾—ï¼ˆè¤‡æ•°ã®ã‚«ãƒ©ãƒ åã«å¯¾å¿œï¼‰
                for col in ['æ‹…å½“æ‰€å“¡', 'æ‹…å½“è€…', 'è·å“¡å', 'å¾“æ¥­å“¡å', 'åå‰']:
                    if col in row and str(row[col]).strip():
                        employee = str(row[col]).strip()
                        break
                
                # æ—¥ä»˜ã®å–å¾—ï¼ˆè¤‡æ•°ã®ã‚«ãƒ©ãƒ åã«å¯¾å¿œï¼‰
                for col in ['æ—¥ä»˜', 'ã‚µãƒ¼ãƒ“ã‚¹æä¾›æ—¥', 'å®Ÿæ–½æ—¥', 'å¹´æœˆæ—¥']:
                    if col in row and str(row[col]).strip():
                        date = str(row[col]).strip()
                        break
                
                # é–‹å§‹æ™‚é–“ã®å–å¾—ï¼ˆè¤‡æ•°ã®ã‚«ãƒ©ãƒ åã«å¯¾å¿œï¼‰
                for col in ['é–‹å§‹æ™‚é–“', 'ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹æ™‚é–“', 'é–‹å§‹', 'é–‹å§‹æ™‚åˆ»']:
                    if col in row and str(row[col]).strip():
                        start_time = str(row[col]).strip()
                        break
                
                # çµ‚äº†æ™‚é–“ã®å–å¾—ï¼ˆè¤‡æ•°ã®ã‚«ãƒ©ãƒ åã«å¯¾å¿œï¼‰
                for col in ['çµ‚äº†æ™‚é–“', 'ã‚µãƒ¼ãƒ“ã‚¹çµ‚äº†æ™‚é–“', 'çµ‚äº†', 'çµ‚äº†æ™‚åˆ»']:
                    if col in row and str(row[col]).strip():
                        end_time = str(row[col]).strip()
                        break
                
                if employee and date and start_time and end_time:
                    # å¾“æ¥­å“¡åã‚’æ­£è¦åŒ–
                    normalized_employee = normalize_name(employee)
                    
                    # æ—¥ä»˜ã‚’è¥¿æš¦å½¢å¼ã«å¤‰æ›
                    iso_date = convert_japanese_date_to_iso(date)
                    if not iso_date:
                        iso_date = date  # å¤‰æ›ã§ããªã„å ´åˆã¯å…ƒã®æ—¥ä»˜ã‚’ä½¿ç”¨
                    
                    service_data.append({
                        'employee': employee,  # å…ƒã®åå‰
                        'employee_normalized': normalized_employee,  # æ­£è¦åŒ–ã—ãŸåå‰
                        'date': iso_date,  # è¥¿æš¦å½¢å¼ã«å¤‰æ›ã—ãŸæ—¥ä»˜
                        'original_date': date,  # å…ƒã®æ—¥ä»˜å½¢å¼
                        'start_time': start_time,
                        'end_time': end_time,
                        'service_content': str(row.get('ã‚µãƒ¼ãƒ“ã‚¹å†…å®¹', row.get('å†…å®¹', ''))).strip(),
                        'implementation_time': str(row.get('å®Ÿæ–½æ™‚é–“', row.get('æ™‚é–“', ''))).strip()
                    })
        
        except Exception as e:
            if hasattr(st, 'session_state'):
                st.error(f"âŒ {service_file}ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    result_df = pd.DataFrame(service_data)
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    if hasattr(st, 'session_state'):
        st.info(f"ğŸ” inputãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰çµ±åˆã•ã‚ŒãŸã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿: {len(result_df)}è¡Œ")
        if not result_df.empty:
            unique_employees = result_df['employee'].nunique()
            unique_dates = result_df['date'].nunique()
            st.write(f"  å¾“æ¥­å“¡æ•°: {unique_employees}, æ—¥ä»˜æ•°: {unique_dates}")
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
            st.write("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€åˆã®5è¡Œï¼‰:")
            st.dataframe(result_df.head())
    
    return result_df

def load_service_data_from_results(workdir: str = None) -> pd.DataFrame:
    """ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯çµæœã‹ã‚‰ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
    service_data = []
    
    if workdir and os.path.exists(workdir):
        # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰result_*.csvãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        result_files = glob.glob(os.path.join(workdir, "result_*.csv"))
    else:
        # ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰result_*.csvãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        result_files = glob.glob("result_*.csv")
    
    for file_path in result_files:
        try:
            df = pd.read_csv(file_path, encoding="utf-8-sig")
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(file_path, encoding="cp932")
            except UnicodeDecodeError:
                df = pd.read_csv(file_path, encoding="utf-8")
        
        # ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        for _, row in df.iterrows():
            employee = str(row.get('æ‹…å½“æ‰€å“¡', '')).strip()
            date = str(row.get('æ—¥ä»˜', '')).strip()
            start_time = str(row.get('é–‹å§‹æ™‚é–“', '')).strip()
            end_time = str(row.get('çµ‚äº†æ™‚é–“', '')).strip()
            
            if employee and date and start_time and end_time:
                # å¾“æ¥­å“¡åã‚’æ­£è¦åŒ–
                normalized_employee = normalize_name(employee)
                
                # æ—¥ä»˜ã‚’è¥¿æš¦å½¢å¼ã«å¤‰æ›
                iso_date = convert_japanese_date_to_iso(date)
                if not iso_date:
                    iso_date = date  # å¤‰æ›ã§ããªã„å ´åˆã¯å…ƒã®æ—¥ä»˜ã‚’ä½¿ç”¨
                
                service_data.append({
                    'employee': employee,
                    'employee_normalized': normalized_employee,
                    'date': iso_date,
                    'original_date': date,
                    'start_time': start_time,
                    'end_time': end_time,
                    'service_content': str(row.get('ã‚µãƒ¼ãƒ“ã‚¹å†…å®¹', '')).strip(),
                    'implementation_time': str(row.get('å®Ÿæ–½æ™‚é–“', '')).strip()
                })
    
    return pd.DataFrame(service_data)

def aggregate_daily_service_times(service_df: pd.DataFrame, employee: str, target_date: str) -> List[Dict]:
    """æŒ‡å®šå¾“æ¥­å“¡ãƒ»æ—¥ä»˜ã®ã‚µãƒ¼ãƒ“ã‚¹æ™‚é–“ã‚’é›†è¨ˆã—ã¦ã‚·ãƒ•ãƒˆãƒªã‚¹ãƒˆã‚’ä½œæˆ"""
    # ã‚µãƒ¼ãƒ“ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®å ´åˆã¯ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
    if service_df.empty or 'employee' not in service_df.columns:
        return []
    
    # å¾“æ¥­å“¡åã‚’æ­£è¦åŒ–
    normalized_employee = normalize_name(employee)
    
    # æŒ‡å®šå¾“æ¥­å“¡ãƒ»æ—¥ä»˜ã®ã‚µãƒ¼ãƒ“ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ç…§åˆï¼‰
    try:
        daily_services = pd.DataFrame()
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: å…ƒã®åå‰ã§æ¤œç´¢
        daily_services = service_df[
            (service_df['employee'] == employee) &
            (service_df['date'] == target_date)
        ].copy()
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ­£è¦åŒ–ã—ãŸåå‰ã§æ¤œç´¢ï¼ˆemployee_normalizedã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨ï¼‰
        if daily_services.empty and 'employee_normalized' in service_df.columns:
            daily_services = service_df[
                (service_df['employee_normalized'] == normalized_employee) &
                (service_df['date'] == target_date)
            ].copy()
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ã‚µãƒ¼ãƒ“ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å¾“æ¥­å“¡åã‚’å‹•çš„ã«æ­£è¦åŒ–ã—ã¦æ¯”è¼ƒ
        if daily_services.empty:
            matching_rows = []
            target_date_data = service_df[service_df['date'] == target_date]
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±: ãƒ‘ã‚¿ãƒ¼ãƒ³3é–‹å§‹
            if hasattr(st, 'session_state'):
                st.write(f"    ğŸ”„ ãƒ‘ã‚¿ãƒ¼ãƒ³3é–‹å§‹: å¯¾è±¡æ—¥ã®ãƒ‡ãƒ¼ã‚¿æ•°={len(target_date_data)}")
            
            for _, row in target_date_data.iterrows():
                service_employee = str(row['employee']).strip()
                service_normalized = normalize_name(service_employee)
                
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±: å„è¡Œã®ç…§åˆçŠ¶æ³
                if hasattr(st, 'session_state'):
                    st.write(f"    ç…§åˆä¸­: '{service_employee}' -> æ­£è¦åŒ–: '{service_normalized}'")
                
                # 4ã¤ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ç…§åˆ
                match_found = False
                if service_employee == employee:
                    match_found = True
                    if hasattr(st, 'session_state'):
                        st.write(f"      âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³1ãƒãƒƒãƒ: å…ƒã®åå‰åŒå£«")
                elif service_normalized == normalized_employee:
                    match_found = True
                    if hasattr(st, 'session_state'):
                        st.write(f"      âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³2ãƒãƒƒãƒ: æ­£è¦åŒ–ã—ãŸåå‰åŒå£«")
                elif service_normalized == employee:
                    match_found = True
                    if hasattr(st, 'session_state'):
                        st.write(f"      âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³3ãƒãƒƒãƒ: æ­£è¦åŒ– vs å…ƒ")
                elif service_employee == normalized_employee:
                    match_found = True
                    if hasattr(st, 'session_state'):
                        st.write(f"      âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³4ãƒãƒƒãƒ: å…ƒ vs æ­£è¦åŒ–")
                
                if match_found:
                    matching_rows.append(row)
            
            if matching_rows:
                daily_services = pd.DataFrame(matching_rows)
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
        if hasattr(st, 'session_state'):
            st.write(f"  ğŸ” å¾“æ¥­å“¡åç…§åˆ: '{employee}' -> æ­£è¦åŒ–: '{normalized_employee}'")
            if not daily_services.empty:
                st.write(f"    âœ… ãƒãƒƒãƒã—ãŸã‚µãƒ¼ãƒ“ã‚¹: {len(daily_services)}ä»¶")
                # ãƒãƒƒãƒã—ãŸå¾“æ¥­å“¡åã‚’è¡¨ç¤º
                matched_names = daily_services['employee'].unique()
                st.write(f"    ãƒãƒƒãƒã—ãŸåå‰: {list(matched_names)}")
            else:
                # åˆ©ç”¨å¯èƒ½ãªå¾“æ¥­å“¡åã‚’è¡¨ç¤º
                available_employees = service_df['employee'].unique()[:10]  # æœ€åˆã®10å
                available_normalized = [normalize_name(name) for name in available_employees]
                st.write(f"    âŒ ãƒãƒƒãƒãªã—ã€‚åˆ©ç”¨å¯èƒ½ãªå¾“æ¥­å“¡åï¼ˆæœ€åˆã®10åï¼‰:")
                for orig, norm in zip(available_employees, available_normalized):
                    st.write(f"      '{orig}' -> æ­£è¦åŒ–: '{norm}'")
                
    except KeyError as e:
        print(f"ã‚«ãƒ©ãƒ ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ : {service_df.columns.tolist()}")
        return []
    
    if daily_services.empty:
        return []
    
    shifts = []
    for _, service in daily_services.iterrows():
        start_time = service.get('start_time', '')
        end_time = service.get('end_time', '')
        
        if start_time and end_time and start_time != 'nan' and end_time != 'nan':
            shifts.append({
                'work_start': start_time,
                'work_end': end_time
            })
    
    return shifts

def get_attendance_shifts(attendance_data: pd.DataFrame, employee: str, target_date: str) -> List[Dict]:
    """å‹¤æ€ å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æŒ‡å®šå¾“æ¥­å“¡ãƒ»æ—¥ä»˜ã®å‡ºå‹¤ãƒ»é€€å‹¤æ™‚é–“ã‚’å–å¾—"""
    shifts = []
    
    # å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©²å½“å¾“æ¥­å“¡ãƒ»æ—¥ä»˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    employee_data = attendance_data[
        (attendance_data['åå‰'].str.strip() == employee.strip()) &
        (attendance_data['*å¹´æœˆæ—¥'].astype(str) == target_date)
    ]
    
    if employee_data.empty:
        return []
    
    row = employee_data.iloc[0]
    
    # å‡ºå‹¤ãƒ»é€€å‹¤æ™‚é–“ã®ãƒšã‚¢ã‚’å–å¾—ï¼ˆæœ€å¤§10ãƒšã‚¢ï¼‰
    for i in range(1, 11):
        start_col = f'å‡ºå‹¤{i}' if i > 1 else 'å‡ºå‹¤1'
        end_col = f'é€€å‹¤{i}' if i > 1 else 'é€€å‹¤1'
        
        start_time = str(row.get(start_col, '')).strip()
        end_time = str(row.get(end_col, '')).strip()
        
        if start_time and end_time and start_time != 'nan' and end_time != 'nan':
            shifts.append({
                'work_start': start_time,
                'work_end': end_time
            })
    
    return shifts

def generate_jinjer_csv(selected_employees: List[str], target_month: str, attendance_data: pd.DataFrame, workdir: str = None) -> str:
    """jinjerå½¢å¼CSVã‚’ç”Ÿæˆï¼ˆã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æœ€é©å‹¤å‹™æ™‚é–“ï¼‰
    
    1. ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆä½¿ç”¨
    2. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯1: ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯çµæœã®ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿
    3. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯2: å‹¤æ€ å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®å‡ºå‹¤ãƒ»é€€å‹¤æ™‚é–“
    
    1æ™‚é–“åŠãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨ã—ã¦ã‚·ãƒ•ãƒˆã‚’æœ€é©åŒ–ã—ã€
    æ‰“åˆ»åŒºåˆ†IDã‚„å‹¤å‹™çŠ¶æ³ãƒ•ãƒ©ã‚°ã‚’é©åˆ‡ã«è¨­å®šã™ã‚‹ã€‚
    """
    headers = create_jinjer_headers()
    work_start_base = headers.index('å‡ºå‹¤1')
    stamp_start_index = headers.index('æ‰“åˆ»åŒºåˆ†ID:1')
    stamp_count = sum(1 for col in headers if col.startswith('æ‰“åˆ»åŒºåˆ†ID:'))
    status_columns = ['æœªæ‰“åˆ»', 'æ¬ å‹¤', 'ä¼‘æ—¥æ‰“åˆ»', 'ä¼‘æš‡æ‰“åˆ»', 'å®Ÿç¸¾ç¢ºå®šçŠ¶æ³']
    status_indices = [headers.index(col) for col in status_columns]
    labor_indices = {
        'total': headers.index('ç·åŠ´åƒæ™‚é–“'),
        'actual': headers.index('å®ŸåŠ´åƒæ™‚é–“'),
        'break': headers.index('ä¼‘æ†©æ™‚é–“'),
        'overtime_total': headers.index('ç·æ®‹æ¥­æ™‚é–“'),
        'overtime_external': headers.index('æ³•å®šå¤–æ®‹æ¥­æ™‚é–“'),
    }
    forced_empty_indices = build_forced_empty_indices(headers)
    csv_content = ','.join(headers) + '\n'
    
    # ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆå„ªå…ˆé †ä½é †ï¼‰
    service_df = load_service_data_from_session()
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯inputãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿
    if service_df.empty:
        service_df = load_service_data_from_input_dir(workdir)
    
    # ãã‚Œã§ã‚‚ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯result_*.csvã‹ã‚‰èª­ã¿è¾¼ã¿
    if service_df.empty:
        service_df = load_service_data_from_results(workdir)
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±: ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®çŠ¶æ³
    if hasattr(st, 'session_state'):
        st.write(f"ğŸ“Š ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿çµæœ:")
        st.write(f"  ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å½¢çŠ¶: {service_df.shape}")
        if not service_df.empty:
            st.write(f"  ã‚«ãƒ©ãƒ : {service_df.columns.tolist()}")
            if 'employee' in service_df.columns:
                unique_employees = service_df['employee'].unique()
                st.write(f"  å¾“æ¥­å“¡æ•°: {len(unique_employees)}")
                st.write(f"  å¾“æ¥­å“¡åï¼ˆæœ€åˆã®10åï¼‰: {list(unique_employees[:10])}")
            else:
                st.error("âŒ 'employee'ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # æ—¥ä»˜å½¢å¼ã®ç¢ºèª
            if 'date' in service_df.columns:
                unique_dates = service_df['date'].unique()
                st.write(f"  æ—¥ä»˜æ•°: {len(unique_dates)}")
                st.write(f"  æ—¥ä»˜å½¢å¼ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®10ä»¶ï¼‰: {list(unique_dates[:10])}")
                
                # å¤§å®®æµ©å­ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹æ—¥ä»˜ã‚’ç¢ºèª
                omiya_data = service_df[service_df['employee_normalized'] == 'å¤§å®® æµ©å­']
                if not omiya_data.empty:
                    omiya_dates = omiya_data['date'].unique()
                    st.write(f"  å¤§å®®æµ©å­ã®ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãŒã‚ã‚‹æ—¥ä»˜: {list(omiya_dates[:5])}")
                else:
                    st.write("  å¤§å®®æµ©å­ã®ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ãªã—")
                
                # æœˆåˆ¥ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‚’ç¢ºèª
                service_df_temp = service_df.copy()
                service_df_temp['year_month'] = service_df_temp['date'].str[:7]  # YYYY-MMéƒ¨åˆ†ã‚’æŠ½å‡º
                month_counts = service_df_temp['year_month'].value_counts().sort_index()
                st.write(f"  ğŸ“… æœˆåˆ¥ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ: {dict(month_counts)}")
        else:
            st.error("âŒ ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
    
    # å¯¾è±¡æœˆã®å…¨æ—¥ä»˜ã‚’ç”Ÿæˆ
    year, month = map(int, target_month.split('-'))
    days_in_month = calendar.monthrange(year, month)[1]
    all_dates = [f"{year:04d}-{month:02d}-{day:02d}" for day in range(1, days_in_month + 1)]
    
    for employee in selected_employees:
        # å¾“æ¥­å“¡IDã‚’å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—
        employee_data = attendance_data[
            attendance_data['åå‰'].str.strip() == employee.strip()
        ].copy()
        
        employee_id = ''
        if not employee_data.empty:
            employee_id = str(employee_data.iloc[0].get('*å¾“æ¥­å“¡ID', '')).strip()
        
        # å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã«ãªã„å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã‚’ä½¿ç”¨
        if not employee_id or employee_id == 'nan':
            employee_id = get_employee_id(employee)
        
        for date in all_dates:
            row = [''] * len(headers)
            
            # åŸºæœ¬æƒ…å ±ã®è¨­å®š
            row[0] = employee  # åå‰
            row[1] = employee_id  # *å¾“æ¥­å“¡ID
            row[2] = date  # *å¹´æœˆæ—¥
            row[3] = '1'  # *æ‰“åˆ»ã‚°ãƒ«ãƒ¼ãƒ—ID
            row[4] = 'æ ªå¼ä¼šç¤¾hot'  # æ‰€å±ã‚°ãƒ«ãƒ¼ãƒ—å
            
            # ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãã®æ—¥ã®ã‚·ãƒ•ãƒˆã‚’å–å¾—
            shifts = aggregate_daily_service_times(service_df, employee, date)
            data_source = "service_data"
            
            # ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã®ã¿å‹¤æ€ å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—
            if not shifts:
                shifts = get_attendance_shifts(attendance_data, employee, date)
                data_source = "attendance_data"
                
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±: ã©ã¡ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
                if not shifts:
                    data_source = "no_data"
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’Streamlitã«å‡ºåŠ›ï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
            if hasattr(st, 'session_state'):
                st.write(f"ğŸ” {employee} {date}: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹={data_source}, ã‚·ãƒ•ãƒˆæ•°={len(shifts)}")
                if shifts:
                    for i, shift in enumerate(shifts):
                        st.write(f"  å…ƒã‚·ãƒ•ãƒˆ{i+1}: {shift['work_start']}-{shift['work_end']}")
            
            if shifts:
                # ã‚·ãƒ•ãƒˆãŒã‚ã‚‹å ´åˆã€1æ™‚é–“åŠãƒ«ãƒ¼ãƒ«ã§æœ€é©åŒ–
                merged_shifts = merge_overlapping_shifts(shifts)
                
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±: æœ€é©åŒ–çµæœï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
                if hasattr(st, 'session_state'):
                    st.write(f"  æœ€é©åŒ–å‰: {len(shifts)}ã‚·ãƒ•ãƒˆ -> æœ€é©åŒ–å¾Œ: {len(merged_shifts)}ã‚·ãƒ•ãƒˆ")
                    for i, shift in enumerate(merged_shifts):
                        st.write(f"    æœ€é©åŒ–ã‚·ãƒ•ãƒˆ{i+1}: {shift['work_start']}-{shift['work_end']}")
            else:
                # ã©ã¡ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚‚ã‚·ãƒ•ãƒˆãŒå–å¾—ã§ããªã„å ´åˆã¯ç©ºã®ã‚·ãƒ•ãƒˆ
                merged_shifts = []
                if hasattr(st, 'session_state'):
                    st.warning(f"âš ï¸ {employee} {date}: ã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # å‡ºå‹¤ãƒ»é€€å‹¤ã¯24æ™‚é–“å›ºå®šã€ä»¥é™ã¯ç©ºæ¬„
            if work_start_base < len(headers):
                row[work_start_base] = '0:00'
            if work_start_base + 1 < len(headers):
                row[work_start_base + 1] = '24:00'
            for shift_idx in range(1, 10):
                start_index = work_start_base + (shift_idx * 2)
                end_index = start_index + 1
                if start_index < len(headers):
                    row[start_index] = ''
                if end_index < len(headers):
                    row[end_index] = ''
            
            # ç®¡ç†æƒ…å ±ã®è¨­å®šï¼ˆå‹¤å‹™çŠ¶æ³ã€é…åˆ»å–æ¶ˆå‡¦ç†ç­‰ï¼‰- ç©ºæ¬„ã®ã¾ã¾
            # row[95-99]ã¯æ—¢ã«''ã§åˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ã®ã§ä½•ã‚‚ã—ãªã„
            
            # ç›´è¡Œãƒ»ç›´å¸°ã®è¨­å®š - ç©ºæ¬„ã®ã¾ã¾
            # row[100-119]ã¯æ—¢ã«''ã§åˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ã®ã§ä½•ã‚‚ã—ãªã„
            
            # æ‰“åˆ»åŒºåˆ†IDï¼ˆå…¨50åˆ—ï¼‰ã¯ãƒ–ãƒ©ãƒ³ã‚¯ã®ã¾ã¾ã«ã™ã‚‹
            for i in range(stamp_count):
                idx = stamp_start_index + i
                if idx < len(headers):
                    row[idx] = ''
            
            # å‹¤å‹™çŠ¶æ³ãƒ•ãƒ©ã‚°ï¼ˆæœªæ‰“åˆ»ã€æ¬ å‹¤ã€ä¼‘æ—¥æ‰“åˆ»ã€ä¼‘æš‡æ‰“åˆ»ã€å®Ÿç¸¾ç¢ºå®šçŠ¶æ³ï¼‰ã‚’ç©ºæ¬„ã«è¨­å®š
            for idx in status_indices:
                if idx < len(headers):
                    row[idx] = ''
            
            # åŠ´åƒæ™‚é–“ã®è¨­å®šï¼ˆå›ºå®šå€¤ï¼‰
            row[labor_indices['total']] = '24:00'
            row[labor_indices['actual']] = '23:00'
            row[labor_indices['break']] = '1:00'
            row[labor_indices['overtime_total']] = '16:00'
            row[labor_indices['overtime_external']] = '16:00'
            
            # CSVã®1è¡Œã¨ã—ã¦è¿½åŠ 
            enforce_forced_empty_fields(row, forced_empty_indices)
            csv_content += ','.join([
                f'"{field}"' if ',' in str(field) else str(field)
                for field in row
            ]) + '\n'
    
    return csv_content

def generate_0_24_jinjer_csv(selected_employees: List[str], target_month: str, attendance_data: pd.DataFrame) -> str:
    """0-24ãƒ‡ãƒ¼ã‚¿ç”¨jinjerå½¢å¼CSVã‚’ç”Ÿæˆï¼ˆå‡ºå‹¤1=0:00ã€é€€å‹¤1=24:00ï¼‰
    
    å…¨æ—¥ç¨‹ã§0:00-24:00ã®å‹¤å‹™ã¨ã—ã¦è¨­å®šã—ã€
    æ‰“åˆ»åŒºåˆ†IDã‚„å‹¤å‹™çŠ¶æ³ãƒ•ãƒ©ã‚°ã‚’é©åˆ‡ã«è¨­å®šã™ã‚‹ã€‚
    """
    headers = create_jinjer_headers()
    work_start_base = headers.index('å‡ºå‹¤1')
    stamp_start_index = headers.index('æ‰“åˆ»åŒºåˆ†ID:1')
    stamp_count = sum(1 for col in headers if col.startswith('æ‰“åˆ»åŒºåˆ†ID:'))
    status_columns = ['æœªæ‰“åˆ»', 'æ¬ å‹¤', 'ä¼‘æ—¥æ‰“åˆ»', 'ä¼‘æš‡æ‰“åˆ»', 'å®Ÿç¸¾ç¢ºå®šçŠ¶æ³']
    status_indices = [headers.index(col) for col in status_columns]
    labor_indices = {
        'total': headers.index('ç·åŠ´åƒæ™‚é–“'),
        'actual': headers.index('å®ŸåŠ´åƒæ™‚é–“'),
        'break': headers.index('ä¼‘æ†©æ™‚é–“'),
        'overtime_total': headers.index('ç·æ®‹æ¥­æ™‚é–“'),
        'overtime_external': headers.index('æ³•å®šå¤–æ®‹æ¥­æ™‚é–“'),
    }
    forced_empty_indices = build_forced_empty_indices(headers)
    csv_content = ','.join(headers) + '\n'
    
    # å¯¾è±¡æœˆã®å…¨æ—¥ä»˜ã‚’ç”Ÿæˆ
    year, month = map(int, target_month.split('-'))
    days_in_month = calendar.monthrange(year, month)[1]
    all_dates = [f"{year:04d}-{month:02d}-{day:02d}" for day in range(1, days_in_month + 1)]
    
    for employee in selected_employees:
        # å¾“æ¥­å“¡IDã‚’å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—
        employee_data = attendance_data[
            attendance_data['åå‰'].str.strip() == employee.strip()
        ].copy()
        
        employee_id = ''
        if not employee_data.empty:
            employee_id = str(employee_data.iloc[0].get('*å¾“æ¥­å“¡ID', '')).strip()
        
        # å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã«ãªã„å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã‚’ä½¿ç”¨
        if not employee_id or employee_id == 'nan':
            employee_id = get_employee_id(employee)
        
        for date in all_dates:
            row = [''] * len(headers)
            
            # åŸºæœ¬æƒ…å ±ã®è¨­å®š
            row[0] = employee  # åå‰
            row[1] = employee_id  # *å¾“æ¥­å“¡ID
            row[2] = date  # *å¹´æœˆæ—¥
            row[3] = '1'  # *æ‰“åˆ»ã‚°ãƒ«ãƒ¼ãƒ—ID
            row[4] = 'æ ªå¼ä¼šç¤¾hot'  # æ‰€å±ã‚°ãƒ«ãƒ¼ãƒ—å
            
            # 0-24ãƒ‡ãƒ¼ã‚¿ã®è¨­å®šï¼ˆå‡ºå‹¤1/é€€å‹¤1ã®ã¿ä½¿ç”¨ï¼‰
            if work_start_base < len(headers):
                row[work_start_base] = '0:00'
            if work_start_base + 1 < len(headers):
                row[work_start_base + 1] = '24:00'
            for shift_idx in range(1, 10):
                start_index = work_start_base + (shift_idx * 2)
                end_index = start_index + 1
                if start_index < len(headers):
                    row[start_index] = ''
                if end_index < len(headers):
                    row[end_index] = ''
            
            # æ‰“åˆ»åŒºåˆ†IDï¼ˆå…¨50åˆ—ï¼‰ã¯ãƒ–ãƒ©ãƒ³ã‚¯ã®ã¾ã¾ã«ã™ã‚‹
            for i in range(stamp_count):
                idx = stamp_start_index + i
                if idx < len(headers):
                    row[idx] = ''
            
            # å‹¤å‹™çŠ¶æ³ãƒ•ãƒ©ã‚°ã‚’ç©ºæ¬„ã«è¨­å®š
            for idx in status_indices:
                if idx < len(headers):
                    row[idx] = ''
            
            # åŠ´åƒæ™‚é–“ã®è¨­å®šï¼ˆ24æ™‚é–“å‹¤å‹™ï¼‰
            row[labor_indices['total']] = '24:00'
            row[labor_indices['actual']] = '23:00'
            row[labor_indices['break']] = '1:00'
            row[labor_indices['overtime_total']] = '16:00'
            row[labor_indices['overtime_external']] = '16:00'
            
            # CSVã®1è¡Œã¨ã—ã¦è¿½åŠ 
            enforce_forced_empty_fields(row, forced_empty_indices)
            csv_content += ','.join([
                f'"{field}"' if ',' in str(field) else str(field)
                for field in row
            ]) + '\n'
    
    return csv_content


def generate_delete_attendance_csv(
    selected_employees: List[str],
    target_month: str,
    attendance_data: pd.DataFrame
) -> str:
    """å…ƒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã™ã‚‹å‡ºå‹¤ãƒ»é€€å‹¤ã‚«ãƒ©ãƒ ã®ã¿'Null'ã§ã‚¯ãƒªã‚¢ã™ã‚‹CSVã‚’ç”Ÿæˆ"""
    headers = create_jinjer_headers()
    work_start_base = headers.index('å‡ºå‹¤1')
    forced_empty_indices = build_forced_empty_indices(headers)
    csv_content = ','.join(headers) + '\n'
    
    year, month = map(int, target_month.split('-'))
    days_in_month = calendar.monthrange(year, month)[1]
    all_dates = [f"{year:04d}-{month:02d}-{day:02d}" for day in range(1, days_in_month + 1)]
    
    for employee in selected_employees:
        employee_data = attendance_data[
            attendance_data['åå‰'].str.strip() == employee.strip()
        ].copy()
        name_col = resolve_column(employee_data, 'åå‰', fallback_suffix='åå‰')
        date_col = resolve_column(employee_data, '*å¹´æœˆæ—¥', fallback_suffix='å¹´æœˆæ—¥')
        start_cols = []
        end_cols = []
        for i in range(1, 11):
            start_cols.append(resolve_column(employee_data, f'å‡ºå‹¤{i}', fallback_suffix=f'å‡ºå‹¤{i}') or f'å‡ºå‹¤{i}')
            end_cols.append(resolve_column(employee_data, f'é€€å‹¤{i}', fallback_suffix=f'é€€å‹¤{i}') or f'é€€å‹¤{i}')
        
        employee_id = ''
        if not employee_data.empty:
            employee_id = str(employee_data.iloc[0].get('*å¾“æ¥­å“¡ID', '')).strip()
        if not employee_id or employee_id == 'nan':
            employee_id = get_employee_id(employee)
        
        for date in all_dates:
            row = [''] * len(headers)
            row[0] = employee
            row[1] = employee_id
            row[2] = date
            row[3] = '1'
            row[4] = 'æ ªå¼ä¼šç¤¾hot'
            
            source_row = None
            if not employee_data.empty and name_col and date_col:
                date_mask = employee_data[date_col].astype(str) == date
                if date_mask.any():
                    source_row = employee_data[date_mask].iloc[0]
            
            # å…ƒãƒ‡ãƒ¼ã‚¿ã«å€¤ãŒå…¥ã£ã¦ã„ãŸå‡ºå‹¤ãƒ»é€€å‹¤ã‚«ãƒ©ãƒ ã®ã¿Nullã«ã™ã‚‹
            for shift_idx in range(0, 10):
                start_index = work_start_base + (shift_idx * 2)
                end_index = start_index + 1
                start_has_value = False
                end_has_value = False
                
                if source_row is not None:
                    start_src_col = start_cols[shift_idx]
                    end_src_col = end_cols[shift_idx]
                    
                    if start_src_col in source_row.index:
                        start_val = source_row[start_src_col]
                        if isinstance(start_val, str):
                            start_has_value = start_val.strip() != ''
                        else:
                            start_has_value = pd.notna(start_val)
                    if end_src_col in source_row.index:
                        end_val = source_row[end_src_col]
                        if isinstance(end_val, str):
                            end_has_value = end_val.strip() != ''
                        else:
                            end_has_value = pd.notna(end_val)
                
                if start_index < len(headers):
                    row[start_index] = 'Null' if start_has_value or end_has_value else ''
                if end_index < len(headers):
                    row[end_index] = 'Null' if start_has_value or end_has_value else ''
            
            # åŠ´åƒæ™‚é–“ç³»ã¯ç©ºæ¬„ã«ã™ã‚‹
            enforce_forced_empty_fields(row, forced_empty_indices)
            csv_content += ','.join([
                f'"{field}"' if ',' in str(field) else str(field)
                for field in row
            ]) + '\n'
    
    return csv_content

def show_optimal_attendance_export():
    """æœ€é©å‹¤æ€ ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›UI"""
    
    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®š
    debug_mode = st.checkbox("ğŸ” ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹", value=False, help="ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚„æœ€é©åŒ–å‡¦ç†ã®è©³ç´°æƒ…å ±ã‚’è¡¨ç¤ºã—ã¾ã™")
    st.session_state.debug_mode = debug_mode
    
    # å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ç¢ºèª
    try:
        attendance_df = None
        attendance_source: Optional[str] = None
        uploaded_attendance = False
        
        if hasattr(st, 'session_state'):
            if st.session_state.get('attendance_df') is not None:
                attendance_df = st.session_state.attendance_df.copy()
                attendance_source = "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿å‹¤æ€ CSVï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰"
                uploaded_attendance = True
            else:
                session_att_path = st.session_state.get('attendance_file_path')
                if session_att_path and os.path.exists(session_att_path):
                    for encoding in ['utf-8-sig', 'cp932', 'utf-8', 'shift_jis']:
                        try:
                            attendance_df = pd.read_csv(session_att_path, encoding=encoding)
                            attendance_source = session_att_path
                            uploaded_attendance = True
                            break
                        except UnicodeDecodeError:
                            continue
        
        if attendance_df is None:
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ç„¡ã„å ´åˆã¯æ—¢å®šã®inputãƒ•ã‚©ãƒ«ãƒ€ã‚’å‚ç…§
            default_path = find_default_attendance_csv()
            if default_path:
                attendance_file_path = str(default_path)
                attendance_source = attendance_file_path
                for encoding in ['utf-8-sig', 'cp932', 'utf-8', 'shift_jis']:
                    try:
                        attendance_df = pd.read_csv(attendance_file_path, encoding=encoding)
                        break
                    except UnicodeDecodeError:
                        continue
        
        if attendance_df is None:
            attendance_df = build_builtin_attendance_dataframe()
            attendance_source = "çµ„ã¿è¾¼ã¿ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ"
        
        total_employees = get_unique_employee_names(attendance_df)
        if not total_employees:
            st.error("å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¾“æ¥­å“¡æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
        
        source_label = attendance_source or "ä¸æ˜"
        st.success(f"å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆã‚½ãƒ¼ã‚¹: {source_label}ï¼‰ã€‚ç™»éŒ²å¾“æ¥­å“¡: {len(total_employees)}å")
        
        available_months: List[str] = []
        # åˆ©ç”¨å¯èƒ½ãªå¹´æœˆã‚’è¡¨ç¤º
        try:
            name_col = resolve_column(attendance_df, 'åå‰', fallback_suffix='åå‰')
            date_col = resolve_column(attendance_df, '*å¹´æœˆæ—¥', fallback_suffix='å¹´æœˆæ—¥')
            if name_col and date_col:
                normalized_dates = attendance_df[date_col].apply(extract_month_string)
                month_counts = normalized_dates.value_counts().sort_index()
                available_months = [month for month in month_counts.index if isinstance(month, str) and month]
                if not month_counts.empty:
                    month_info = ', '.join([f"{month} ({count}ä»¶)" for month, count in month_counts.items()])
                    st.info(f"åˆ©ç”¨å¯èƒ½ãªå¹´æœˆ: {month_info}")
        except Exception:
            pass
        
        latest_available_month = max(available_months) if available_months else None
        now = datetime.now()
        if now.month == 1:
            prev_year = now.year - 1
            prev_month = 12
        else:
            prev_year = now.year
            prev_month = now.month - 1
        previous_month_str = f"{prev_year:04d}-{prev_month:02d}"

        if uploaded_attendance and latest_available_month:
            default_month_str = latest_available_month
        elif uploaded_attendance:
            default_month_str = f"{now.year:04d}-{now.month:02d}"
        else:
            default_month_str = previous_month_str

        try:
            default_year = int(default_month_str.split('-')[0])
            default_month = int(default_month_str.split('-')[1])
        except (ValueError, IndexError):
            default_year = now.year
            default_month = now.month
            default_month_str = f"{default_year:04d}-{default_month:02d}"

        year_candidates = {
            default_year,
            now.year,
            now.year - 1,
            now.year + 1,
        }
        year_candidates.update(
            int(month.split('-')[0]) for month in available_months if isinstance(month, str) and '-' in month
        )
        year_options = sorted({year for year in year_candidates if year >= 1900})

        month_options = list(range(1, 13))
        default_month_index = month_options.index(default_month) if default_month in month_options else now.month - 1

        # å¯¾è±¡æœˆã®é¸æŠ
        col1, col2 = st.columns(2)
        with col1:
            target_year = st.selectbox("å¯¾è±¡å¹´", year_options, index=year_options.index(default_year))
        with col2:
            target_month = st.selectbox(
                "å¯¾è±¡æœˆ",
                month_options,
                index=default_month_index,
                format_func=lambda m: f"{m:02d}æœˆ"
            )
        
        target_month_str = f"{target_year}-{target_month:02d}"
        
        # å¯¾è±¡æœˆã®å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        month_mask = build_month_mask(attendance_df, target_month_str)
        month_attendance_df = attendance_df[month_mask].copy() if len(attendance_df) else attendance_df.copy()
        
        if month_attendance_df.empty:
            st.warning(f"{target_month_str} ã®å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®æœˆã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            if 'selected_employees_export' in st.session_state:
                st.session_state.selected_employees_export = []
            st.stop()
        
        available_employees = get_unique_employee_names(month_attendance_df)
        if not available_employees:
            st.warning(f"{target_month_str} ã®å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã«å¾“æ¥­å“¡æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            if 'selected_employees_export' in st.session_state:
                st.session_state.selected_employees_export = []
            st.stop()
        
        st.caption(f"{target_month_str} ã®å¯¾è±¡å¾“æ¥­å“¡: {len(available_employees)}å")
        
        # å¾“æ¥­å“¡é¸æŠ
        st.markdown("### ğŸ‘¥ å‡ºåŠ›å¯¾è±¡å¾“æ¥­å“¡ã®é¸æŠ")
        
        if 'selected_employees_export' not in st.session_state:
            st.session_state.selected_employees_export = []
        else:
            st.session_state.selected_employees_export = [
                emp for emp in st.session_state.selected_employees_export
                if emp in available_employees
            ]
        
        # å…¨é¸æŠãƒ»å…¨è§£é™¤ãƒœã‚¿ãƒ³
        col1, col2, col3 = st.columns([1, 1, 2])
        with col1:
            if st.button("å…¨å“¡é¸æŠ", key="select_all_export"):
                st.session_state.selected_employees_export = available_employees.copy()
                st.rerun()
        with col2:
            if st.button("é¸æŠè§£é™¤", key="clear_all_export"):
                st.session_state.selected_employees_export = []
                st.rerun()
        
        # å¾“æ¥­å“¡ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹
        st.markdown("#### ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã§å¾“æ¥­å“¡ã‚’é¸æŠã—ã¦ãã ã•ã„")
        
        # 3åˆ—ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã§ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚’è¡¨ç¤º
        cols = st.columns(3)
        for i, employee in enumerate(sorted(available_employees)):
            with cols[i % 3]:
                is_selected = employee in st.session_state.selected_employees_export
                if st.checkbox(employee, value=is_selected, key=f"emp_check_{i}"):
                    if employee not in st.session_state.selected_employees_export:
                        st.session_state.selected_employees_export.append(employee)
                else:
                    if employee in st.session_state.selected_employees_export:
                        st.session_state.selected_employees_export.remove(employee)
        
        # é¸æŠã•ã‚ŒãŸå¾“æ¥­å“¡ã®è¡¨ç¤º
        if st.session_state.selected_employees_export:
            st.info(f"é¸æŠã•ã‚ŒãŸå¾“æ¥­å“¡: {len(st.session_state.selected_employees_export)}å")
            with st.expander("é¸æŠã•ã‚ŒãŸå¾“æ¥­å“¡ä¸€è¦§"):
                for i, emp in enumerate(st.session_state.selected_employees_export, 1):
                    st.write(f"{i}. {emp}")
        
        st.markdown("### ğŸ“¥ CSVå‡ºåŠ›")
        
        st.markdown("#### ğŸ¯ æœ€é©å‹¤æ€ ãƒ‡ãƒ¼ã‚¿CSV")
        st.caption("é¸æŠã—ãŸå¾“æ¥­å“¡ã®å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‚’jinjerå½¢å¼ã§ã¾ã¨ã‚ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")
        if st.session_state.selected_employees_export:
            if st.button("ğŸ¯ æœ€é©å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‚’CSVå‡ºåŠ›", type="primary", key="export_csv"):
                with st.spinner("CSVç”Ÿæˆä¸­..."):
                    try:
                        csv_content = generate_jinjer_csv(
                            st.session_state.selected_employees_export,
                            target_month_str,
                            month_attendance_df,
                            None
                        )
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename = f"æœ€é©å‹¤æ€ ãƒ‡ãƒ¼ã‚¿_{target_month_str}_{timestamp}.csv"
                        st.download_button(
                            label="ğŸ“¥ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=csv_content.encode('shift_jis', errors='ignore'),
                            file_name=filename,
                            mime="text/csv",
                            help="jinjerå½¢å¼ï¼ˆ194åˆ—ï¼‰ã®æœ€é©å‹¤æ€ ãƒ‡ãƒ¼ã‚¿CSVãƒ•ã‚¡ã‚¤ãƒ«"
                        )
                        st.success(f"âœ… CSVç”Ÿæˆå®Œäº†ï¼{len(st.session_state.selected_employees_export)}åã®å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›ã—ã¾ã—ãŸã€‚")
                        lines = csv_content.count('\n') - 1
                        st.info(f"ğŸ“Š å‡ºåŠ›è©³ç´°: {lines}è¡Œã®ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼å«ã‚€{lines + 1}è¡Œï¼‰")
                    except Exception as e:
                        st.error(f"CSVç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        else:
            st.info("å¾“æ¥­å“¡ã‚’é¸æŠã™ã‚‹ã¨ã€å€‹åˆ¥ã®æœ€é©å‹¤æ€ ãƒ‡ãƒ¼ã‚¿CSVã‚’ç”Ÿæˆã§ãã¾ã™ã€‚")

        st.write("")

        st.markdown("#### ğŸ•‘ æœ€é©ä¼‘æ†©æ™‚é–“CSV")
        st.caption("å‹¤æ€ CSVå…¨ä½“ã®ä¼‘æ†©æ ã‚’30åˆ†åˆ»ã¿ã«è¿‘ã¥ã‘ã€åˆè¨ˆä¼‘æ†©æ™‚é–“ã¯å¤‰ãˆãšã«å‡ºåŠ›ã—ã¾ã™ã€‚")
        if st.button("ğŸ•‘ æœ€é©ä¼‘æ†©æ™‚é–“CSVã‚’ç”Ÿæˆ", key="export_break_auto"):
            with st.spinner("ä¼‘æ†©æ™‚é–“ã‚’è£œæ­£ã—ã¦ã„ã¾ã™..."):
                try:
                    adjusted_df, rounded_rows, rounded_slots = auto_round_break_times(month_attendance_df)
                    csv_bytes = dataframe_to_jinjer_csv_bytes(
                        adjusted_df,
                        column_order=list(month_attendance_df.columns)
                    )
                    
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"æœ€é©ä¼‘æ†©æ™‚é–“_{target_month_str}_{timestamp}.csv"
                    
                    st.download_button(
                        label="ğŸ“¥ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=csv_bytes,
                        file_name=filename,
                        mime="text/csv",
                        help="å…¨å¾“æ¥­å“¡ã®ä¼‘æ†©æ™‚åˆ»ã‚’30åˆ†åˆ»ã¿ã§è£œæ­£ã—ãŸå‹¤æ€ CSV",
                        key="download_break_auto"
                    )
                    
                    if rounded_rows > 0:
                        st.success(f"âœ… {rounded_rows}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã§ä¼‘æ†©æ ï¼ˆ{rounded_slots}æ ï¼‰ã‚’è£œæ­£ã—ã¾ã—ãŸã€‚å…¨å¾“æ¥­å“¡ã«é©ç”¨ã—ã¦ã„ã¾ã™ã€‚")
                    else:
                        st.info("è£œæ­£å¯¾è±¡ã®ä¼‘æ†©æ™‚é–“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å…ƒã®å€¤ã®ã¾ã¾å‡ºåŠ›ã—ã¾ã™ã€‚")
                    
                    if debug_mode:
                        name_col = resolve_column(adjusted_df, 'åå‰', fallback_suffix='åå‰')
                        date_col = resolve_column(adjusted_df, '*å¹´æœˆæ—¥', fallback_suffix='å¹´æœˆæ—¥')
                        preview_pairs = get_break_column_pairs(adjusted_df)[:3]
                        preview_cols = [
                            col for col in [name_col, date_col] if col and col in adjusted_df.columns
                        ]
                        preview_cols += [
                            col for pair in preview_pairs for col in pair if col in adjusted_df.columns
                        ]
                        st.dataframe(adjusted_df.loc[:, preview_cols].head(), use_container_width=True)
                except Exception as e:
                    st.error(f"ä¼‘æ†©æ™‚é–“è£œæ­£ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

        st.write("")

        st.markdown("#### ğŸ” ä¼‘æ†©æ™‚é–“ä¸€æ‹¬å¤‰æ›´CSV")
        st.caption("é¸æŠã—ãŸå¾“æ¥­å“¡ãƒ»å¯¾è±¡æœˆã®ä¼‘æ†©æ ã‚’æŒ‡å®šã—ãŸæ™‚é–“å¸¯ã«ã¾ã¨ã‚ã¦ç½®ãæ›ãˆã¾ã™ã€‚")
        col_start, col_end = st.columns(2)
        with col_start:
            bulk_start_input = st.text_input(
                "ä¼‘æ†©é–‹å§‹æ™‚åˆ»ï¼ˆä¾‹: 14:00 ã¾ãŸã¯ 26:30ï¼‰",
                key="bulk_break_start"
            )
        with col_end:
            bulk_end_input = st.text_input(
                "ä¼‘æ†©çµ‚äº†æ™‚åˆ»ï¼ˆä¾‹: 15:00 ã¾ãŸã¯ 27:30ï¼‰",
                key="bulk_break_end"
            )
        
        if st.button("ğŸ” æŒ‡å®šä¼‘æ†©æ™‚é–“ã§CSVå‡ºåŠ›", key="export_break_bulk"):
            if not st.session_state.selected_employees_export:
                st.error("å…ˆã«å¾“æ¥­å“¡ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            else:
                start_minutes = parse_minute_of_day(bulk_start_input)
                end_minutes = parse_minute_of_day(bulk_end_input)
                
                if start_minutes is None or end_minutes is None:
                    st.error("æ™‚åˆ»ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚'HH:MM'å½¢å¼ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                elif end_minutes <= start_minutes:
                    st.error("çµ‚äº†æ™‚åˆ»ã¯é–‹å§‹æ™‚åˆ»ã‚ˆã‚Šå¾Œã«ãªã‚‹ã‚ˆã†ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
                else:
                    with st.spinner("ä¼‘æ†©æ™‚é–“ã‚’ä¸€æ‹¬å¤‰æ›´ã—ã¦ã„ã¾ã™..."):
                        try:
                            new_start_formatted = minutes_to_extended_time(start_minutes)
                            new_end_formatted = minutes_to_extended_time(end_minutes)
                            
                            target_mask = build_employee_month_mask(
                                month_attendance_df,
                                st.session_state.selected_employees_export,
                                target_month_str
                            )
                            matching_rows = month_attendance_df[target_mask]
                            
                            break_pairs = get_break_column_pairs(month_attendance_df)
                            existing_count = 0
                            if break_pairs:
                                start_col, end_col = break_pairs[0]
                                if start_col in month_attendance_df.columns and end_col in month_attendance_df.columns:
                                    def has_time(val):
                                        if isinstance(val, str):
                                            return val.strip() != ''
                                        return pd.notna(val)
                                    existing_mask = matching_rows[start_col].apply(has_time) | matching_rows[end_col].apply(has_time)
                                    existing_count = int(existing_mask.sum())
                            
                            st.info(f"å¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰: {len(matching_rows)}ä»¶ / ä¼‘æ†©1ãƒ»å¾©å¸°1ãŒè¨­å®šæ¸ˆã¿: {existing_count}ä»¶")
                            
                            overridden_df, overridden_rows, overridden_slots = bulk_override_break_times(
                                month_attendance_df,
                                st.session_state.selected_employees_export,
                                target_month_str,
                                new_start_formatted,
                                new_end_formatted
                            )
                            
                            download_df = overridden_df[
                                build_employee_month_mask(
                                    overridden_df,
                                    st.session_state.selected_employees_export,
                                    target_month_str
                                )
                            ].copy()
                            
                            if download_df.empty:
                                st.warning("æŒ‡å®šã•ã‚ŒãŸå¾“æ¥­å“¡ã«è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ç©ºã®CSVã‚’å‡ºåŠ›ã—ã¾ã™ã€‚")
                            csv_bytes = dataframe_to_jinjer_csv_bytes(
                                download_df,
                                column_order=list(month_attendance_df.columns)
                            )
                            
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            filename = f"ä¼‘æ†©æ™‚é–“ä¸€æ‹¬å¤‰æ›´_{target_month_str}_{timestamp}.csv"
                            
                            st.download_button(
                                label="ğŸ“¥ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                data=csv_bytes,
                                file_name=filename,
                                mime="text/csv",
                                help=f"ä¼‘æ†©æ™‚é–“ã‚’{new_start_formatted}ã€œ{new_end_formatted}ã«çµ±ä¸€ã—ãŸå‹¤æ€ CSV",
                                key="download_break_bulk"
                            )
                            
                            if overridden_rows > 0:
                                st.success(f"âœ… {overridden_rows}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã§ä¼‘æ†©æ ï¼ˆ{overridden_slots}æ ï¼‰ã‚’{new_start_formatted}ã€œ{new_end_formatted}ã«å¤‰æ›´ã—ã¾ã—ãŸã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ã¯é¸æŠã—ãŸå¾“æ¥­å“¡ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ã„ã¾ã™ã€‚")
                            else:
                                st.info("å¤‰æ›´å¯¾è±¡ã®ä¼‘æ†©1/å¾©å¸°1ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å…ƒã®å€¤ã®ã¾ã¾å‡ºåŠ›ã—ã¾ã™ã€‚")
                            
                            if debug_mode:
                                mask = build_employee_month_mask(
                                    overridden_df,
                                    st.session_state.selected_employees_export,
                                    target_month_str
                                )
                                if mask.any():
                                    name_col = resolve_column(overridden_df, 'åå‰', fallback_suffix='åå‰')
                                    date_col = resolve_column(overridden_df, '*å¹´æœˆæ—¥', fallback_suffix='å¹´æœˆæ—¥')
                                    preview_pairs = get_break_column_pairs(overridden_df)[:3]
                                    preview_cols = [
                                        col for col in [name_col, date_col] if col and col in overridden_df.columns
                                    ]
                                    preview_cols += [
                                        col for pair in preview_pairs for col in pair if col in overridden_df.columns
                                    ]
                                    st.dataframe(overridden_df.loc[mask, preview_cols].head(), use_container_width=True)
                        except Exception as e:
                            st.error(f"ä¼‘æ†©æ™‚é–“ä¸€æ‹¬å¤‰æ›´ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

        st.write("")

        st.markdown("#### ğŸ•› 24æ™‚é–“ãƒ‡ãƒ¼ã‚¿CSV")
        st.caption("é¸æŠã—ãŸå¾“æ¥­å“¡ãƒ»å¯¾è±¡æœˆã®å…¨ã‚·ãƒ•ãƒˆã‚’0:00ã€œ24:00ã¨ã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚")
        if st.button("ğŸ•› 24æ™‚é–“ãƒ‡ãƒ¼ã‚¿CSVã‚’ç”Ÿæˆ", key="export_full_day"):
            if not st.session_state.selected_employees_export:
                st.error("å…ˆã«å¾“æ¥­å“¡ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            else:
                with st.spinner("24æ™‚é–“ãƒ‡ãƒ¼ã‚¿CSVã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™..."):
                    try:
                        csv_content = generate_0_24_jinjer_csv(
                            st.session_state.selected_employees_export,
                            target_month_str,
                            month_attendance_df
                        )
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename = f"24æ™‚é–“ãƒ‡ãƒ¼ã‚¿_{target_month_str}_{timestamp}.csv"
                        st.download_button(
                            label="ğŸ“¥ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=csv_content.encode('shift_jis', errors='ignore'),
                            file_name=filename,
                            mime="text/csv",
                            help="å…¨æ—¥0:00ã€œ24:00ã®å‹¤æ€ ãƒ‡ãƒ¼ã‚¿CSV",
                            key="download_full_day"
                        )
                        st.success(f"âœ… {len(st.session_state.selected_employees_export)}ååˆ†ã®24æ™‚é–“ãƒ‡ãƒ¼ã‚¿CSVã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
                        lines = csv_content.count('\n') - 1
                        st.info(f"ğŸ“Š å‡ºåŠ›è©³ç´°: {lines}è¡Œã®ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼å«ã‚€{lines + 1}è¡Œï¼‰")
                    except Exception as e:
                        st.error(f"24æ™‚é–“ãƒ‡ãƒ¼ã‚¿CSVç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        
        # ä¸€æ‹¬å‰Šé™¤CSVã‚»ã‚¯ã‚·ãƒ§ãƒ³
        st.write("")
        st.markdown("#### ğŸ—‘ï¸ ä¸€æ‹¬å‰Šé™¤CSV")
        st.caption("é¸æŠã—ãŸå¾“æ¥­å“¡ãƒ»å¯¾è±¡æœˆã®å‡ºå‹¤/é€€å‹¤ã‚«ãƒ©ãƒ ã‚’ã™ã¹ã¦ Null ã§å‡ºåŠ›ã—ã¾ã™ã€‚æ“ä½œå‰ã«å¿…ãšç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        
        if 'delete_confirm_emps' not in st.session_state:
            st.session_state.delete_confirm_emps = []
        if 'delete_csv_bytes' not in st.session_state:
            st.session_state.delete_csv_bytes = None
        if 'delete_csv_filename' not in st.session_state:
            st.session_state.delete_csv_filename = ''
        
        # é¸æŠå¤–ã®å¾“æ¥­å“¡ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ãƒªã‚»ãƒƒãƒˆ
        if st.session_state.delete_confirm_emps:
            current_set = set(st.session_state.selected_employees_export)
            if not current_set.issuperset(st.session_state.delete_confirm_emps):
                st.session_state.delete_confirm_emps = []
                st.session_state.delete_csv_bytes = None
                st.session_state.delete_csv_filename = ''
        
        if st.button("ğŸ—‘ï¸ ä¸€æ‹¬å‰Šé™¤CSVã®ç¢ºèªã«é€²ã‚€", key="prepare_delete_csv"):
            if not st.session_state.selected_employees_export:
                st.error("å…ˆã«å¾“æ¥­å“¡ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            else:
                st.session_state.delete_confirm_emps = list(st.session_state.selected_employees_export)
                st.session_state.delete_csv_bytes = None
                st.session_state.delete_csv_filename = ''
        
        if st.session_state.delete_confirm_emps:
            st.warning("ä¸‹è¨˜ã®å¾“æ¥­å“¡ã§å‡ºå‹¤/é€€å‹¤ã‚’ Null ã«ã—ã¾ã™ã€‚å¿…ãšç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            for emp in st.session_state.delete_confirm_emps:
                st.write(f"- {emp}")
            
            col_confirm, col_cancel = st.columns([1, 1])
            with col_confirm:
                if st.button("âœ… ä¸Šè¨˜ã®å¾“æ¥­å“¡ã§CSVç”Ÿæˆ", key="confirm_delete_csv"):
                    try:
                        csv_content = generate_delete_attendance_csv(
                            st.session_state.delete_confirm_emps,
                            target_month_str,
                            month_attendance_df
                        )
                        st.session_state.delete_csv_bytes = csv_content.encode('shift_jis', errors='ignore')
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        st.session_state.delete_csv_filename = f"å‹¤æ€ ä¸€æ‹¬å‰Šé™¤_{target_month_str}_{timestamp}.csv"
                        st.success("âœ… CSVã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‹ã‚‰ä¿å­˜ã§ãã¾ã™ã€‚")
                    except Exception as e:
                        st.error(f"ä¸€æ‹¬å‰Šé™¤CSVç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            with col_cancel:
                if st.button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«", key="cancel_delete_csv"):
                    st.session_state.delete_confirm_emps = []
                    st.session_state.delete_csv_bytes = None
                    st.session_state.delete_csv_filename = ''
                    st.info("ä¸€æ‹¬å‰Šé™¤ã®æ“ä½œã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚")
            
            if st.session_state.delete_csv_bytes:
                st.download_button(
                    label="ğŸ“¥ ä¸€æ‹¬å‰Šé™¤CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=st.session_state.delete_csv_bytes,
                    file_name=st.session_state.delete_csv_filename or "å‹¤æ€ ä¸€æ‹¬å‰Šé™¤.csv",
                    mime="text/csv",
                    key="download_delete_csv"
                )
        
        elif st.session_state.delete_csv_bytes:
            # çŠ¶æ…‹ãŒãƒªã‚»ãƒƒãƒˆã•ã‚ŒãŸå ´åˆã®å®‰å…¨å¯¾ç­–
            st.session_state.delete_csv_bytes = None
            st.session_state.delete_csv_filename = ''
            
    except FileNotFoundError:
        st.error("å‹¤æ€ å±¥æ­´.csvãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚inputãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")

if __name__ == "__main__":
    show_optimal_attendance_export()
