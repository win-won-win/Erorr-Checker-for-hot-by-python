import streamlit as st
import pandas as pd
import tempfile
import os
import io
import zipfile
import shutil
import subprocess
import sys
from datetime import datetime
import calendar
from typing import List, Dict, Any

# optimal_attendance_export.pyã®æ©Ÿèƒ½ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from optimal_attendance_export import (
    create_jinjer_headers,
    time_to_minutes,
    minutes_to_time,
    format_time_for_csv,
    merge_overlapping_shifts,
    get_employee_id,
    generate_jinjer_csv,
    show_optimal_attendance_export
)

# è©³ç´°åˆ†ææ©Ÿèƒ½ï¼ˆé–¢æ•°å®šç¾©ï¼‰
def show_overlap_analysis(df):
    """é‡è¤‡ã®è©³ç´°åˆ†æã‚’è¡¨ç¤º"""
    st.markdown("#### ğŸ“Š é‡è¤‡åˆ†æ")
    
    # ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª
    if 'H' not in df.columns:
        st.warning("é‡è¤‡æ™‚é–“ï¼ˆåˆ†ï¼‰ã®ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ã‚’å†ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")
        return
    
    # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    overlap_data = df[df['H'].notna() & (pd.to_numeric(df['H'], errors='coerce') > 0)]
    
    if overlap_data.empty:
        st.info("é‡è¤‡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    # é‡è¤‡çµ±è¨ˆ
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("é‡è¤‡ä»¶æ•°", len(overlap_data))
    with col2:
        total_overlap_minutes = pd.to_numeric(overlap_data['H'], errors='coerce').sum()
        st.metric("ç·é‡è¤‡æ™‚é–“", f"{total_overlap_minutes:.0f}åˆ†")
    with col3:
        avg_overlap = pd.to_numeric(overlap_data['H'], errors='coerce').mean()
        st.metric("å¹³å‡é‡è¤‡æ™‚é–“", f"{avg_overlap:.1f}åˆ†")
    with col4:
        max_overlap = pd.to_numeric(overlap_data['H'], errors='coerce').max()
        st.metric("æœ€å¤§é‡è¤‡æ™‚é–“", f"{max_overlap:.0f}åˆ†")
    
    # é‡è¤‡ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
    st.markdown("##### é‡è¤‡ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ")
    if 'L' in overlap_data.columns and overlap_data['L'].notna().any():
        valid_type_data = overlap_data[overlap_data['L'].notna() & (overlap_data['L'] != '')]
        if not valid_type_data.empty:
            overlap_type_stats = valid_type_data.groupby('L').agg({
                'H': lambda x: len(x),  # ä»¶æ•°
                'C': 'nunique'  # é–¢ä¸è·å“¡æ•°
            })
            overlap_type_stats.columns = ['ä»¶æ•°', 'é–¢ä¸è·å“¡æ•°']
            st.dataframe(overlap_type_stats, use_container_width=True)
        else:
            st.info("é‡è¤‡ã‚¿ã‚¤ãƒ—ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        st.info("é‡è¤‡ã‚¿ã‚¤ãƒ—ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

def show_attendance_excess_analysis(df):
    """å‹¤æ€ è¶…éã®è©³ç´°åˆ†æã‚’è¡¨ç¤º"""
    st.markdown("#### ğŸ“Š å‹¤æ€ è¶…éåˆ†æ")
    
    # ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª
    if 'I' not in df.columns:
        st.warning("è¶…éæ™‚é–“ï¼ˆåˆ†ï¼‰ã®ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ã‚’å†ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")
        return
    
    # è¶…éãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    excess_data = df[df['I'].notna() & (pd.to_numeric(df['I'], errors='coerce') > 0)]
    
    if excess_data.empty:
        st.info("å‹¤æ€ è¶…éãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    # è¶…éçµ±è¨ˆ
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("è¶…éä»¶æ•°", len(excess_data))
    with col2:
        total_excess_minutes = pd.to_numeric(excess_data['I'], errors='coerce').sum()
        st.metric("ç·è¶…éæ™‚é–“", f"{total_excess_minutes:.0f}åˆ†")
    with col3:
        avg_excess = pd.to_numeric(excess_data['I'], errors='coerce').mean()
        st.metric("å¹³å‡è¶…éæ™‚é–“", f"{avg_excess:.1f}åˆ†")
    with col4:
        max_excess = pd.to_numeric(excess_data['I'], errors='coerce').max()
        st.metric("æœ€å¤§è¶…éæ™‚é–“", f"{max_excess:.0f}åˆ†")
    
    # ã‚«ãƒãƒ¼çŠ¶æ³åˆ¥é›†è¨ˆ
    st.markdown("##### ã‚«ãƒãƒ¼çŠ¶æ³åˆ¥é›†è¨ˆ")
    if 'M' in excess_data.columns and excess_data['M'].notna().any():
        valid_coverage_data = excess_data[excess_data['M'].notna() & (excess_data['M'] != '')]
        if not valid_coverage_data.empty:
            coverage_stats = valid_coverage_data.groupby('M').agg({
                'I': lambda x: len(x),  # ä»¶æ•°
                'C': 'nunique'  # é–¢ä¸è·å“¡æ•°
            })
            coverage_stats.columns = ['ä»¶æ•°', 'é–¢ä¸è·å“¡æ•°']
            st.dataframe(coverage_stats, use_container_width=True)
        else:
            st.info("ã‚«ãƒãƒ¼çŠ¶æ³ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        st.info("ã‚«ãƒãƒ¼çŠ¶æ³ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    
    # è·å“¡åˆ¥è¶…éãƒ©ãƒ³ã‚­ãƒ³ã‚°
    st.markdown("##### è·å“¡åˆ¥è¶…éæ™‚é–“ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä¸Šä½10åï¼‰")
    if 'C' in excess_data.columns:
        staff_excess = excess_data.groupby('C').agg({
            'I': lambda x: len(x),  # è¶…éä»¶æ•°
            'C': 'first'  # è·å“¡å
        })
        staff_excess.columns = ['è¶…éä»¶æ•°', 'è·å“¡å']
        staff_excess = staff_excess.sort_values('è¶…éä»¶æ•°', ascending=False).head(10)
        st.dataframe(staff_excess[['è¶…éä»¶æ•°']], use_container_width=True)
    else:
        st.info("è·å“¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

def show_time_slot_analysis(df):
    """æ™‚é–“å¸¯åˆ†æã‚’è¡¨ç¤º"""
    st.markdown("#### ğŸ“Š æ™‚é–“å¸¯åˆ†æ")
    
    # æ™‚é–“å¸¯åˆ¥ã®ã‚¨ãƒ©ãƒ¼åˆ†å¸ƒ
    if 'E' in df.columns and df['E'].notna().any():
        # é–‹å§‹æ™‚é–“ã‹ã‚‰æ™‚é–“å¸¯ã‚’æŠ½å‡º
        df_copy = df.copy()
        df_copy['æ™‚é–“å¸¯'] = df_copy['E'].apply(lambda x: f"{str(x)[:2]}:00" if pd.notna(x) and str(x) else "ä¸æ˜")
        
        time_slot_stats = df_copy.groupby('æ™‚é–“å¸¯').agg({
            'A': lambda x: (x == 'â—¯').sum(),  # ã‚¨ãƒ©ãƒ¼ä»¶æ•°
            'C': 'count',  # ç·ä»¶æ•°
            'H': 'sum',  # é‡è¤‡æ™‚é–“åˆè¨ˆ
            'I': 'sum'   # è¶…éæ™‚é–“åˆè¨ˆ
        }).round(1)
        time_slot_stats.columns = ['ã‚¨ãƒ©ãƒ¼ä»¶æ•°', 'ç·ä»¶æ•°', 'é‡è¤‡æ™‚é–“åˆè¨ˆ(åˆ†)', 'è¶…éæ™‚é–“åˆè¨ˆ(åˆ†)']
        time_slot_stats['ã‚¨ãƒ©ãƒ¼ç‡'] = (time_slot_stats['ã‚¨ãƒ©ãƒ¼ä»¶æ•°'] / time_slot_stats['ç·ä»¶æ•°'] * 100).round(1)
        
        st.dataframe(time_slot_stats, use_container_width=True)
    else:
        st.info("æ™‚é–“å¸¯åˆ†æç”¨ã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")

def show_staff_workload_analysis(df):
    """è·å“¡è² è·åˆ†æã‚’è¡¨ç¤º"""
    st.markdown("#### ğŸ“Š è·å“¡è² è·åˆ†æ")
    
    if 'C' in df.columns and df['C'].notna().any():
        staff_workload = df.groupby('C').agg({
            'A': lambda x: (x == 'â—¯').sum(),  # ã‚¨ãƒ©ãƒ¼ä»¶æ•°
            'C': 'count',  # ç·ä»¶æ•°
            'H': 'sum',  # é‡è¤‡æ™‚é–“åˆè¨ˆ
            'I': 'sum',  # è¶…éæ™‚é–“åˆè¨ˆ
            'N': 'mean'  # å¹³å‡å‹¤å‹™åŒºé–“æ•°
        }).round(1)
        staff_workload.columns = ['ã‚¨ãƒ©ãƒ¼ä»¶æ•°', 'ç·ä»¶æ•°', 'é‡è¤‡æ™‚é–“åˆè¨ˆ(åˆ†)', 'è¶…éæ™‚é–“åˆè¨ˆ(åˆ†)', 'å¹³å‡å‹¤å‹™åŒºé–“æ•°']
        staff_workload['ã‚¨ãƒ©ãƒ¼ç‡'] = (staff_workload['ã‚¨ãƒ©ãƒ¼ä»¶æ•°'] / staff_workload['ç·ä»¶æ•°'] * 100).round(1)
        staff_workload = staff_workload.sort_values('ã‚¨ãƒ©ãƒ¼ä»¶æ•°', ascending=False)
        
        st.dataframe(staff_workload, use_container_width=True)
    else:
        st.info("è·å“¡è² è·åˆ†æç”¨ã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")

def show_row_detail_modal(row):
    """é¸æŠã•ã‚ŒãŸè¡Œã®è©³ç´°æƒ…å ±ã‚’ãƒ¢ãƒ¼ãƒ€ãƒ«é¢¨ã«è¡¨ç¤º"""
    with st.expander(f"ğŸ“‹ è©³ç´°æƒ…å ± - {row.get('C', 'N/A')} ({row.get('D', 'N/A')})", expanded=True):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**åŸºæœ¬æƒ…å ±**")
            st.write(f"â€¢ ã‚¨ãƒ©ãƒ¼: {row.get('A', 'N/A')}")
            st.write(f"â€¢ ã‚«ãƒ†ã‚´ãƒª: {row.get('B', 'N/A')}")
            st.write(f"â€¢ æ‹…å½“æ‰€å“¡: {row.get('C', 'N/A')}")
            st.write(f"â€¢ æ—¥ä»˜: {row.get('D', 'N/A')}")
            st.write(f"â€¢ æ™‚é–“: {row.get('E', 'N/A')} - {row.get('F', 'N/A')}")
        
        with col2:
            st.markdown("**è©³ç´°æƒ…å ±**")
            if pd.notna(row.get('H', 0)) and row.get('H', 0) > 0:
                st.write(f"â€¢ é‡è¤‡æ™‚é–“: {row.get('H', 'N/A')}åˆ†")
                st.write(f"â€¢ é‡è¤‡ç›¸æ‰‹æ–½è¨­: {row.get('J', 'N/A')}")
            if pd.notna(row.get('I', 0)) and row.get('I', 0) > 0:
                st.write(f"â€¢ è¶…éæ™‚é–“: {row.get('I', 'N/A')}åˆ†")

def save_upload_to(path: str, uploaded_file):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã«ä¿å­˜ã™ã‚‹"""
    import time
    import stat
    import tempfile
    
    debug_info = []
    
    try:
        debug_info.append(f"é–‹å§‹: path={path}")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        dir_path = os.path.dirname(path)
        debug_info.append(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹: {dir_path}")
        
        os.makedirs(dir_path, exist_ok=True)
        debug_info.append(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆå®Œäº†: å­˜åœ¨={os.path.exists(dir_path)}")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ¨©é™ã‚’ç¢ºèªãƒ»è¨­å®š
        try:
            os.chmod(dir_path, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)
            debug_info.append("ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¨©é™è¨­å®šå®Œäº†")
        except Exception as e:
            debug_info.append(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¨©é™è¨­å®šå¤±æ•—: {e}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ç‰¹æ®Šæ–‡å­—ã‚’æ­£è¦åŒ–ï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ãƒœãƒƒã‚¯ã‚¹å¯¾å¿œï¼‰
        import unicodedata
        normalized_path = unicodedata.normalize('NFC', path)
        debug_info.append(f"æ­£è¦åŒ–ãƒ‘ã‚¹: {normalized_path}")
        
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°å‰Šé™¤
        if os.path.exists(normalized_path):
            os.remove(normalized_path)
            debug_info.append("æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å®Œäº†")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        data = uploaded_file.getbuffer()
        expected_size = len(data)
        debug_info.append(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {expected_size} bytes")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚ˆã‚Šç¢ºå®Ÿã«ä¿å­˜
        temp_path = normalized_path + ".tmp"
        debug_info.append(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: {temp_path}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿
        with open(temp_path, "wb") as f:
            f.write(data)
            f.flush()  # ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
            os.fsync(f.fileno())  # ãƒ‡ã‚£ã‚¹ã‚¯ã«å¼·åˆ¶æ›¸ãè¾¼ã¿
        
        debug_info.append("ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿å®Œäº†")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
        if not os.path.exists(temp_path):
            raise Exception(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—: {temp_path}")
        
        temp_size = os.path.getsize(temp_path)
        debug_info.append(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {temp_size} bytes")
        
        if temp_size != expected_size:
            raise Exception(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒä¸æ­£: æœŸå¾…å€¤={expected_size}, å®Ÿéš›={temp_size}")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ç§»å‹•
        import shutil
        shutil.move(temp_path, normalized_path)
        debug_info.append("ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•å®Œäº†")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™ã‚’è¨­å®š
        try:
            os.chmod(normalized_path, stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH)
            debug_info.append("ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™è¨­å®šå®Œäº†")
        except Exception as e:
            debug_info.append(f"ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™è¨­å®šå¤±æ•—: {e}")
        
        # æœ€çµ‚ç¢ºèª
        time.sleep(0.5)  # 500mså¾…æ©Ÿ
        
        if not os.path.exists(normalized_path):
            raise Exception(f"æœ€çµ‚ç¢ºèªã§ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {normalized_path}")
        
        actual_size = os.path.getsize(normalized_path)
        debug_info.append(f"æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {actual_size} bytes")
        
        if actual_size == 0:
            raise Exception(f"ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µã‚¤ã‚ºãŒ0ã§ã™: {normalized_path}")
        
        if actual_size != expected_size:
            raise Exception(f"æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒä¸€è‡´ã—ã¾ã›ã‚“: æœŸå¾…å€¤={expected_size}, å®Ÿéš›={actual_size}")
        
        debug_info.append("ä¿å­˜å‡¦ç†å®Œäº†")
        return normalized_path, debug_info
            
    except Exception as e:
        debug_info.append(f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")
        raise Exception(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼ ({path}): {str(e)}\nãƒ‡ãƒãƒƒã‚°æƒ…å ±: {'; '.join(debug_info)}")

def collect_summary(result_paths):
    summary_rows = []
    for p in result_paths:
        try:
            df = pd.read_csv(p, encoding="utf-8-sig")
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(p, encoding="cp932")
            except UnicodeDecodeError:
                df = pd.read_csv(p, encoding="utf-8")
        total = len(df)
        err_cnt = int((df.get("ã‚¨ãƒ©ãƒ¼","") == "â—¯").sum())
        cat_counts = {}
        if "ã‚«ãƒ†ã‚´ãƒª" in df.columns:
            for v, c in df["ã‚«ãƒ†ã‚´ãƒª"].value_counts().items():
                if isinstance(v, str) and v:
                    cat_counts[v] = int(c)
        summary_rows.append({
            "ãƒ•ã‚¡ã‚¤ãƒ«": os.path.basename(p),
            "ç·ä»¶æ•°": total,
            "ã‚¨ãƒ©ãƒ¼ä»¶æ•°": err_cnt,
            **{f"ã‚«ãƒ†ã‚´ãƒª:{k}": v for k, v in cat_counts.items()}
        })
    return pd.DataFrame(summary_rows)

def prepare_grid_data(result_paths):
    """
    result_*.csvãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤ºç”¨ã®DataFrameã‚’ä½œæˆ
    æŒ‡å®šã•ã‚ŒãŸé †åºã§ã‚«ãƒ©ãƒ ã‚’ä¸¦ã³æ›¿ãˆã€å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿ã‚’å«ã‚ã‚‹
    """
    grid_data = []
    
    for p in result_paths:
        try:
            df = pd.read_csv(p, encoding="utf-8-sig")
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(p, encoding="cp932")
            except UnicodeDecodeError:
                df = pd.read_csv(p, encoding="utf-8")
        
        # æŒ‡å®šã•ã‚ŒãŸé †åºã§ã‚«ãƒ©ãƒ ã‚’æ§‹ç¯‰
        for idx, row in df.iterrows():
            grid_row = {
                # æŒ‡å®šã•ã‚ŒãŸé †åº: ã‚¨ãƒ©ãƒ¼ã€€ã‚«ãƒ†ã‚´ãƒªã€€ä»£æ›¿è·å“¡ãƒªã‚¹ãƒˆã€€æ‹…å½“æ‰€å“¡ã€€åˆ©ç”¨è€…åã€€æ—¥ä»˜ã€€é–‹å§‹æ™‚é–“ã€€çµ‚äº†æ™‚é–“ã€€ã‚µãƒ¼ãƒ“ã‚¹è©³ç´°ã€€é‡è¤‡æ™‚é–“ã€€è¶…éæ™‚é–“
                'ã‚¨ãƒ©ãƒ¼': row.get('ã‚¨ãƒ©ãƒ¼', ''),
                'ã‚«ãƒ†ã‚´ãƒª': row.get('ã‚«ãƒ†ã‚´ãƒª', ''),
                'ä»£æ›¿è·å“¡ãƒªã‚¹ãƒˆ': row.get('ä»£æ›¿è·å“¡ãƒªã‚¹ãƒˆ', 'ãƒ¼'),
                'æ‹…å½“æ‰€å“¡': row.get('æ‹…å½“æ‰€å“¡', ''),
                'åˆ©ç”¨è€…å': row.get('åˆ©ç”¨è€…å', ''),
                'æ—¥ä»˜': row.get('æ—¥ä»˜', ''),
                'é–‹å§‹æ™‚é–“': row.get('é–‹å§‹æ™‚é–“', ''),
                'çµ‚äº†æ™‚é–“': row.get('çµ‚äº†æ™‚é–“', ''),
                'ã‚µãƒ¼ãƒ“ã‚¹è©³ç´°': f"{row.get('ã‚µãƒ¼ãƒ“ã‚¹å†…å®¹', '')} - {row.get('å®Ÿæ–½æ™‚é–“', '')}".strip(' -'),
                'é‡è¤‡æ™‚é–“': row.get('é‡è¤‡æ™‚é–“ï¼ˆåˆ†ï¼‰', 0),
                'è¶…éæ™‚é–“': row.get('è¶…éæ™‚é–“ï¼ˆåˆ†ï¼‰', 0),
                # å‹¤å‹™æ™‚é–“ã®è©³ç´°æƒ…å ±ã‚’è¿½åŠ 
                'ã‚«ãƒãƒ¼çŠ¶æ³': row.get('ã‚«ãƒãƒ¼çŠ¶æ³', ''),
                'ã‚¨ãƒ©ãƒ¼è·å“¡å‹¤å‹™æ™‚é–“': row.get('ã‚¨ãƒ©ãƒ¼è·å“¡å‹¤å‹™æ™‚é–“', ''),
                'ä»£æ›¿è·å“¡å‹¤å‹™æ™‚é–“': row.get('ä»£æ›¿è·å“¡å‹¤å‹™æ™‚é–“', ''),
                'å‹¤å‹™æ™‚é–“è©³ç´°': row.get('å‹¤å‹™æ™‚é–“è©³ç´°', ''),
                'å‹¤å‹™æ™‚é–“å¤–è©³ç´°': row.get('å‹¤å‹™æ™‚é–“å¤–è©³ç´°', ''),
                'æœªã‚«ãƒãƒ¼åŒºé–“': row.get('æœªã‚«ãƒãƒ¼åŒºé–“', ''),
                'å‹¤å‹™åŒºé–“æ•°': row.get('å‹¤å‹™åŒºé–“æ•°', 0)
            }
            grid_data.append(grid_row)
    
    # æŒ‡å®šã•ã‚ŒãŸé †åºã§DataFrameã‚’ä½œæˆ
    column_order = [
        'ã‚¨ãƒ©ãƒ¼', 'ã‚«ãƒ†ã‚´ãƒª', 'ä»£æ›¿è·å“¡ãƒªã‚¹ãƒˆ', 'æ‹…å½“æ‰€å“¡', 'åˆ©ç”¨è€…å',
        'æ—¥ä»˜', 'é–‹å§‹æ™‚é–“', 'çµ‚äº†æ™‚é–“', 'ã‚µãƒ¼ãƒ“ã‚¹è©³ç´°', 'é‡è¤‡æ™‚é–“', 'è¶…éæ™‚é–“',
        'ã‚«ãƒãƒ¼çŠ¶æ³', 'ã‚¨ãƒ©ãƒ¼è·å“¡å‹¤å‹™æ™‚é–“', 'ä»£æ›¿è·å“¡å‹¤å‹™æ™‚é–“', 'å‹¤å‹™æ™‚é–“è©³ç´°',
        'å‹¤å‹™æ™‚é–“å¤–è©³ç´°', 'æœªã‚«ãƒãƒ¼åŒºé–“', 'å‹¤å‹™åŒºé–“æ•°'
    ]
    
    df = pd.DataFrame(grid_data)
    return df[column_order] if not df.empty else pd.DataFrame(columns=column_order)

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="ã‚µãƒ¼ãƒ“ã‚¹å®Ÿæ…‹ Ã— å‹¤æ€  ä¸æ•´åˆãƒã‚§ãƒƒã‚¯", layout="wide")

st.title("ã‚µãƒ¼ãƒ“ã‚¹å®Ÿæ…‹ Ã— å‹¤æ€  ä¸æ•´åˆãƒã‚§ãƒƒã‚¯ UI")
st.markdown("æ–½è¨­ã”ã¨ã®ã‚µãƒ¼ãƒ“ã‚¹å®Ÿæ…‹CSVï¼ˆè¤‡æ•°ï¼‰ã¨å‹¤æ€ å±¥æ­´CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒœã‚¿ãƒ³1ã¤ã§çªåˆãƒ»æ¤œå‡ºã¨result CSVã®å‡ºåŠ›ã‚’è¡Œã„ã¾ã™ã€‚")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'processing_complete' not in st.session_state:
    st.session_state.processing_complete = False
if 'result_paths' not in st.session_state:
    st.session_state.result_paths = []
if 'diagnostic_paths' not in st.session_state:
    st.session_state.diagnostic_paths = []
if 'summary_df' not in st.session_state:
    st.session_state.summary_df = None
if 'workdir' not in st.session_state:
    st.session_state.workdir = None

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
with st.sidebar:
    st.header("ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    identical_prefer = st.selectbox(
        "å®Œå…¨ä¸€è‡´æ™‚ã®ãƒ•ãƒ©ã‚°ä»˜ä¸ï¼ˆæ–½è¨­åã®æ˜‡é †ã§ï¼‰",
        options=["earlier", "later"],
        index=0,
        help="é–‹å§‹/çµ‚äº†ãŒå®Œå…¨ä¸€è‡´ã®ã¨ãã«ã€æ–½è¨­åã®æ˜‡é †ã§ earlier/later ã®ã©ã¡ã‚‰ã«ãƒ•ãƒ©ã‚°ä»˜ä¸ã™ã‚‹ã‹"
    )
    alt_delim = st.text_input("ä»£æ›¿è·å“¡ãƒªã‚¹ãƒˆã®åŒºåˆ‡ã‚Šæ–‡å­—", value="/")
    use_schedule_when_missing = st.checkbox("å®Ÿæ‰“åˆ»ãŒæ¬ æã®ã¨ãã«äºˆå®šã§ä»£ç”¨ã™ã‚‹ (--use-schedule-when-missing)", value=True)
    service_staff_col = st.text_input("ã‚µãƒ¼ãƒ“ã‚¹å®Ÿæ…‹ã®å¾“æ¥­å“¡åˆ—å", value="æ‹…å½“æ‰€å“¡")
    att_name_col = st.text_input("å‹¤æ€ ã®å¾“æ¥­å“¡åˆ—å", value="åå‰")
    generate_diagnostics = st.checkbox("è¨ºæ–­CSVã‚’å‡ºåŠ›ã™ã‚‹", value=True)

# ã‚¿ãƒ–ã®ä½œæˆ
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
    "ğŸ“Š æ¤œå‡ºã‚µãƒãƒªãƒ¼",
    "ğŸ“‹ è©³ç´°ãƒ‡ãƒ¼ã‚¿",
    "ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»æœ€é©åŒ–",
    "ğŸ¯ æœ€é©å‹¤æ€ ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›"
])

# ã‚¿ãƒ–1: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
with tab1:
    st.header("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    
    st.subheader("1. æ–½è¨­ã”ã¨ã®ã‚µãƒ¼ãƒ“ã‚¹å®Ÿæ…‹CSVï¼ˆè¤‡æ•°å¯ï¼‰")
    st.markdown("ã‚µãƒ¼ãƒ“ã‚¹å®Ÿæ…‹CSVï¼ˆA,B,C... ãªã©è¤‡æ•°ï¼‰")
    svc_files = st.file_uploader(
        "Drag and drop files here", 
        type=["csv"], 
        accept_multiple_files=True, 
        key="svc",
        help="Limit 200MB per file â€¢ CSV"
    )
    
    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®è¡¨ç¤º
    if svc_files:
        st.markdown("**ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«:**")
        for file in svc_files:
            file_size = len(file.getvalue()) / 1024  # KB
            if file_size < 1024:
                size_str = f"{file_size:.1f}KB"
            else:
                size_str = f"{file_size/1024:.1f}MB"
            st.write(f"â€¢ {file.name} ({size_str})")
    
    st.subheader("2. å‹¤æ€ å±¥æ­´CSVï¼ˆ1ä»¶ï¼‰")
    st.markdown("å‹¤æ€ å±¥æ­´CSV")
    att_file = st.file_uploader(
        "Drag and drop file here", 
        type=["csv"], 
        accept_multiple_files=False, 
        key="att",
        help="Limit 200MB per file â€¢ CSV"
    )
    
    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®è¡¨ç¤º
    if att_file:
        file_size = len(att_file.getvalue()) / 1024  # KB
        if file_size < 1024:
            size_str = f"{file_size:.1f}KB"
        else:
            size_str = f"{file_size/1024:.1f}MB"
        st.write(f"â€¢ {att_file.name} ({size_str})")
    
    st.markdown("---")
    
    # å®Ÿè¡Œãƒœã‚¿ãƒ³
    run = st.button("ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã™ã‚‹", type="primary", use_container_width=True)
    
    if run:
        if not svc_files:
            st.error("ã‚µãƒ¼ãƒ“ã‚¹å®Ÿæ…‹CSVã‚’1ä»¶ä»¥ä¸Šã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            st.stop()
        if not att_file:
            st.error("å‹¤æ€ å±¥æ­´CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        with st.spinner("å‡¦ç†ä¸­..."):
            # ä½œæ¥­ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            workdir = tempfile.mkdtemp(prefix=f"svc_att_check_{ts}_")
            indir = os.path.join(workdir, "input")
            os.makedirs(indir, exist_ok=True)

            # src.py ã‚’ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼ï¼ˆåŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹æƒ³å®šï¼‰
            # å®Ÿè¡Œç’°å¢ƒã§ã¯ã“ã®ã‚¢ãƒ—ãƒªã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã« src.py ã‚’ç½®ã„ã¦ãã ã•ã„ã€‚
            src_source = os.path.join(os.path.dirname(__file__), "src.py")
            src_target = os.path.join(workdir, "src.py")
            shutil.copyfile(src_source, src_target)

            # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            # æ–½è¨­CSVã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åã‚’ãã®ã¾ã¾ä½¿ã†ï¼ˆresult_*.csvã®æ–½è¨­åã«å½±éŸ¿ï¼‰
            st.info(f"ã‚µãƒ¼ãƒ“ã‚¹å®Ÿæ…‹CSVãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(svc_files)}")
            
            # åŒåãƒ•ã‚¡ã‚¤ãƒ«ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ã¨é€£ç•ªä»˜ä¸
            saved_files = {}
            saved_service_files = []
            
            for i, up in enumerate(svc_files):
                original_name = up.name
                base_name, ext = os.path.splitext(original_name)
                
                # åŒåãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯é€£ç•ªã‚’ä»˜ä¸
                if original_name in saved_files:
                    saved_files[original_name] += 1
                    new_name = f"{base_name}_{saved_files[original_name]}{ext}"
                else:
                    saved_files[original_name] = 0
                    new_name = original_name
                
                file_path = os.path.join(indir, new_name)
                try:
                    # save_upload_toã¯æ­£è¦åŒ–ã•ã‚ŒãŸãƒ‘ã‚¹ã¨ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿”ã™
                    actual_path, debug_info = save_upload_to(file_path, up)
                    
                    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
                    with st.expander(f"ğŸ”§ ãƒ‡ãƒãƒƒã‚°æƒ…å ±: {original_name}", expanded=False):
                        for info in debug_info:
                            st.write(f"â€¢ {info}")
                    
                    # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
                    if os.path.exists(actual_path):
                        file_size = os.path.getsize(actual_path)
                        st.success(f"âœ… ä¿å­˜æˆåŠŸ: {original_name} -> {os.path.basename(actual_path)} ({file_size} bytes)")
                        saved_service_files.append(os.path.basename(actual_path))
                    else:
                        st.error(f"âŒ ä¿å­˜å¤±æ•—: {original_name} -> ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        st.error(f"å®Ÿéš›ã®ãƒ‘ã‚¹: {actual_path}")
                        
                    # è¿½åŠ ã®ç¢ºèªï¼šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
                    st.info(f"ğŸ” ä¿å­˜ç›´å¾Œã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª ({i+1}/{len(svc_files)}):")
                    try:
                        files_in_dir = os.listdir(indir)
                        all_files = [f for f in files_in_dir]
                        # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„CSVãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º
                        csv_files_in_dir = [f for f in files_in_dir if f.lower().endswith('.csv')]
                        st.write(f"å…¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(all_files)}")
                        st.write(f"CSVãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(csv_files_in_dir)}")
                        
                        if all_files:
                            st.write("å…¨ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:")
                            for f in all_files:
                                f_path = os.path.join(indir, f)
                                f_size = os.path.getsize(f_path)
                                is_csv = f.lower().endswith('.csv')
                                st.write(f"  - {f} ({f_size} bytes) {'[CSV]' if is_csv else ''}")
                        else:
                            st.warning("ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼")
                            
                    except Exception as dir_e:
                        st.error(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèªã‚¨ãƒ©ãƒ¼: {str(dir_e)}")
                        
                except Exception as e:
                    st.error(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {original_name} -> {str(e)}")
            
            att_file_path = os.path.join(indir, att_file.name)
            try:
                # save_upload_toã¯æ­£è¦åŒ–ã•ã‚ŒãŸãƒ‘ã‚¹ã¨ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿”ã™
                actual_att_path, att_debug_info = save_upload_to(att_file_path, att_file)
                
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
                with st.expander(f"ğŸ”§ å‹¤æ€ å±¥æ­´CSVãƒ‡ãƒãƒƒã‚°æƒ…å ±", expanded=False):
                    for info in att_debug_info:
                        st.write(f"â€¢ {info}")
                
                if os.path.exists(actual_att_path):
                    file_size = os.path.getsize(actual_att_path)
                    st.success(f"âœ… å‹¤æ€ å±¥æ­´CSVä¿å­˜æˆåŠŸ: {os.path.basename(actual_att_path)} ({file_size} bytes)")
                else:
                    st.error(f"âŒ å‹¤æ€ å±¥æ­´CSVä¿å­˜å¤±æ•—: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    st.error(f"å®Ÿéš›ã®ãƒ‘ã‚¹: {actual_att_path}")
                    st.stop()
            except Exception as e:
                st.error(f"âŒ å‹¤æ€ å±¥æ­´CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
                st.stop()
            
            # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€çµ‚ç¢ºèª
            st.info("ğŸ“ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€çµ‚ç¢ºèª:")
            all_files = os.listdir(indir)
            # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„CSVãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º
            csv_files = [f for f in all_files if f.lower().endswith('.csv')]
            
            actual_service_files = []
            actual_attendance_files = []
            
            for csv_file in csv_files:
                file_path = os.path.join(indir, csv_file)
                file_size = os.path.getsize(file_path)
                st.write(f"â€¢ {csv_file} ({file_size} bytes)")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†é¡
                if csv_file == att_file.name:
                    actual_attendance_files.append(csv_file)
                else:
                    actual_service_files.append(csv_file)
            
            # ä¿å­˜çŠ¶æ³ã®æ¤œè¨¼
            st.info(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«åˆ†é¡çµæœ:")
            st.write(f"â€¢ ã‚µãƒ¼ãƒ“ã‚¹å®Ÿæ…‹CSVãƒ•ã‚¡ã‚¤ãƒ«: {len(actual_service_files)}ä»¶")
            for sf in actual_service_files:
                st.write(f"  - {sf}")
            st.write(f"â€¢ å‹¤æ€ å±¥æ­´CSVãƒ•ã‚¡ã‚¤ãƒ«: {len(actual_attendance_files)}ä»¶")
            for af in actual_attendance_files:
                st.write(f"  - {af}")
            
            if len(actual_service_files) == 0:
                st.error("âŒ ã‚µãƒ¼ãƒ“ã‚¹å®Ÿæ…‹CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒ1ã¤ã‚‚ä¿å­˜ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.error("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
                st.write(f"â€¢ æœŸå¾…ã•ã‚ŒãŸã‚µãƒ¼ãƒ“ã‚¹å®Ÿæ…‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(svc_files)}")
                st.write(f"â€¢ ä¿å­˜æˆåŠŸã¨å ±å‘Šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(saved_service_files)}")
                st.write(f"â€¢ å®Ÿéš›ã«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(actual_service_files)}")
                st.write(f"â€¢ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {indir}")
                st.stop()
            
            if len(actual_attendance_files) == 0:
                st.error("âŒ å‹¤æ€ å±¥æ­´CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.stop()

            # ã‚³ãƒãƒ³ãƒ‰çµ„ã¿ç«‹ã¦
            cmd = [sys.executable, src_target, "--input", indir, "--identical-prefer", identical_prefer, "--alt-delim", alt_delim]
            # å‹¤æ€ å±¥æ­´CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
            cmd += ["--attendance-file", att_file.name]
            if use_schedule_when_missing:
                cmd.append("--use-schedule-when-missing")
            if not generate_diagnostics:
                cmd.append("--no-diagnostics")
            # åˆ—åã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆæ—¢å®šã¨é•ã†å ´åˆã®ã¿æ¸¡ã™ï¼‰
            if service_staff_col and service_staff_col != "æ‹…å½“æ‰€å“¡":
                cmd += ["--service-staff-col", service_staff_col]
            if att_name_col and att_name_col != "åå‰":
                cmd += ["--att-name-col", att_name_col]

            # å®Ÿè¡Œ
            proc = subprocess.run(cmd, capture_output=True, text=True)
            if proc.returncode != 0:
                st.error("å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                
                # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º
                with st.expander("ğŸ” è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±", expanded=True):
                    if proc.stderr:
                        st.markdown("**æ¨™æº–ã‚¨ãƒ©ãƒ¼å‡ºåŠ› (stderr):**")
                        st.code(proc.stderr)
                    if proc.stdout:
                        st.markdown("**æ¨™æº–å‡ºåŠ› (stdout):**")
                        st.code(proc.stdout)
                    
                    st.markdown("**å®Ÿè¡Œã•ã‚ŒãŸã‚³ãƒãƒ³ãƒ‰:**")
                    st.code(" ".join(cmd))
                    
                    st.markdown("**å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:**")
                    try:
                        for file in os.listdir(indir):
                            if file.endswith('.csv'):
                                file_path = os.path.join(indir, file)
                                file_size = os.path.getsize(file_path)
                                st.write(f"â€¢ {file} ({file_size} bytes)")
                    except Exception as e:
                        st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã®å–å¾—ã«å¤±æ•—: {e}")
                
                st.stop()

            # å‡ºåŠ›ï¼ˆresult_*.csv ã¨ diagnosticsï¼‰ã‚’åé›†
            result_paths = []
            diagnostic_paths = []
            for root, dirs, files in os.walk(indir):
                for fn in files:
                    if fn.startswith("result_") and fn.endswith(".csv"):
                        result_paths.append(os.path.join(root, fn))
            diag_dir = os.path.join(indir, "diagnostics")
            if os.path.isdir(diag_dir):
                for fn in os.listdir(diag_dir):
                    diagnostic_paths.append(os.path.join(diag_dir, fn))

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«çµæœã‚’ä¿å­˜
            st.session_state.result_paths = result_paths
            st.session_state.diagnostic_paths = diagnostic_paths
            st.session_state.workdir = workdir
            st.session_state.processing_complete = True
            
            if result_paths:
                st.session_state.summary_df = collect_summary(result_paths)
            else:
                st.session_state.summary_df = None
            
            st.success("å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼ä»–ã®ã‚¿ãƒ–ã§çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

# ã‚¿ãƒ–2: æ¤œå‡ºã‚µãƒãƒªãƒ¼
with tab2:
    st.header("ğŸ“Š æ¤œå‡ºã‚µãƒãƒªãƒ¼")
    
    if st.session_state.processing_complete:
        if st.session_state.summary_df is not None:
            st.dataframe(st.session_state.summary_df, use_container_width=True)
        else:
            st.info("çµæœCSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ãƒ»ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
    else:
        st.info("ã¾ãšã€Œãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ã‚¿ãƒ–ã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# ã‚¿ãƒ–3: è©³ç´°ãƒ‡ãƒ¼ã‚¿
with tab3:
    st.header("ğŸ“‹ è©³ç´°ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚°ãƒªãƒƒãƒ‰è¡¨ç¤ºï¼‰")
    
    if st.session_state.processing_complete and st.session_state.result_paths:
        try:
            grid_df = prepare_grid_data(st.session_state.result_paths)
            
            if not grid_df.empty:
                # ãƒ¡ã‚¤ãƒ³ã®ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤ºã‚’æœ€å„ªå…ˆã§é…ç½®
                st.markdown("### ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰")
                
                # åŸºæœ¬çš„ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
                col_filter1, col_filter2, col_filter3 = st.columns(3)
                
                with col_filter1:
                    error_filter = st.selectbox(
                        "ã‚¨ãƒ©ãƒ¼ãƒ•ã‚£ãƒ«ã‚¿",
                        options=["ã™ã¹ã¦", "ã‚¨ãƒ©ãƒ¼ã®ã¿", "æ­£å¸¸ã®ã¿"],
                        index=0,
                        key="error_filter"
                    )
                
                with col_filter2:
                    category_filter = st.selectbox(
                        "ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿",
                        options=["ã™ã¹ã¦"] + [cat for cat in grid_df['ã‚«ãƒ†ã‚´ãƒª'].unique() if pd.notna(cat) and cat != ''],
                        index=0,
                        key="category_filter"
                    )
                
                with col_filter3:
                    # ç©ºã®ã‚«ãƒ©ãƒ ï¼ˆãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´ç”¨ï¼‰
                    st.write("")
                
                # å¾“æ¥­å“¡ã¨åˆ©ç”¨è€…ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§è¡¨ç¤ºï¼‰
                st.markdown("#### ğŸ‘¥ å¾“æ¥­å“¡ãƒ»åˆ©ç”¨è€…ãƒ•ã‚£ãƒ«ã‚¿")
                col_staff, col_user = st.columns(2)
                
                with col_staff:
                    available_staff = [staff for staff in grid_df['æ‹…å½“æ‰€å“¡'].dropna().unique() if staff != '']
                    selected_staff = st.multiselect(
                        "æ‹…å½“æ‰€å“¡ã§çµã‚Šè¾¼ã¿",
                        options=sorted(available_staff),
                        default=[],
                        key="staff_filter_main"
                    )
                
                with col_user:
                    available_users = [user for user in grid_df['åˆ©ç”¨è€…å'].dropna().unique() if user != '']
                    selected_users = st.multiselect(
                        "åˆ©ç”¨è€…ã§çµã‚Šè¾¼ã¿",
                        options=sorted(available_users),
                        default=[],
                        key="user_filter_main"
                    )
                
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†
                filtered_df = grid_df.copy()
                
                if error_filter == "ã‚¨ãƒ©ãƒ¼ã®ã¿":
                    filtered_df = filtered_df[filtered_df['ã‚¨ãƒ©ãƒ¼'] == 'â—¯']
                elif error_filter == "æ­£å¸¸ã®ã¿":
                    filtered_df = filtered_df[filtered_df['ã‚¨ãƒ©ãƒ¼'] != 'â—¯']
                
                if category_filter != "ã™ã¹ã¦":
                    filtered_df = filtered_df[filtered_df['ã‚«ãƒ†ã‚´ãƒª'] == category_filter]
                
                if selected_staff:
                    filtered_df = filtered_df[filtered_df['æ‹…å½“æ‰€å“¡'].isin(selected_staff)]
                
                if selected_users:
                    filtered_df = filtered_df[filtered_df['åˆ©ç”¨è€…å'].isin(selected_users)]
                
                # åŸºæœ¬çµ±è¨ˆæƒ…å ±ï¼ˆã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã«è¡¨ç¤ºï¼‰
                total_records = len(grid_df)
                error_records = len(grid_df[grid_df['ã‚¨ãƒ©ãƒ¼'] == 'â—¯'])
                filtered_records = len(filtered_df)
                
                col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)
                with col_stat1:
                    st.metric("ç·ä»¶æ•°", total_records)
                with col_stat2:
                    st.metric("ã‚¨ãƒ©ãƒ¼ä»¶æ•°", error_records)
                with col_stat3:
                    st.metric("è¡¨ç¤ºä»¶æ•°", filtered_records)
                with col_stat4:
                    error_rate = (error_records / total_records * 100) if total_records > 0 else 0
                    st.metric("ã‚¨ãƒ©ãƒ¼ç‡", f"{error_rate:.1f}%")
                
                # ãƒ¡ã‚¤ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤ºï¼ˆå¤§ããè¡¨ç¤ºï¼‰
                st.dataframe(
                    filtered_df,
                    use_container_width=True,
                    height=600,  # é«˜ã•ã‚’å¤§ããã—ã¦è¦‹ã‚„ã™ã
                    hide_index=True
                )
                
                # è¿½åŠ æ©Ÿèƒ½ã‚’ãƒˆã‚°ãƒ«ã§è¡¨ç¤º/éè¡¨ç¤º
                show_advanced = st.toggle("ğŸ”§ è©³ç´°æ©Ÿèƒ½ã‚’è¡¨ç¤º", value=False, key="show_advanced_features")
                
                if show_advanced:
                    st.markdown("---")
                    
                    # ãƒ“ãƒ¥ãƒ¼é¸æŠã‚»ã‚¯ã‚·ãƒ§ãƒ³
                    st.markdown("### ğŸ“Š è¡¨ç¤ºãƒ“ãƒ¥ãƒ¼é¸æŠ")
                    view_type = st.radio(
                        "è¡¨ç¤ºæ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„",
                        options=["å…¨ä½“è¡¨ç¤º", "åˆ©ç”¨è€…åˆ¥è¡¨ç¤º", "å¾“æ¥­å“¡åˆ¥è¡¨ç¤º", "è©³ç´°åˆ†æè¡¨ç¤º", "ã‚«ã‚¹ã‚¿ãƒ è¡¨ç¤º"],
                        horizontal=True,
                        key="view_type"
                    )
                    
                    # è©³ç´°çµ±è¨ˆæƒ…å ±
                    st.markdown("### ğŸ“ˆ è©³ç´°çµ±è¨ˆ")
                    col_detail1, col_detail2, col_detail3, col_detail4 = st.columns(4)
                    
                    with col_detail1:
                        unique_users = len(grid_df['åˆ©ç”¨è€…å'].dropna().unique())
                        st.metric("åˆ©ç”¨è€…æ•°", unique_users)
                    
                    with col_detail2:
                        # é‡è¤‡é–¢é€£çµ±è¨ˆ
                        overlap_records = len(grid_df[grid_df['é‡è¤‡æ™‚é–“'] > 0])
                        total_overlap_minutes = grid_df['é‡è¤‡æ™‚é–“'].sum()
                        st.metric("é‡è¤‡ä»¶æ•°", overlap_records)
                        st.metric("ç·é‡è¤‡æ™‚é–“", f"{total_overlap_minutes}åˆ†")
                    
                    with col_detail3:
                        # è¶…éé–¢é€£çµ±è¨ˆ
                        excess_records = len(grid_df[grid_df['è¶…éæ™‚é–“'] > 0])
                        total_excess_minutes = grid_df['è¶…éæ™‚é–“'].sum()
                        st.metric("è¶…éä»¶æ•°", excess_records)
                        st.metric("ç·è¶…éæ™‚é–“", f"{total_excess_minutes}åˆ†")
                    
                    with col_detail4:
                        # è·å“¡é–¢é€£çµ±è¨ˆ
                        unique_staff = len(grid_df['æ‹…å½“æ‰€å“¡'].dropna().unique())
                        st.metric("è·å“¡æ•°", unique_staff)
                
                    # ãƒ“ãƒ¥ãƒ¼åˆ¥ã®è©³ç´°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    if view_type == "åˆ©ç”¨è€…åˆ¥è¡¨ç¤º":
                        st.markdown("### ğŸ‘¥ åˆ©ç”¨è€…é¸æŠ")
                        available_users = [user for user in grid_df['åˆ©ç”¨è€…å'].dropna().unique() if user != '']
                        if available_users:
                            selected_users = st.multiselect(
                                "è¡¨ç¤ºã™ã‚‹åˆ©ç”¨è€…ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰",
                                options=sorted(available_users),
                                default=[],
                                key="user_filter_advanced"
                            )
                            if selected_users:
                                filtered_df = filtered_df[filtered_df['åˆ©ç”¨è€…å'].isin(selected_users)]
                        else:
                            st.warning("åˆ©ç”¨è€…ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                            
                    elif view_type == "å¾“æ¥­å“¡åˆ¥è¡¨ç¤º":
                        st.markdown("### ğŸ‘¨â€ğŸ’¼ å¾“æ¥­å“¡é¸æŠ")
                        available_staff = [staff for staff in grid_df['æ‹…å½“æ‰€å“¡'].dropna().unique() if staff != '']
                        if available_staff:
                            selected_staff = st.multiselect(
                                "è¡¨ç¤ºã™ã‚‹å¾“æ¥­å“¡ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰",
                                options=sorted(available_staff),
                                default=[],
                                key="staff_filter_advanced"
                            )
                            if selected_staff:
                                filtered_df = filtered_df[filtered_df['æ‹…å½“æ‰€å“¡'].isin(selected_staff)]
                        else:
                            st.warning("å¾“æ¥­å“¡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                            
                    elif view_type == "è©³ç´°åˆ†æè¡¨ç¤º":
                        st.markdown("### ğŸ” è©³ç´°åˆ†æè¡¨ç¤º")
                        
                        # åˆ†æã‚¿ã‚¤ãƒ—é¸æŠ
                        analysis_type = st.selectbox(
                            "åˆ†æã‚¿ã‚¤ãƒ—ã‚’é¸æŠ",
                            options=["é‡è¤‡åˆ†æ", "å‹¤æ€ è¶…éåˆ†æ", "æ™‚é–“å¸¯åˆ†æ", "è·å“¡è² è·åˆ†æ"],
                            key="analysis_type"
                        )
                        
                        if analysis_type == "é‡è¤‡åˆ†æ":
                            show_overlap_analysis(filtered_df)
                        elif analysis_type == "å‹¤æ€ è¶…éåˆ†æ":
                            show_attendance_excess_analysis(filtered_df)
                        elif analysis_type == "æ™‚é–“å¸¯åˆ†æ":
                            show_time_slot_analysis(filtered_df)
                        elif analysis_type == "è·å“¡è² è·åˆ†æ":
                            show_staff_workload_analysis(filtered_df)
                            
                    elif view_type == "ã‚«ã‚¹ã‚¿ãƒ è¡¨ç¤º":
                        st.markdown("### ğŸ”§ ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ«ã‚¿")
                        
                        # è¤‡åˆæ¡ä»¶ãƒ•ã‚£ãƒ«ã‚¿
                        col_custom1, col_custom2 = st.columns(2)
                        with col_custom1:
                            custom_users = st.multiselect(
                                "åˆ©ç”¨è€…é¸æŠ",
                                options=sorted([user for user in grid_df['åˆ©ç”¨è€…å'].dropna().unique() if user != '']),
                                key="custom_users"
                            )
                        with col_custom2:
                            custom_staff = st.multiselect(
                                "å¾“æ¥­å“¡é¸æŠ",
                                options=sorted([staff for staff in grid_df['æ‹…å½“æ‰€å“¡'].dropna().unique() if staff != '']),
                                key="custom_staff"
                            )
                        
                        # ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
                        if custom_users:
                            filtered_df = filtered_df[filtered_df['åˆ©ç”¨è€…å'].isin(custom_users)]
                        if custom_staff:
                            filtered_df = filtered_df[filtered_df['æ‹…å½“æ‰€å“¡'].isin(custom_staff)]
                    
                    # è©³ç´°æ©Ÿèƒ½ç”¨ã®ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤º
                    st.markdown("### ğŸ“‹ ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰")
                    st.dataframe(
                        filtered_df,
                        use_container_width=True,
                        height=400,
                        hide_index=True
                    )
                
            else:
                st.info("ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤ºç”¨ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        except Exception as e:
            st.error(f"ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤ºã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    else:
        st.info("ã¾ãšã€Œãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ã‚¿ãƒ–ã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# ã‚¿ãƒ–4: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»æœ€é©åŒ–
with tab4:
    st.header("ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»æœ€é©åŒ–")
    
    if st.session_state.processing_complete:
        # å€‹åˆ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if st.session_state.result_paths:
            st.subheader("ğŸ“¥ çµæœCSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
            for p in sorted(st.session_state.result_paths):
                with open(p, "rb") as f:
                    st.download_button(
                        label=f"Download {os.path.basename(p)}",
                        data=f.read(),
                        file_name=os.path.basename(p),
                        mime="text/csv",
                    )

        # è¨ºæ–­ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if st.session_state.diagnostic_paths and generate_diagnostics:
            st.subheader("ğŸ” è¨ºæ–­CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (diagnostics)")
            for p in sorted(st.session_state.diagnostic_paths):
                with open(p, "rb") as f:
                    st.download_button(
                        label=f"Download {os.path.basename(p)}",
                        data=f.read(),
                        file_name=os.path.basename(p),
                        mime="text/csv",
                    )

        # ã¾ã¨ã‚ã¦ZIP
        if st.session_state.result_paths:
            st.subheader("ğŸ“¦ ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆZIPï¼‰")
            buf = io.BytesIO()
            with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as zf:
                for p in st.session_state.result_paths:
                    zf.write(p, arcname=os.path.basename(p))
                if generate_diagnostics and st.session_state.diagnostic_paths:
                    for p in st.session_state.diagnostic_paths:
                        zf.write(p, arcname=os.path.join("diagnostics", os.path.basename(p)))
            buf.seek(0)
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            st.download_button(
                label="çµæœä¸€å¼ã‚’ZIPã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=buf,
                file_name=f"results_{ts}.zip",
                mime="application/zip",
            )

    else:
        st.info("ã¾ãšã€Œãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ã‚¿ãƒ–ã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# ã‚¿ãƒ–5: æœ€é©å‹¤æ€ ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›
with tab5:
    st.header("ğŸ¯ æœ€é©å‹¤æ€ ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›")
    
    # å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ç¢ºèª
    try:
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰å‹¤æ€ å±¥æ­´CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å‹•çš„ã«å–å¾—
        if st.session_state.processing_complete and st.session_state.workdir:
            # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®inputãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰å‹¤æ€ å±¥æ­´CSVã‚’æ¢ã™
            input_dir = os.path.join(st.session_state.workdir, "input")
            attendance_file_path = None
            
            if os.path.exists(input_dir):
                for filename in os.listdir(input_dir):
                    if filename.endswith('.csv') and not filename.startswith('result_'):
                        # ã‚µãƒ¼ãƒ“ã‚¹å®Ÿæ…‹CSVã§ã¯ãªã„å¯èƒ½æ€§ãŒé«˜ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‹¤æ€ å±¥æ­´CSVã¨ã—ã¦åˆ¤å®š
                        if 'å‹¤æ€ ' in filename or 'ãƒã‚§ãƒƒã‚¯' in filename or 'attendance' in filename.lower():
                            attendance_file_path = os.path.join(input_dir, filename)
                            break
                
                # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€æœ€åˆã«è¦‹ã¤ã‹ã£ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
                if not attendance_file_path:
                    csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv') and not f.startswith('result_')]
                    if csv_files:
                        # ã‚µãƒ¼ãƒ“ã‚¹å®Ÿæ…‹CSVãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’æ¨å®šã—ã¦ã€æ®‹ã‚Šã‚’å‹¤æ€ å±¥æ­´CSVã¨ã™ã‚‹
                        service_files = [f for f in csv_files if not ('å‹¤æ€ ' in f or 'ãƒã‚§ãƒƒã‚¯' in f or 'attendance' in f.lower())]
                        attendance_files = [f for f in csv_files if f not in service_files]
                        if attendance_files:
                            attendance_file_path = os.path.join(input_dir, attendance_files[0])
            
            if not attendance_file_path:
                raise FileNotFoundError("å‹¤æ€ å±¥æ­´CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        else:
            raise FileNotFoundError("å‡¦ç†ãŒå®Œäº†ã—ã¦ã„ãªã„ã‹ã€ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        
        attendance_df = pd.read_csv(attendance_file_path, encoding='cp932')
        
        # åˆ©ç”¨å¯èƒ½ãªå¾“æ¥­å“¡ãƒªã‚¹ãƒˆã‚’å–å¾—
        available_employees = []
        for _, row in attendance_df.iterrows():
            emp_name = str(row.get('åå‰', '')).strip()
            if emp_name and emp_name not in available_employees:
                available_employees.append(emp_name)
        
        if not available_employees:
            st.error("å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¾“æ¥­å“¡æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            st.success(f"å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚åˆ©ç”¨å¯èƒ½ãªå¾“æ¥­å“¡: {len(available_employees)}å")
            
            # å¯¾è±¡æœˆã®é¸æŠ
            col1, col2 = st.columns(2)
            with col1:
                target_year = st.selectbox("å¯¾è±¡å¹´", range(2023, 2026), index=2, key="export_year")
            with col2:
                target_month = st.selectbox("å¯¾è±¡æœˆ", range(1, 13), index=datetime.now().month - 1, key="export_month")
            
            target_month_str = f"{target_year}-{target_month:02d}"
            
            # å¾“æ¥­å“¡é¸æŠ
            st.markdown("### ğŸ‘¥ å‡ºåŠ›å¯¾è±¡å¾“æ¥­å“¡ã®é¸æŠ")
            
            # å…¨é¸æŠãƒ»å…¨è§£é™¤ãƒœã‚¿ãƒ³
            col1, col2, col3 = st.columns([1, 1, 2])
            with col1:
                if st.button("å…¨å“¡é¸æŠ", key="select_all_export"):
                    st.session_state.selected_employees_export = available_employees.copy()
                    st.rerun()
            with col2:
                if st.button("é¸æŠè§£é™¤", key="clear_all_export"):
                    st.session_state.selected_employees_export = []
                    st.rerun()
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
            if 'selected_employees_export' not in st.session_state:
                st.session_state.selected_employees_export = []
            
            # å¾“æ¥­å“¡ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹
            st.markdown("#### ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã§å¾“æ¥­å“¡ã‚’é¸æŠã—ã¦ãã ã•ã„")
            
            # 3åˆ—ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã§ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚’è¡¨ç¤º
            cols = st.columns(3)
            for i, employee in enumerate(sorted(available_employees)):
                with cols[i % 3]:
                    is_selected = employee in st.session_state.selected_employees_export
                    if st.checkbox(employee, value=is_selected, key=f"emp_check_tab5_{i}"):
                        if employee not in st.session_state.selected_employees_export:
                            st.session_state.selected_employees_export.append(employee)
                    else:
                        if employee in st.session_state.selected_employees_export:
                            st.session_state.selected_employees_export.remove(employee)
            
            # é¸æŠã•ã‚ŒãŸå¾“æ¥­å“¡ã®è¡¨ç¤º
            if st.session_state.selected_employees_export:
                st.info(f"é¸æŠã•ã‚ŒãŸå¾“æ¥­å“¡: {len(st.session_state.selected_employees_export)}å")
                with st.expander("é¸æŠã•ã‚ŒãŸå¾“æ¥­å“¡ä¸€è¦§"):
                    for i, emp in enumerate(st.session_state.selected_employees_export, 1):
                        st.write(f"{i}. {emp}")
            
            # CSVå‡ºåŠ›ãƒœã‚¿ãƒ³
            if st.session_state.selected_employees_export:
                st.markdown("### ğŸ“¥ CSVå‡ºåŠ›")
                
                if st.button("ğŸ¯ æœ€é©å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‚’CSVå‡ºåŠ›", type="primary", key="export_csv_tab5"):
                    with st.spinner("CSVç”Ÿæˆä¸­..."):
                        try:
                            # jinjerå½¢å¼CSVã‚’ç”Ÿæˆ
                            csv_content = generate_jinjer_csv(
                                st.session_state.selected_employees_export,
                                target_month_str,
                                attendance_df
                            )
                            
                            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            filename = f"æœ€é©å‹¤æ€ ãƒ‡ãƒ¼ã‚¿_{target_month_str}_{timestamp}.csv"
                            
                            st.download_button(
                                label="ğŸ“¥ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                data=csv_content.encode('shift_jis', errors='ignore'),
                                file_name=filename,
                                mime="text/csv",
                                help="jinjerå½¢å¼ï¼ˆ133åˆ—ï¼‰ã®æœ€é©å‹¤æ€ ãƒ‡ãƒ¼ã‚¿CSVãƒ•ã‚¡ã‚¤ãƒ«",
                                key="download_csv_tab5"
                            )
                            
                            st.success(f"âœ… CSVç”Ÿæˆå®Œäº†ï¼{len(st.session_state.selected_employees_export)}åã®å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›ã—ã¾ã—ãŸã€‚")
                            
                            # ç”Ÿæˆã•ã‚ŒãŸCSVã®è©³ç´°æƒ…å ±
                            lines = csv_content.count('\n') - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’é™¤ã
                            st.info(f"ğŸ“Š å‡ºåŠ›è©³ç´°: {lines}è¡Œã®ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼å«ã‚€{lines + 1}è¡Œï¼‰")
                            
                        except Exception as e:
                            st.error(f"CSVç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            else:
                st.warning("å‡ºåŠ›å¯¾è±¡ã®å¾“æ¥­å“¡ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
                
    except FileNotFoundError as e:
        st.error("å‹¤æ€ å±¥æ­´CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        st.info("ã¾ãšã€Œãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ã‚¿ãƒ–ã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        with st.expander("ğŸ” è©³ç´°æƒ…å ±"):
            st.write(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")
            if st.session_state.workdir:
                st.write(f"ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {st.session_state.workdir}")
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        with st.expander("ğŸ” è©³ç´°æƒ…å ±"):
            st.write(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
            st.write(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")

st.caption("â€» ã“ã®UIã¯ãƒ­ãƒ¼ã‚«ãƒ«ã® src.py ã‚’å‘¼ã³å‡ºã—ã¦å®Ÿè¡Œã—ã¾ã™ã€‚ã‚¢ãƒ—ãƒªã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã« src.py ã‚’ç½®ã„ã¦ãã ã•ã„ã€‚")
