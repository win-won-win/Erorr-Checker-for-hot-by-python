import streamlit as st
import pandas as pd
import tempfile
import os
import io
import zipfile
import shutil
import subprocess
import sys
import re
import json
import hashlib
import uuid
from datetime import datetime
import calendar
from typing import List, Dict, Any, Optional, Tuple
from functools import lru_cache
from pathlib import Path
from st_aggrid import AgGrid, GridOptionsBuilder, GridUpdateMode, DataReturnMode, JsCode
from src import normalize_name

try:
    from PIL import Image
except ImportError:
    Image = None  # Pillowæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ™‚ã¯ç”»åƒã‚’ä½¿ã‚ãªã„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

# ã‚¢ã‚¤ã‚³ãƒ³å®šç¾©ï¼ˆæ´—ç·´ã•ã‚ŒãŸUnicodeã‚¢ã‚¤ã‚³ãƒ³ï¼‰
ICONS = {
    'folder': 'ğŸ—‚ï¸',      # ãƒ•ã‚©ãƒ«ãƒ€
    'chart': 'ğŸ“ˆ',       # ãƒãƒ£ãƒ¼ãƒˆ
    'list': 'ğŸ“',        # ãƒªã‚¹ãƒˆ
    'download': 'â¬‡ï¸',     # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    'target': 'ğŸ¯',      # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
    'settings': 'âš™ï¸',    # è¨­å®š
    'search': 'ğŸ”',      # æ¤œç´¢
    'info': 'â„¹ï¸',        # æƒ…å ±
    'warning': 'âš ï¸',     # è­¦å‘Š
    'error': 'ğŸš«',       # ã‚¨ãƒ©ãƒ¼
    'success': 'âœ…',     # æˆåŠŸ
    'debug': 'ğŸ› ï¸',       # ãƒ‡ãƒãƒƒã‚°
    'users': 'ğŸ‘¥',       # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¤‡æ•°
    'user': 'ğŸ‘¤',        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å˜ä½“
    'staff': 'ğŸ‘¨â€ğŸ’¼',      # ã‚¹ã‚¿ãƒƒãƒ•
    'time': 'ğŸ•',        # æ™‚é–“
    'calendar': 'ğŸ“…',    # ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼
    'file': 'ğŸ“„',        # ãƒ•ã‚¡ã‚¤ãƒ«
    'zip': 'ğŸ—œï¸'          # ZIP
}

# optimal_attendance_export.pyã®æ©Ÿèƒ½ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from optimal_attendance_export import (
    create_jinjer_headers,
    time_to_minutes,
    minutes_to_time,
    format_time_for_csv,
    merge_overlapping_shifts,
    get_employee_id,
    generate_jinjer_csv,
    show_optimal_attendance_export,
    find_default_attendance_csv,
    build_builtin_attendance_dataframe,
    get_builtin_attendance_csv_bytes
)

# è©³ç´°åˆ†ææ©Ÿèƒ½ï¼ˆé–¢æ•°å®šç¾©ï¼‰
def show_overlap_analysis(df):
    """é‡è¤‡ã®è©³ç´°åˆ†æã‚’è¡¨ç¤º"""
    st.markdown("#### é‡è¤‡åˆ†æ")
    
    # ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª
    if 'H' not in df.columns:
        st.warning("é‡è¤‡æ™‚é–“ï¼ˆåˆ†ï¼‰ã®ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ã‚’å†ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")
        return
    
    # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    overlap_data = df[df['H'].notna() & (pd.to_numeric(df['H'], errors='coerce') > 0)]
    
    if overlap_data.empty:
        st.info("é‡è¤‡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    # é‡è¤‡çµ±è¨ˆ
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("é‡è¤‡ä»¶æ•°", len(overlap_data))
    with col2:
        total_overlap_minutes = pd.to_numeric(overlap_data['H'], errors='coerce').sum()
        st.metric("ç·é‡è¤‡æ™‚é–“", f"{total_overlap_minutes:.0f}åˆ†")
    with col3:
        avg_overlap = pd.to_numeric(overlap_data['H'], errors='coerce').mean()
        st.metric("å¹³å‡é‡è¤‡æ™‚é–“", f"{avg_overlap:.1f}åˆ†")
    with col4:
        max_overlap = pd.to_numeric(overlap_data['H'], errors='coerce').max()
        st.metric("æœ€å¤§é‡è¤‡æ™‚é–“", f"{max_overlap:.0f}åˆ†")
    
    # é‡è¤‡ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
    st.markdown("##### é‡è¤‡ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ")
    if 'L' in overlap_data.columns and overlap_data['L'].notna().any():
        valid_type_data = overlap_data[overlap_data['L'].notna() & (overlap_data['L'] != '')]
        if not valid_type_data.empty:
            overlap_type_stats = valid_type_data.groupby('L').agg({
                'H': lambda x: len(x),  # ä»¶æ•°
                'C': 'nunique'  # é–¢ä¸è·å“¡æ•°
            })
            overlap_type_stats.columns = ['ä»¶æ•°', 'é–¢ä¸è·å“¡æ•°']
            st.dataframe(overlap_type_stats, use_container_width=True)
        else:
            st.info("é‡è¤‡ã‚¿ã‚¤ãƒ—ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        st.info("é‡è¤‡ã‚¿ã‚¤ãƒ—ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

def show_attendance_excess_analysis(df):
    """å‹¤æ€ è¶…éã®è©³ç´°åˆ†æã‚’è¡¨ç¤º"""
    st.markdown("#### å‹¤æ€ è¶…éåˆ†æ")
    
    # ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª
    if 'I' not in df.columns:
        st.warning("è¶…éæ™‚é–“ï¼ˆåˆ†ï¼‰ã®ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ã‚’å†ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")
        return
    
    # è¶…éãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    excess_data = df[df['I'].notna() & (pd.to_numeric(df['I'], errors='coerce') > 0)]
    
    if excess_data.empty:
        st.info("å‹¤æ€ è¶…éãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    # è¶…éçµ±è¨ˆ
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("è¶…éä»¶æ•°", len(excess_data))
    with col2:
        total_excess_minutes = pd.to_numeric(excess_data['I'], errors='coerce').sum()
        st.metric("ç·è¶…éæ™‚é–“", f"{total_excess_minutes:.0f}åˆ†")
    with col3:
        avg_excess = pd.to_numeric(excess_data['I'], errors='coerce').mean()
        st.metric("å¹³å‡è¶…éæ™‚é–“", f"{avg_excess:.1f}åˆ†")
    with col4:
        max_excess = pd.to_numeric(excess_data['I'], errors='coerce').max()
        st.metric("æœ€å¤§è¶…éæ™‚é–“", f"{max_excess:.0f}åˆ†")
    
    # ã‚«ãƒãƒ¼çŠ¶æ³åˆ¥é›†è¨ˆ
    st.markdown("##### ã‚«ãƒãƒ¼çŠ¶æ³åˆ¥é›†è¨ˆ")
    if 'M' in excess_data.columns and excess_data['M'].notna().any():
        valid_coverage_data = excess_data[excess_data['M'].notna() & (excess_data['M'] != '')]
        if not valid_coverage_data.empty:
            coverage_stats = valid_coverage_data.groupby('M').agg({
                'I': lambda x: len(x),  # ä»¶æ•°
                'C': 'nunique'  # é–¢ä¸è·å“¡æ•°
            })
            coverage_stats.columns = ['ä»¶æ•°', 'é–¢ä¸è·å“¡æ•°']
            st.dataframe(coverage_stats, use_container_width=True)
        else:
            st.info("ã‚«ãƒãƒ¼çŠ¶æ³ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        st.info("ã‚«ãƒãƒ¼çŠ¶æ³ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    
    # è·å“¡åˆ¥è¶…éãƒ©ãƒ³ã‚­ãƒ³ã‚°
    st.markdown("##### è·å“¡åˆ¥è¶…éæ™‚é–“ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä¸Šä½10åï¼‰")
    if 'C' in excess_data.columns:
        staff_excess = excess_data.groupby('C').agg({
            'I': lambda x: len(x),  # è¶…éä»¶æ•°
            'C': 'first'  # è·å“¡å
        })
        staff_excess.columns = ['è¶…éä»¶æ•°', 'è·å“¡å']
        staff_excess = staff_excess.sort_values('è¶…éä»¶æ•°', ascending=False).head(10)
        st.dataframe(staff_excess[['è¶…éä»¶æ•°']], use_container_width=True)
    else:
        st.info("è·å“¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

def show_time_slot_analysis(df):
    """æ™‚é–“å¸¯åˆ†æã‚’è¡¨ç¤º"""
    st.markdown("#### æ™‚é–“å¸¯åˆ†æ")
    
    # æ™‚é–“å¸¯åˆ¥ã®ã‚¨ãƒ©ãƒ¼åˆ†å¸ƒ
    if 'E' in df.columns and df['E'].notna().any():
        # é–‹å§‹æ™‚é–“ã‹ã‚‰æ™‚é–“å¸¯ã‚’æŠ½å‡º
        df_copy = df.copy()
        df_copy['æ™‚é–“å¸¯'] = df_copy['E'].apply(lambda x: f"{str(x)[:2]}:00" if pd.notna(x) and str(x) else "ä¸æ˜")
        
        time_slot_stats = df_copy.groupby('æ™‚é–“å¸¯').agg({
            'A': lambda x: (x == 'â—¯').sum(),  # ã‚¨ãƒ©ãƒ¼ä»¶æ•°
            'C': 'count',  # ç·ä»¶æ•°
            'H': 'sum',  # é‡è¤‡æ™‚é–“åˆè¨ˆ
            'I': 'sum'   # è¶…éæ™‚é–“åˆè¨ˆ
        }).round(1)
        time_slot_stats.columns = ['ã‚¨ãƒ©ãƒ¼ä»¶æ•°', 'ç·ä»¶æ•°', 'é‡è¤‡æ™‚é–“åˆè¨ˆ(åˆ†)', 'è¶…éæ™‚é–“åˆè¨ˆ(åˆ†)']
        time_slot_stats['ã‚¨ãƒ©ãƒ¼ç‡'] = (time_slot_stats['ã‚¨ãƒ©ãƒ¼ä»¶æ•°'] / time_slot_stats['ç·ä»¶æ•°'] * 100).round(1)
        
        st.dataframe(time_slot_stats, use_container_width=True)
    else:
        st.info("æ™‚é–“å¸¯åˆ†æç”¨ã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")

def show_staff_workload_analysis(df):
    """è·å“¡è² è·åˆ†æã‚’è¡¨ç¤º"""
    st.markdown("#### è·å“¡è² è·åˆ†æ")
    
    if 'C' in df.columns and df['C'].notna().any():
        staff_workload = df.groupby('C').agg({
            'A': lambda x: (x == 'â—¯').sum(),  # ã‚¨ãƒ©ãƒ¼ä»¶æ•°
            'C': 'count',  # ç·ä»¶æ•°
            'H': 'sum',  # é‡è¤‡æ™‚é–“åˆè¨ˆ
            'I': 'sum',  # è¶…éæ™‚é–“åˆè¨ˆ
            'N': 'mean'  # å¹³å‡å‹¤å‹™åŒºé–“æ•°
        }).round(1)
        staff_workload.columns = ['ã‚¨ãƒ©ãƒ¼ä»¶æ•°', 'ç·ä»¶æ•°', 'é‡è¤‡æ™‚é–“åˆè¨ˆ(åˆ†)', 'è¶…éæ™‚é–“åˆè¨ˆ(åˆ†)', 'å¹³å‡å‹¤å‹™åŒºé–“æ•°']
        staff_workload['ã‚¨ãƒ©ãƒ¼ç‡'] = (staff_workload['ã‚¨ãƒ©ãƒ¼ä»¶æ•°'] / staff_workload['ç·ä»¶æ•°'] * 100).round(1)
        staff_workload = staff_workload.sort_values('ã‚¨ãƒ©ãƒ¼ä»¶æ•°', ascending=False)
        
        st.dataframe(staff_workload, use_container_width=True)
    else:
        st.info("è·å“¡è² è·åˆ†æç”¨ã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")

def show_row_detail_modal(row):
    """é¸æŠã•ã‚ŒãŸè¡Œã®è©³ç´°æƒ…å ±ã‚’ãƒ¢ãƒ¼ãƒ€ãƒ«é¢¨ã«è¡¨ç¤º"""
    with st.expander(f"è©³ç´°æƒ…å ± - {row.get('C', 'N/A')} ({row.get('D', 'N/A')})", expanded=True):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**åŸºæœ¬æƒ…å ±**")
            st.write(f"â€¢ ã‚¨ãƒ©ãƒ¼: {row.get('A', 'N/A')}")
            st.write(f"â€¢ ã‚«ãƒ†ã‚´ãƒª: {row.get('B', 'N/A')}")
            st.write(f"â€¢ æ‹…å½“æ‰€å“¡: {row.get('C', 'N/A')}")
            st.write(f"â€¢ æ—¥ä»˜: {row.get('D', 'N/A')}")
            st.write(f"â€¢ æ™‚é–“: {row.get('E', 'N/A')} - {row.get('F', 'N/A')}")
        
        with col2:
            st.markdown("**è©³ç´°æƒ…å ±**")
            if pd.notna(row.get('H', 0)) and row.get('H', 0) > 0:
                st.write(f"â€¢ é‡è¤‡æ™‚é–“: {row.get('H', 'N/A')}åˆ†")
                st.write(f"â€¢ é‡è¤‡ç›¸æ‰‹æ–½è¨­: {row.get('J', 'N/A')}")
            if pd.notna(row.get('I', 0)) and row.get('I', 0) > 0:
                st.write(f"â€¢ è¶…éæ™‚é–“: {row.get('I', 'N/A')}åˆ†")

def save_upload_to(path: str, uploaded_file):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã«ä¿å­˜ã™ã‚‹"""
    import time
    import stat
    import tempfile
    
    debug_info = []
    
    try:
        debug_info.append(f"é–‹å§‹: path={path}")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        dir_path = os.path.dirname(path)
        debug_info.append(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹: {dir_path}")
        
        os.makedirs(dir_path, exist_ok=True)
        debug_info.append(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆå®Œäº†: å­˜åœ¨={os.path.exists(dir_path)}")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ¨©é™ã‚’ç¢ºèªãƒ»è¨­å®š
        try:
            os.chmod(dir_path, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)
            debug_info.append("ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¨©é™è¨­å®šå®Œäº†")
        except Exception as e:
            debug_info.append(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¨©é™è¨­å®šå¤±æ•—: {e}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ç‰¹æ®Šæ–‡å­—ã‚’æ­£è¦åŒ–ï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ãƒœãƒƒã‚¯ã‚¹å¯¾å¿œï¼‰
        import unicodedata
        normalized_path = unicodedata.normalize('NFC', path)
        debug_info.append(f"æ­£è¦åŒ–ãƒ‘ã‚¹: {normalized_path}")
        
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°å‰Šé™¤
        if os.path.exists(normalized_path):
            os.remove(normalized_path)
            debug_info.append("æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å®Œäº†")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        data = uploaded_file.getbuffer()
        expected_size = len(data)
        debug_info.append(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {expected_size} bytes")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚ˆã‚Šç¢ºå®Ÿã«ä¿å­˜
        temp_path = normalized_path + ".tmp"
        debug_info.append(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: {temp_path}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿
        with open(temp_path, "wb") as f:
            f.write(data)
            f.flush()  # ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
            os.fsync(f.fileno())  # ãƒ‡ã‚£ã‚¹ã‚¯ã«å¼·åˆ¶æ›¸ãè¾¼ã¿
        
        debug_info.append("ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿å®Œäº†")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
        if not os.path.exists(temp_path):
            raise Exception(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—: {temp_path}")
        
        temp_size = os.path.getsize(temp_path)
        debug_info.append(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {temp_size} bytes")
        
        if temp_size != expected_size:
            raise Exception(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒä¸æ­£: æœŸå¾…å€¤={expected_size}, å®Ÿéš›={temp_size}")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ç§»å‹•
        import shutil
        shutil.move(temp_path, normalized_path)
        debug_info.append("ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•å®Œäº†")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™ã‚’è¨­å®š
        try:
            os.chmod(normalized_path, stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH)
            debug_info.append("ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™è¨­å®šå®Œäº†")
        except Exception as e:
            debug_info.append(f"ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™è¨­å®šå¤±æ•—: {e}")
        
        # æœ€çµ‚ç¢ºèª
        time.sleep(0.5)  # 500mså¾…æ©Ÿ
        
        if not os.path.exists(normalized_path):
            raise Exception(f"æœ€çµ‚ç¢ºèªã§ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {normalized_path}")
        
        actual_size = os.path.getsize(normalized_path)
        debug_info.append(f"æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {actual_size} bytes")
        
        if actual_size == 0:
            raise Exception(f"ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µã‚¤ã‚ºãŒ0ã§ã™: {normalized_path}")
        
        if actual_size != expected_size:
            raise Exception(f"æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒä¸€è‡´ã—ã¾ã›ã‚“: æœŸå¾…å€¤={expected_size}, å®Ÿéš›={actual_size}")
        
        debug_info.append("ä¿å­˜å‡¦ç†å®Œäº†")
        return normalized_path, debug_info
            
    except Exception as e:
        debug_info.append(f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")
        raise Exception(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼ ({path}): {str(e)}\nãƒ‡ãƒãƒƒã‚°æƒ…å ±: {'; '.join(debug_info)}")

def resolve_upload_cache_dir() -> Tuple[Path, Path, str]:
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å…ˆã‚’æ±ºå®šã™ã‚‹ã€‚
    1. Desktop/ChofukuChecker_upload_cache ã‚’å„ªå…ˆã—ã¦ä½œæˆãƒ»ä½¿ç”¨ã€‚
    2. Desktop ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ãƒ›ãƒ¼ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã«éš ã—ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç”¨æ„ã—ã¦ä½¿ç”¨ã€‚
    """
    desktop_dir = Path.home() / "Desktop" / "ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯é¸æŠè¨˜æ†¶ç”¨ãƒ•ã‚¡ã‚¤ãƒ«"
    try:
        desktop_dir.mkdir(parents=True, exist_ok=True)
        display_path = str(desktop_dir)
        return desktop_dir, desktop_dir / "manifest.json", display_path
    except Exception:
        pass

    fallback_dir = Path.home() / ".streamlit_upload_cache"
    fallback_dir.mkdir(parents=True, exist_ok=True)
    display_path = str(fallback_dir)
    return fallback_dir, fallback_dir / "manifest.json", display_path


UPLOAD_CACHE_DIR, UPLOAD_CACHE_MANIFEST, _upload_cache_display_raw = resolve_upload_cache_dir()
_home_prefix = str(Path.home())
if _upload_cache_display_raw.startswith(_home_prefix):
    UPLOAD_CACHE_DISPLAY = "~" + _upload_cache_display_raw[len(_home_prefix):]
else:
    UPLOAD_CACHE_DISPLAY = _upload_cache_display_raw


def _default_upload_manifest() -> Dict[str, List[Dict[str, Any]]]:
    return {"service": [], "attendance": []}


def ensure_upload_cache_dir() -> None:
    UPLOAD_CACHE_DIR.mkdir(parents=True, exist_ok=True)


def read_upload_manifest() -> Dict[str, List[Dict[str, Any]]]:
    ensure_upload_cache_dir()
    if not UPLOAD_CACHE_MANIFEST.exists():
        write_upload_manifest(_default_upload_manifest())
    try:
        with open(UPLOAD_CACHE_MANIFEST, "r", encoding="utf-8") as f:
            data = json.load(f)
    except (json.JSONDecodeError, FileNotFoundError):
        data = _default_upload_manifest()
        write_upload_manifest(data)
    for key in ["service", "attendance"]:
        if key not in data or not isinstance(data.get(key), list):
            data[key] = []
    return data


def write_upload_manifest(data: Dict[str, List[Dict[str, Any]]]) -> None:
    ensure_upload_cache_dir()
    with open(UPLOAD_CACHE_MANIFEST, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


class CachedUploadedFile:
    """st.file_uploaderã§é¸æŠã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å†ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼"""

    def __init__(self, name: str, stored_path: Path, size: Optional[int] = None, saved_at: Optional[str] = None, stored_name: Optional[str] = None):
        self.name = name
        self._stored_path = stored_path
        self._size = size
        self.saved_at = saved_at
        self.stored_name = stored_name or stored_path.name
        self._data: Optional[bytes] = None

    def _load(self) -> bytes:
        if self._data is None:
            self._data = self._stored_path.read_bytes()
            self._size = len(self._data)
        return self._data

    def getvalue(self) -> bytes:
        return self._load()

    def getbuffer(self):
        return memoryview(self._load())

    def read(self) -> bytes:
        return self._load()

    @property
    def size(self) -> int:
        if self._size is None:
            try:
                self._size = self._stored_path.stat().st_size
            except FileNotFoundError:
                self._size = 0
        return self._size

    def __len__(self) -> int:
        return self.size


def save_uploaded_files_to_cache(category: str, uploaded_files: List[Any]) -> None:
    if not uploaded_files:
        return

    manifest = read_upload_manifest()
    existing_entries = list(manifest.get(category, []))
    ensure_upload_cache_dir()

    for uploaded in uploaded_files:
        safe_name = os.path.basename(getattr(uploaded, "name", "uploaded.csv"))
        file_bytes = uploaded.getvalue()
        digest = hashlib.sha1(file_bytes).hexdigest()
        ext = os.path.splitext(safe_name)[1]
        stored_basename = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{uuid.uuid4().hex}_{digest[:8]}"
        stored_name = f"{stored_basename}{ext}" if ext else stored_basename
        stored_path = UPLOAD_CACHE_DIR / stored_name
        if stored_path.exists():
            # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã¨è¡çªã™ã‚‹ã¯ãšãŒãªã„ãŒå¿µã®ãŸã‚å‰Šé™¤
            stored_path.unlink()

        with open(stored_path, "wb") as f:
            f.write(file_bytes)

        # åŒåã®æ—¢å­˜ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤ã—ã¦å·®ã—æ›¿ãˆã‚‹
        filtered_entries = []
        for entry in existing_entries:
            if entry.get("name") == safe_name:
                old_stored = entry.get("stored_name")
                if old_stored:
                    old_path = UPLOAD_CACHE_DIR / old_stored
                    try:
                        old_path.unlink()
                    except FileNotFoundError:
                        pass
                continue
            filtered_entries.append(entry)
        existing_entries = filtered_entries

        existing_entries.append({
            "name": safe_name,
            "stored_name": stored_name,
            "size": len(file_bytes),
            "saved_at": datetime.now().isoformat()
        })

    manifest[category] = existing_entries
    write_upload_manifest(manifest)


def load_cached_uploaded_files(category: str) -> Tuple[List[CachedUploadedFile], List[Dict[str, Any]]]:
    manifest = read_upload_manifest()
    entries = manifest.get(category, [])
    loaded_files: List[CachedUploadedFile] = []
    valid_entries: List[Dict[str, Any]] = []

    for entry in entries:
        stored_name = entry.get("stored_name")
        if not stored_name:
            continue
        stored_path = UPLOAD_CACHE_DIR / stored_name
        if not stored_path.exists():
            continue
        cached_file = CachedUploadedFile(
            name=entry.get("name", stored_name),
            stored_path=stored_path,
            size=entry.get("size"),
            saved_at=entry.get("saved_at"),
            stored_name=stored_name
        )
        loaded_files.append(cached_file)
        valid_entries.append(entry)

    if len(valid_entries) != len(entries):
        manifest[category] = valid_entries
        write_upload_manifest(manifest)

    return loaded_files, valid_entries


def remove_cached_file(category: str, stored_name: str) -> bool:
    manifest = read_upload_manifest()
    entries = manifest.get(category, [])
    if not entries:
        return False

    remaining_entries = []
    removed = False
    for entry in entries:
        entry_stored = entry.get("stored_name")
        if not removed and entry_stored == stored_name:
            removed = True
            if entry_stored:
                path = UPLOAD_CACHE_DIR / entry_stored
                try:
                    path.unlink()
                except FileNotFoundError:
                    pass
            continue
        remaining_entries.append(entry)

    if removed:
        manifest[category] = remaining_entries
        write_upload_manifest(manifest)
    return removed


def trigger_streamlit_rerun() -> None:
    """Streamlitã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³å·®ç•°ã‚’å¸åã—ãªãŒã‚‰å®‰å…¨ã«rerunã‚’è¦æ±‚ã™ã‚‹ã€‚"""
    rerun_fn = getattr(st, "rerun", None)
    if callable(rerun_fn):
        rerun_fn()
        return

    rerun_fn = getattr(st, "experimental_rerun", None)
    if callable(rerun_fn):
        rerun_fn()
        return

    set_params = getattr(st, "experimental_set_query_params", None)
    get_params = getattr(st, "experimental_get_query_params", None)
    if callable(set_params) and callable(get_params):
        params = get_params()
        params["_refresh_token"] = [uuid.uuid4().hex]
        set_params(**params)
        return

    st.warning("ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’æ›´æ–°ã™ã‚‹ã«ã¯ãƒ–ãƒ©ã‚¦ã‚¶ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„ã€‚")


def collect_summary(result_paths):
    summary_rows = []
    for p in result_paths:
        try:
            df = pd.read_csv(p, encoding="utf-8-sig")
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(p, encoding="cp932")
            except UnicodeDecodeError:
                df = pd.read_csv(p, encoding="utf-8")
        total = len(df)
        err_cnt = int((df.get("ã‚¨ãƒ©ãƒ¼","") == "â—¯").sum())
        cat_counts = {}
        if "ã‚«ãƒ†ã‚´ãƒª" in df.columns:
            for v, c in df["ã‚«ãƒ†ã‚´ãƒª"].value_counts().items():
                if isinstance(v, str) and v:
                    cat_counts[v] = int(c)
        summary_rows.append({
            "ãƒ•ã‚¡ã‚¤ãƒ«": os.path.basename(p),
            "ç·ä»¶æ•°": total,
            "ã‚¨ãƒ©ãƒ¼ä»¶æ•°": err_cnt,
            **{f"ã‚«ãƒ†ã‚´ãƒª:{k}": v for k, v in cat_counts.items()}
        })
    return pd.DataFrame(summary_rows)

def extract_facility_name_from_filename(filename):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰äº‹æ¥­æ‰€åã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
    å…ˆé ­ã‹ã‚‰ã®ã²ã‚‰ãŒãªãƒ»æ¼¢å­—ãƒ»ã‚«ã‚¿ã‚«ãƒŠã¾ãŸã¯ã‚¹ãƒšãƒ¼ã‚¹ã¾ã§ã‚’èªè­˜
    ä¾‹: 'ã•ãã‚‰202508)3.csv' -> 'ã•ãã‚‰'
    ä¾‹: 'ã»ã£ã¨202508)3.csv' -> 'ã»ã£ã¨'
    ä¾‹: 'ã•ãã‚‰ã€€2020508 3.csv' -> 'ã•ãã‚‰'
    """
    import re
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã‚’é™¤å»
    base_name = os.path.splitext(filename)[0]
    
    # result_ ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»
    if base_name.startswith('result_'):
        base_name = base_name[7:]  # 'result_' ã‚’é™¤å»
    
    # å…ˆé ­ã‹ã‚‰ã®ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ãƒ»è‹±å­—ã‚’æŠ½å‡ºï¼ˆæ•°å­—ãƒ»è¨˜å·ã§çµ‚äº†ï¼‰
    # ã²ã‚‰ãŒãª: ã‚-ã‚“ (U+3042-U+3093)
    # ã‚«ã‚¿ã‚«ãƒŠ: ã‚¢-ãƒ³ (U+30A2-U+30F3) + é•·éŸ³ç¬¦ãƒ»ä¸­ç‚¹
    # æ¼¢å­—: ä¸€-é¾¯ (U+4E00-U+9FAF)
    # è‹±å­—: a-zA-Z
    # è¨˜å·: ãƒ¼ï¼ˆé•·éŸ³ç¬¦ï¼‰ã€ãƒ»ï¼ˆä¸­ç‚¹ï¼‰
    pattern = r'^([ã‚-ã‚“ã‚¢-ãƒ³ãƒ¼ãƒ»ä¸€-é¾¯a-zA-Zï½-ï½šï¼¡-ï¼º]+)'
    
    match = re.match(pattern, base_name.strip())
    if match:
        facility_name = match.group(1).strip()
        return facility_name
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¹ãƒšãƒ¼ã‚¹ã§åŒºåˆ‡ã‚‰ã‚ŒãŸæœ€åˆã®éƒ¨åˆ†
    parts = re.split(r'[ã€€\s]+', base_name.strip())
    if parts and parts[0]:
        # æœ€åˆã®éƒ¨åˆ†ã‹ã‚‰æ—¥æœ¬èªãƒ»è‹±å­—ã®ã¿ã‚’æŠ½å‡º
        first_part = parts[0]
        clean_match = re.match(r'^([ã‚-ã‚“ã‚¢-ãƒ³ä¸€-é¾¯a-zA-Zï½-ï½šï¼¡-ï¼º]+)', first_part)
        if clean_match:
            return clean_match.group(1).strip()
        return first_part.strip()
    
    # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ãã®ã¾ã¾è¿”ã™
    return base_name.strip()

def extract_facility_names_from_partner_facilities(partner_facilities_str):
    """
    é‡è¤‡ç›¸æ‰‹æ–½è¨­ã®æ–‡å­—åˆ—ã‹ã‚‰äº‹æ¥­æ‰€åã®ãƒªã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
    ä¾‹: 'result_ã•ãã‚‰ã€€2020508 3.csvï¼Œresult_ã²ã¾ã‚ã‚Š 456.csv' -> ['ã•ãã‚‰', 'ã²ã¾ã‚ã‚Š']
    """
    if not partner_facilities_str or pd.isna(partner_facilities_str):
        return []
    
    facility_names = []
    # ã‚«ãƒ³ãƒã§åˆ†å‰²
    facilities = [f.strip() for f in str(partner_facilities_str).split('ï¼Œ') if f.strip()]
    
    for facility in facilities:
        facility_name = extract_facility_name_from_filename(facility)
        if facility_name and facility_name not in facility_names:
            facility_names.append(facility_name)
    
    return facility_names

def normalize_staff_list(raw_value: Any) -> str:
    """
    ä»£æ›¿è·å“¡ãƒªã‚¹ãƒˆãªã©ã®ã‚¹ã‚¿ãƒƒãƒ•åãƒªã‚¹ãƒˆã‚’æ­£è¦åŒ–ã€‚
    åŒºåˆ‡ã‚Šæ–‡å­—ä»˜ãã®æ–‡å­—åˆ—ã‹ã‚‰å€‹ã€…ã®åå‰ã‚’æŠ½å‡ºã—ã€normalize_nameã‚’é©ç”¨ã™ã‚‹ã€‚
    """
    if raw_value is None or (isinstance(raw_value, float) and pd.isna(raw_value)):
        return ''
    
    text = str(raw_value).strip()
    
    # æ˜ç¤ºçš„ã«å€™è£œãªã—ã‚’ç¤ºã™è¨˜å·ã¯ãã®ã¾ã¾è¿”ã™
    if text in {'', 'ãƒ¼', '-'}:
        return text
    
    # åŒºåˆ‡ã‚Šæ–‡å­—ã§åˆ†å‰²ï¼ˆã‚¹ãƒ©ãƒƒã‚·ãƒ¥ãƒ»å…¨è§’ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ãƒ»ã‚«ãƒ³ãƒé¡ï¼‰
    parts = re.split(r'[ï¼/ï¼Œ,ã€]+', text)
    normalized_parts = []
    
    for part in parts:
        candidate = part.strip()
        if not candidate:
            continue
        normalized_candidate = normalize_name(candidate) or candidate
        if normalized_candidate not in normalized_parts:
            normalized_parts.append(normalized_candidate)
    
    if not normalized_parts:
        return ''
    
    return ' / '.join(normalized_parts)

def prepare_grid_data(result_paths):
    """
    result_*.csvãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤ºç”¨ã®DataFrameã‚’ä½œæˆ
    æŒ‡å®šã•ã‚ŒãŸé †åºã§ã‚«ãƒ©ãƒ ã‚’ä¸¦ã³æ›¿ãˆã€å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿ã‚’å«ã‚ã‚‹
    """
    grid_data = []
    
    for p in result_paths:
        try:
            df = pd.read_csv(p, encoding="utf-8-sig")
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(p, encoding="cp932")
            except UnicodeDecodeError:
                df = pd.read_csv(p, encoding="utf-8")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰äº‹æ¥­æ‰€åã‚’æŠ½å‡º
        filename = os.path.basename(p)
        facility_name = extract_facility_name_from_filename(filename)
        
        # æŒ‡å®šã•ã‚ŒãŸé †åºã§ã‚«ãƒ©ãƒ ã‚’æ§‹ç¯‰
        for idx, row in df.iterrows():
            # é‡è¤‡ç›¸æ‰‹æ–½è¨­ã‹ã‚‰äº‹æ¥­æ‰€åã‚’æŠ½å‡ºï¼ˆå¾“æ¥ã®æ–¹æ³•ï¼‰
            partner_facilities = row.get('é‡è¤‡ç›¸æ‰‹æ–½è¨­', '')
            duplicate_facility_names = extract_facility_names_from_partner_facilities(partner_facilities)
            
            # é‡è¤‡ç›¸æ‰‹æ–½è¨­ãŒç©ºã®å ´åˆã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æŠ½å‡ºã—ãŸäº‹æ¥­æ‰€åã‚’ä½¿ç”¨
            if not duplicate_facility_names and facility_name:
                # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹è¡Œã®ã¿äº‹æ¥­æ‰€åã‚’è¡¨ç¤º
                if row.get('ã‚¨ãƒ©ãƒ¼', '') == 'â—¯':
                    duplicate_facility_display = facility_name
                else:
                    duplicate_facility_display = ''
            else:
                duplicate_facility_display = 'ï¼Œ'.join(duplicate_facility_names) if duplicate_facility_names else ''
            
            # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆæœ€åˆã®5è¡Œã®ã¿ï¼‰
            if idx < 5:
                print(f"ãƒ‡ãƒãƒƒã‚° - ãƒ•ã‚¡ã‚¤ãƒ«{filename}: è¡Œ{idx}, ã‚¨ãƒ©ãƒ¼='{row.get('ã‚¨ãƒ©ãƒ¼', '')}', äº‹æ¥­æ‰€å='{facility_name}', è¡¨ç¤ºç”¨='{duplicate_facility_display}'")
            
            # é‡è¤‡åˆ©ç”¨è€…åã®è¨­å®šï¼ˆã‚¨ãƒ©ãƒ¼è¡Œã®ã¿é‡è¤‡ç›¸æ‰‹ã®åˆ©ç”¨è€…åã‚’è¡¨ç¤ºï¼‰
            if row.get('ã‚¨ãƒ©ãƒ¼', '') == 'â—¯':
                duplicate_user_name = row.get('é‡è¤‡åˆ©ç”¨è€…å', '')
            else:
                duplicate_user_name = ''
            
            # é‡è¤‡ã‚µãƒ¼ãƒ“ã‚¹æ™‚é–“ã®è¨­å®šï¼ˆã‚¨ãƒ©ãƒ¼è¡Œã®ã¿è¡¨ç¤ºã€NaNå€¤ã‚’é©åˆ‡ã«å‡¦ç†ï¼‰
            if row.get('ã‚¨ãƒ©ãƒ¼', '') == 'â—¯':
                service_time = row.get('é‡è¤‡ã‚µãƒ¼ãƒ“ã‚¹æ™‚é–“', '')
                # NaNå€¤ã‚„nullå€¤ã‚’ç©ºæ–‡å­—ã«å¤‰æ›
                if pd.isna(service_time) or service_time == 'nan':
                    duplicate_service_time = ''
                else:
                    duplicate_service_time = str(service_time) if service_time else ''
            else:
                duplicate_service_time = ''
            
            raw_staff = row.get('æ‹…å½“æ‰€å“¡', '')
            normalized_staff = row.get('æ‹…å½“æ‰€å“¡_norm', '')
            if isinstance(normalized_staff, str) and normalized_staff.strip():
                display_staff = normalized_staff.strip()
            elif isinstance(raw_staff, str) and raw_staff.strip():
                display_staff = normalize_name(raw_staff.strip()) or raw_staff.strip()
            else:
                display_staff = ''
            
            raw_alt_staff = row.get('ä»£æ›¿è·å“¡ãƒªã‚¹ãƒˆ', '')
            normalized_alt_staff = normalize_staff_list(raw_alt_staff)
            if normalized_alt_staff:
                display_alt_staff = normalized_alt_staff
            else:
                raw_alt_text = str(raw_alt_staff).strip() if raw_alt_staff is not None else ''
                display_alt_staff = 'ãƒ¼' if raw_alt_text == '' or raw_alt_text.lower() == 'nan' else raw_alt_text
            
            grid_row = {
                'ã‚¨ãƒ©ãƒ¼': row.get('ã‚¨ãƒ©ãƒ¼', ''),
                'ã‚«ãƒ†ã‚´ãƒª': row.get('ã‚«ãƒ†ã‚´ãƒª', ''),
                'ä»£æ›¿å¾“æ¥­å“¡ãƒªã‚¹ãƒˆ': display_alt_staff,
                'æ‹…å½“æ‰€å“¡': display_staff,
                'åˆ©ç”¨è€…å': row.get('åˆ©ç”¨è€…å', ''),
                'é‡è¤‡åˆ©ç”¨è€…å': duplicate_user_name,
                'é‡è¤‡ã‚¨ãƒ©ãƒ¼äº‹æ¥­æ‰€å': duplicate_facility_display,
                'é‡è¤‡ã‚µãƒ¼ãƒ“ã‚¹æ™‚é–“': duplicate_service_time,
                'æ—¥ä»˜': row.get('æ—¥ä»˜', ''),
                'é–‹å§‹æ™‚é–“': row.get('é–‹å§‹æ™‚é–“', ''),
                'çµ‚äº†æ™‚é–“': row.get('çµ‚äº†æ™‚é–“', ''),
                'ã‚µãƒ¼ãƒ“ã‚¹è©³ç´°': f"{row.get('ã‚µãƒ¼ãƒ“ã‚¹å†…å®¹', '')} - {row.get('å®Ÿæ–½æ™‚é–“', '')}".strip(' -'),
                'é‡è¤‡æ™‚é–“': row.get('é‡è¤‡æ™‚é–“ï¼ˆåˆ†ï¼‰', 0),
                'æ‡²æˆ’æ™‚é–“': row.get('æ‡²æˆ’æ™‚é–“', row.get('è¶…éæ™‚é–“ï¼ˆåˆ†ï¼‰', 0)),
                'ã‚«ãƒãƒ¼çŠ¶æ³': row.get('ã‚«ãƒãƒ¼çŠ¶æ³', ''),
                'ã‚¨ãƒ©ãƒ¼è·å“¡å‹¤å‹™æ™‚é–“': row.get('ã‚¨ãƒ©ãƒ¼è·å“¡å‹¤å‹™æ™‚é–“', ''),
                'ä»£æ›¿è·å“¡å‹¤å‹™æ™‚é–“': row.get('ä»£æ›¿è·å“¡å‹¤å‹™æ™‚é–“', ''),
                'å‹¤å‹™æ™‚é–“è©³ç´°': row.get('å‹¤å‹™æ™‚é–“è©³ç´°', ''),
                'å‹¤å‹™æ™‚é–“å¤–è©³ç´°': row.get('å‹¤å‹™æ™‚é–“å¤–è©³ç´°', ''),
                'æœªã‚«ãƒãƒ¼åŒºé–“': row.get('æœªã‚«ãƒãƒ¼åŒºé–“', ''),
                'å‹¤å‹™åŒºé–“æ•°': row.get('å‹¤å‹™åŒºé–“æ•°', 0)
            }
            grid_data.append(grid_row)
    
    desired_columns = [
        'ä»£æ›¿å¾“æ¥­å“¡ãƒªã‚¹ãƒˆ', 'é‡è¤‡ã‚¨ãƒ©ãƒ¼äº‹æ¥­æ‰€å', 'åˆ©ç”¨è€…å', 'æ—¥ä»˜', 'é–‹å§‹æ™‚é–“', 'çµ‚äº†æ™‚é–“',
        'æ‹…å½“æ‰€å“¡', 'ã‚µãƒ¼ãƒ“ã‚¹è©³ç´°', 'é‡è¤‡åˆ©ç”¨è€…å', 'é‡è¤‡ã‚µãƒ¼ãƒ“ã‚¹æ™‚é–“', 'é‡è¤‡æ™‚é–“', 'æ‡²æˆ’æ™‚é–“',
        'ã‚¨ãƒ©ãƒ¼è·å“¡å‹¤å‹™æ™‚é–“', 'ä»£æ›¿è·å“¡å‹¤å‹™æ™‚é–“', 'å‹¤å‹™æ™‚é–“è©³ç´°', 'å‹¤å‹™æ™‚é–“å¤–è©³ç´°', 'æœªã‚«ãƒãƒ¼åŒºé–“'
    ]

    base_columns = desired_columns + ['ã‚¨ãƒ©ãƒ¼', 'ã‚«ãƒ†ã‚´ãƒª', 'ã‚«ãƒãƒ¼çŠ¶æ³', 'å‹¤å‹™åŒºé–“æ•°']
    base_columns = list(dict.fromkeys(base_columns))
    
    if not grid_data:
        return pd.DataFrame(columns=base_columns)
    
    df = pd.DataFrame(grid_data)
    
    remaining_columns = [col for col in df.columns if col not in desired_columns]
    ordered_columns = desired_columns + remaining_columns
    
    return df[ordered_columns]

def create_styled_grid(df):
    """
    æ¡ä»¶ä»˜ãèƒŒæ™¯è‰²ã‚’é©ç”¨ã—ãŸAgGridã‚’ä½œæˆã™ã‚‹é–¢æ•°
    - ã€Œã•ãã‚‰ã€ã‚’å«ã‚€è¡Œï¼šè–„ã„èµ¤ã®èƒŒæ™¯è‰² (#ffebee)
    - ã€Œã»ã£ã¨ã€ã‚’å«ã‚€è¡Œï¼šè–„ã„é’ã®èƒŒæ™¯è‰² (#e3f2fd)
    """
    # GridOptionsBuilderã‚’ä½¿ç”¨ã—ã¦ã‚°ãƒªãƒƒãƒ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¨­å®š
    gb = GridOptionsBuilder.from_dataframe(df)
    
    # åŸºæœ¬è¨­å®š
    gb.configure_pagination(paginationAutoPageSize=False, paginationPageSize=100)
    gb.configure_side_bar()
    gb.configure_selection('single', use_checkbox=False)
    gb.configure_default_column(
        groupable=True,
        value=True,
        enableRowGroup=True,
        aggFunc='sum',
        editable=False,
        resizable=True,
        sortable=True,
        filter=True
    )
    
    # æ¡ä»¶ä»˜ãã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°ã®JavaScriptã‚³ãƒ¼ãƒ‰
    cell_style_jscode = JsCode("""
    function(params) {
        const facilityName = params.data['é‡è¤‡ã‚¨ãƒ©ãƒ¼äº‹æ¥­æ‰€å'];
        if (facilityName) {
            // æ­£ç¢ºã«ã€Œã•ãã‚‰ã€ã¾ãŸã¯ã€Œã»ã£ã¨ã€ã«ä¸€è‡´ã™ã‚‹å ´åˆã®ã¿è‰²ã‚’é©ç”¨
            if (facilityName === 'ã•ãã‚‰' || facilityName.includes('ã•ãã‚‰')) {
                // ã€Œã•ãã‚‰ã€ã®å ´åˆï¼šè–„ã„èµ¤
                return {
                    'backgroundColor': '#ffebee',
                    'color': 'black'
                };
            } else if (facilityName === 'ã»ã£ã¨' || facilityName.includes('ã»ã£ã¨')) {
                // ã€Œã»ã£ã¨ã€ã®å ´åˆï¼šè–„ã„é’
                return {
                    'backgroundColor': '#e3f2fd',
                    'color': 'black'
                };
            }
        }
        return {};
    }
    """)
    
    # è¡Œå…¨ä½“ã«ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©ç”¨
    row_style_jscode = JsCode("""
    function(params) {
        const facilityName = params.data['é‡è¤‡ã‚¨ãƒ©ãƒ¼äº‹æ¥­æ‰€å'];
        if (facilityName) {
            // æ­£ç¢ºã«ã€Œã•ãã‚‰ã€ã¾ãŸã¯ã€Œã»ã£ã¨ã€ã«ä¸€è‡´ã™ã‚‹å ´åˆã®ã¿è‰²ã‚’é©ç”¨
            if (facilityName === 'ã•ãã‚‰' || facilityName.includes('ã•ãã‚‰')) {
                // ã€Œã•ãã‚‰ã€ã®å ´åˆï¼šè–„ã„èµ¤
                return {'backgroundColor': '#ffebee'};
            } else if (facilityName === 'ã»ã£ã¨' || facilityName.includes('ã»ã£ã¨')) {
                // ã€Œã»ã£ã¨ã€ã®å ´åˆï¼šè–„ã„é’
                return {'backgroundColor': '#e3f2fd'};
            }
        }
        return {};
    }
    """)
    
    # å„ã‚«ãƒ©ãƒ ã«ã‚»ãƒ«ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©ç”¨ã—ã€ç‰¹å®šã®åˆ—ã«å¹…ã‚’è¨­å®š
    for col in df.columns:
        gb.configure_column(col, cellStyle=cell_style_jscode)
    
    # æ–°ã—ãè¿½åŠ ã—ãŸåˆ—ã®å¹…ã‚’è¨­å®š
    if 'é‡è¤‡åˆ©ç”¨è€…å' in df.columns:
        gb.configure_column('é‡è¤‡åˆ©ç”¨è€…å', width=120)
    if 'é‡è¤‡ã‚µãƒ¼ãƒ“ã‚¹æ™‚é–“' in df.columns:
        gb.configure_column('é‡è¤‡ã‚µãƒ¼ãƒ“ã‚¹æ™‚é–“', width=150)
    
    # ãã®ä»–ã®é‡è¦ãªåˆ—ã®å¹…ã‚‚èª¿æ•´
    if 'é‡è¤‡ã‚¨ãƒ©ãƒ¼äº‹æ¥­æ‰€å' in df.columns:
        gb.configure_column('é‡è¤‡ã‚¨ãƒ©ãƒ¼äº‹æ¥­æ‰€å', width=150)
    if 'åˆ©ç”¨è€…å' in df.columns:
        gb.configure_column('åˆ©ç”¨è€…å', width=120)
    
    # è¡Œã‚¹ã‚¿ã‚¤ãƒ«ã‚’è¨­å®š
    gb.configure_grid_options(getRowStyle=row_style_jscode)
    
    # ã‚°ãƒªãƒƒãƒ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰
    gridOptions = gb.build()

    return gridOptions

def _safe_text(value: Any) -> str:
    """NaNã‚„Noneã‚’ç©ºæ–‡å­—åˆ—ã«æ­£è¦åŒ–ã—ã€è¡¨ç¤ºã—ã‚„ã™ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™ã€‚"""
    if value is None:
        return ""
    if isinstance(value, float) and pd.isna(value):
        return ""
    text = str(value).strip()
    if not text or text.lower() == "nan":
        return ""
    return text

@lru_cache(maxsize=32)
def _get_indicator_image(width: int, height: int, color: Tuple[int, int, int]) -> Optional[Any]:
    """æŒ‡å®šè‰²ãƒ»ã‚µã‚¤ã‚ºã®ç¸¦ãƒ©ã‚¤ãƒ³ç”»åƒã‚’ç”Ÿæˆï¼ˆPillowæœªå°å…¥ãªã‚‰Noneã‚’è¿”ã™ï¼‰ã€‚"""
    if Image is None:
        return None
    return Image.new("RGB", (width, height), color)

def show_card_view(df: pd.DataFrame) -> None:
    """
    AgGridã¨åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚«ãƒ¼ãƒ‰å½¢å¼ã§è¡¨ç¤ºã™ã‚‹ã€‚
    CSSã‚„unsafe_allow_htmlã‚’ä½¿ã‚ãšã€æ¨™æº–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã¿ã§æ§‹æˆã™ã‚‹ã€‚
    """
    st.caption(f"{len(df)}ä»¶ã‚’ãƒªã‚¹ãƒˆè¡¨ç¤ºä¸­")

    for idx, (_, row) in enumerate(df.iterrows(), start=1):
        date_text = _safe_text(row.get("æ—¥ä»˜")) or "æ—¥ä»˜æœªè¨­å®š"
        start_time = _safe_text(row.get("é–‹å§‹æ™‚é–“")) or "ãƒ¼"
        end_time = _safe_text(row.get("çµ‚äº†æ™‚é–“")) or "ãƒ¼"
        user_name = _safe_text(row.get("åˆ©ç”¨è€…å")) or "åˆ©ç”¨è€…ä¸æ˜"
        staff_name = _safe_text(row.get("æ‹…å½“æ‰€å“¡")) or "æ‹…å½“æœªè¨­å®š"
        facility_name = _safe_text(row.get("é‡è¤‡ã‚¨ãƒ©ãƒ¼äº‹æ¥­æ‰€å")) or "äº‹æ¥­æ‰€æœªè¨­å®š"
        alt_staff = _safe_text(row.get("ä»£æ›¿å¾“æ¥­å“¡ãƒªã‚¹ãƒˆ")) or "ãƒ¼"

        duplicate_user = _safe_text(row.get("é‡è¤‡åˆ©ç”¨è€…å"))
        duplicate_service = _safe_text(row.get("é‡è¤‡ã‚µãƒ¼ãƒ“ã‚¹æ™‚é–“"))
        duplicate_minutes = _safe_text(row.get("é‡è¤‡æ™‚é–“"))
        penalty_minutes = _safe_text(row.get("æ‡²æˆ’æ™‚é–“"))
        service_detail = _safe_text(row.get("ã‚µãƒ¼ãƒ“ã‚¹è©³ç´°"))
        error_shift = _safe_text(row.get("ã‚¨ãƒ©ãƒ¼è·å“¡å‹¤å‹™æ™‚é–“"))
        alternate_shift = _safe_text(row.get("ä»£æ›¿è·å“¡å‹¤å‹™æ™‚é–“"))
        shift_detail = _safe_text(row.get("å‹¤å‹™æ™‚é–“è©³ç´°"))
        off_detail = _safe_text(row.get("å‹¤å‹™æ™‚é–“å¤–è©³ç´°"))
        uncovered = _safe_text(row.get("æœªã‚«ãƒãƒ¼åŒºé–“"))
        coverage = _safe_text(row.get("ã‚«ãƒãƒ¼çŠ¶æ³"))
        error_mark = _safe_text(row.get("ã‚¨ãƒ©ãƒ¼"))
        category = _safe_text(row.get("ã‚«ãƒ†ã‚´ãƒª"))

        detail_lines: List[str] = []
        if facility_name:
            detail_lines.append(f"é‡è¤‡ã‚¨ãƒ©ãƒ¼äº‹æ¥­æ‰€å: {facility_name}")
        for label, value in [
            ("ã‚¨ãƒ©ãƒ¼", error_mark),
            ("ã‚«ãƒ†ã‚´ãƒª", category),
            ("ã‚«ãƒãƒ¼çŠ¶æ³", coverage),
            ("é‡è¤‡åˆ©ç”¨è€…", duplicate_user),
            ("é‡è¤‡ã‚µãƒ¼ãƒ“ã‚¹", duplicate_service),
            ("é‡è¤‡æ™‚é–“", duplicate_minutes),
            ("æ‡²æˆ’æ™‚é–“", penalty_minutes),
            ("ã‚µãƒ¼ãƒ“ã‚¹è©³ç´°", service_detail),
            ("å‹¤å‹™æ™‚é–“è©³ç´°", error_shift or shift_detail),
            ("ä»£æ›¿è·å“¡å‹¤å‹™æ™‚é–“", alternate_shift),
            ("å‹¤å‹™æ™‚é–“å¤–è©³ç´°", off_detail),
            ("æœªã‚«ãƒãƒ¼åŒºé–“", uncovered),
        ]:
            if value:
                detail_lines.append(f"{label}: {value}")

        tooltip_text = "\n".join(detail_lines) if detail_lines else "è¿½åŠ ã®è©³ç´°æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"

        indicator_color: Optional[Tuple[int, int, int]] = None
        indicator_fallback = None
        if facility_name and "ã•ãã‚‰" in facility_name:
            indicator_color = (255, 224, 236)  # è–„ã„ãƒ”ãƒ³ã‚¯
            indicator_fallback = ":red[â”ƒ]"
        elif facility_name and "ã»ã£ã¨" in facility_name:
            indicator_color = (224, 238, 255)  # è–„ã„é’
            indicator_fallback = ":blue[â”ƒ]"

        left_line_count = 1  # æ—¥ä»˜è¡Œ
        left_line_count += max(1, staff_name.count('\n') + 1)
        left_line_count += max(1, alt_staff.count('\n') + 1)

        right_line_count = max(1, user_name.count('\n') + 1) + 1  # +1ã¯â„¹ï¸ãƒœã‚¿ãƒ³åˆ†

        total_lines = max(left_line_count + 1, right_line_count)
        line_height_px = 24
        indicator_height = max(36, int(total_lines * line_height_px * 1.0))

        with st.container(border=True):
            row_cols = st.columns([0.2, 5, 2])
            with row_cols[0]:
                if indicator_color:
                    indicator_img = _get_indicator_image(12, indicator_height, indicator_color)
                    if indicator_img is not None:
                        st.image(indicator_img, width=12, clamp=True)
                    elif indicator_fallback:
                        st.markdown(indicator_fallback)
                else:
                    st.write("")
            with row_cols[1]:
                st.markdown(f"**{date_text}ã€€{start_time} ã€œ {end_time}**")
                st.caption(f"æ‹…å½“è€…: {staff_name}")
                st.caption(f"ä»£æ›¿å¾“æ¥­å“¡: {alt_staff}")
            with row_cols[2]:
                top_cols = st.columns([4, 1])
                with top_cols[0]:
                    st.markdown(f"**{user_name}**")
                with top_cols[1]:
                    st.button(
                        "â„¹ï¸",
                        key=f"detail_hint_{idx}",
                        help=tooltip_text,
                        disabled=True
                    )

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="é‡è¤‡ãƒã‚§ãƒƒã‚«ãƒ¼ for hot", layout="wide")

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
/* ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒ«èª¿æ•´ */
.stFileUploader > div > div > div > div {
    border: 2px dashed #cccccc;
    border-radius: 10px;
    padding: 20px;
    text-align: center;
    background-color: #f8f9fa;
}

.stFileUploader > div > div > div > div:hover {
    border-color: #007bff;
    background-color: #e3f2fd;
}

/* ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’éš ã™ï¼ˆå®Œå…¨ã«ã¯éš ã›ãªã„ãŒç›®ç«‹ãŸãªãã™ã‚‹ï¼‰ */
.stFileUploader > div > div > div > div > small {
    color: #6c757d;
    font-size: 0.8em;
}

</style>
""", unsafe_allow_html=True)

st.title("é‡è¤‡ãƒã‚§ãƒƒã‚«ãƒ¼ for hot")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'processing_complete' not in st.session_state:
    st.session_state.processing_complete = False
if 'result_paths' not in st.session_state:
    st.session_state.result_paths = []
if 'diagnostic_paths' not in st.session_state:
    st.session_state.diagnostic_paths = []
if 'summary_df' not in st.session_state:
    st.session_state.summary_df = None
if 'workdir' not in st.session_state:
    st.session_state.workdir = None

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
with st.sidebar:
    st.header("è¨­å®š")
    identical_prefer = st.selectbox("å®Œå…¨ä¸€è‡´æ™‚", ["earlier", "later"], index=0)
    alt_delim = st.text_input("åŒºåˆ‡ã‚Šæ–‡å­—", value="/")
    use_schedule_when_missing = st.checkbox("äºˆå®šã§ä»£ç”¨", value=True)
    service_staff_col = st.text_input("ã‚µãƒ¼ãƒ“ã‚¹å¾“æ¥­å“¡åˆ—", value="æ‹…å½“æ‰€å“¡")
    att_name_col = st.text_input("å‹¤æ€ å¾“æ¥­å“¡åˆ—", value="åå‰")
    generate_diagnostics = st.checkbox("è¨ºæ–­CSVå‡ºåŠ›", value=True)
    show_debug_logs = st.checkbox("ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’è¡¨ç¤º", value=False)

# ã‚¿ãƒ–ã®ä½œæˆ
tab1, tab2, tab3, tab4 = st.tabs([
    "â‘ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
    "â‘¡ã‚¨ãƒ©ãƒ¼ç¢ºèª",
    "â‘¢å‹¤æ€ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    "â‘£ã‚¨ãƒ©ãƒ¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
])

# ã‚¿ãƒ–1: â‘ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
with tab1:
    st.header("â‘ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    
    st.subheader("ã‚µãƒ¼ãƒ“ã‚¹å®Ÿæ…‹CSVï¼ˆè¤‡æ•°å¯ï¼‰")
    st.info("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã“ã“ã«ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã™ã‚‹ã‹ã€ä¸‹ã®ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
    cached_service_files, cached_service_meta = load_cached_uploaded_files("service")
    svc_uploaded_files = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ", type=["csv"], accept_multiple_files=True, key="svc", label_visibility="collapsed")
    svc_uploaded_files = list(svc_uploaded_files) if svc_uploaded_files else []

    if svc_uploaded_files:
        for file in svc_uploaded_files:
            file_size = len(file.getvalue()) / 1024
            size_str = f"{file_size:.1f}KB" if file_size < 1024 else f"{file_size/1024:.1f}MB"
            st.write(f"â€¢ {file.name} ({size_str})")
        save_uploaded_files_to_cache("service", svc_uploaded_files)
        cached_service_files, cached_service_meta = load_cached_uploaded_files("service")

    if cached_service_files:
        st.caption("å‰å›ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚µãƒ¼ãƒ“ã‚¹CSVã‚’è‡ªå‹•ã§ä½¿ç”¨ã—ã¾ã™ã€‚")
        for meta in list(cached_service_meta):
            file_size = (meta.get("size") or 0) / 1024
            size_str = f"{file_size:.1f}KB" if file_size < 1024 else f"{file_size/1024:.1f}MB"
            cols = st.columns([12, 1])
            with cols[0]:
                st.write(f"â€¢ {meta.get('name')} ({size_str})")
            with cols[1]:
                button_label = "Ã—"
                if st.button(button_label, key=f"remove_service_{meta.get('stored_name')}"):
                    if remove_cached_file("service", meta.get("stored_name")):
                        cached_service_meta = [m for m in cached_service_meta if m.get("stored_name") != meta.get("stored_name")]
                        cached_service_files = [obj for obj in cached_service_files if getattr(obj, "stored_name", None) != meta.get("stored_name")]
                        st.success(f"{meta.get('name')} ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
                        trigger_streamlit_rerun()

    svc_files: List[Any] = list(cached_service_files)
    
    st.subheader("å‹¤æ€ å±¥æ­´CSV")
    st.info("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã“ã“ã«ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã™ã‚‹ã‹ã€ä¸‹ã®ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
    cached_attendance_files, cached_attendance_meta = load_cached_uploaded_files("attendance")
    att_uploaded_files = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ", type=["csv"], accept_multiple_files=True, key="att", label_visibility="collapsed")
    att_uploaded_files = list(att_uploaded_files) if att_uploaded_files else []
    
    if att_uploaded_files:
        for uploaded in att_uploaded_files:
            file_size = len(uploaded.getvalue()) / 1024
            size_str = f"{file_size:.1f}KB" if file_size < 1024 else f"{file_size/1024:.1f}MB"
            st.write(f"â€¢ {uploaded.name} ({size_str})")
        save_uploaded_files_to_cache("attendance", att_uploaded_files)
        cached_attendance_files, cached_attendance_meta = load_cached_uploaded_files("attendance")

    if cached_attendance_files:
        st.caption("å‰å›ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸå‹¤æ€ CSVã‚’è‡ªå‹•ã§ä½¿ç”¨ã—ã¾ã™ã€‚")
        for meta in list(cached_attendance_meta):
            file_size = (meta.get("size") or 0) / 1024
            size_str = f"{file_size:.1f}KB" if file_size < 1024 else f"{file_size/1024:.1f}MB"
            cols = st.columns([12, 1])
            with cols[0]:
                st.write(f"â€¢ {meta.get('name')} ({size_str})")
            with cols[1]:
                if st.button("Ã—", key=f"remove_att_{meta.get('stored_name')}"):
                    if remove_cached_file("attendance", meta.get("stored_name")):
                        cached_attendance_meta = [m for m in cached_attendance_meta if m.get("stored_name") != meta.get("stored_name")]
                        cached_attendance_files = [obj for obj in cached_attendance_files if getattr(obj, "stored_name", None) != meta.get("stored_name")]
                        st.success(f"{meta.get('name')} ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
                        trigger_streamlit_rerun()

    att_files: List[Any] = list(cached_attendance_files)

    if att_files and len(att_files) > 1:
        st.caption("è¤‡æ•°ã®å‹¤æ€ CSVã‚’é¸æŠã™ã‚‹ã¨è‡ªå‹•ã§çµåˆã—ã¦ã‹ã‚‰å‡¦ç†ã—ã¾ã™ã€‚")

    st.caption(f"â€» ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸCSVã¯ãƒ­ãƒ¼ã‚«ãƒ«ã® `{UPLOAD_CACHE_DISPLAY}` ã«ä¿å­˜ã•ã‚Œã€æ¬¡å›ã®å…¥ã‚Œæ›¿ãˆã‚„å‰Šé™¤ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¾ã§ä¿æŒã•ã‚Œã¾ã™ã€‚")
    
    run = st.button("ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ", type="primary", use_container_width=True)
    
    if run:
        if not svc_files:
            st.error("ã‚µãƒ¼ãƒ“ã‚¹å®Ÿæ…‹CSVã‚’1ä»¶ä»¥ä¸Šã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            st.stop()
        
        default_attendance_path = find_default_attendance_csv()
        use_default_attendance = False
        
        if not att_files:
            use_default_attendance = True
            if default_attendance_path and default_attendance_path.exists():
                st.info("å‹¤æ€ å±¥æ­´CSVãŒæœªã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®ãŸã‚ã€`input/å‹¤æ€ å±¥æ­´.csv` ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            else:
                st.info("å‹¤æ€ å±¥æ­´CSVãŒæœªã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®ãŸã‚ã€çµ„ã¿è¾¼ã¿ã®å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

        with st.spinner("å‡¦ç†ä¸­..."):
            # ä½œæ¥­ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            workdir = tempfile.mkdtemp(prefix=f"svc_att_check_{ts}_")
            indir = os.path.join(workdir, "input")
            os.makedirs(indir, exist_ok=True)

            # src.py ã‚’ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼ï¼ˆåŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹æƒ³å®šï¼‰
            # å®Ÿè¡Œç’°å¢ƒã§ã¯ã“ã®ã‚¢ãƒ—ãƒªã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã« src.py ã‚’ç½®ã„ã¦ãã ã•ã„ã€‚
            src_source = os.path.join(os.path.dirname(__file__), "src.py")
            src_target = os.path.join(workdir, "src.py")
            shutil.copyfile(src_source, src_target)

            # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            # æ–½è¨­CSVã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åã‚’ãã®ã¾ã¾ä½¿ã†ï¼ˆresult_*.csvã®æ–½è¨­åã«å½±éŸ¿ï¼‰
            
            # åŒåãƒ•ã‚¡ã‚¤ãƒ«ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ã¨é€£ç•ªä»˜ä¸
            saved_files = {}
            saved_service_files = []
            
            for i, up in enumerate(svc_files):
                original_name = up.name
                base_name, ext = os.path.splitext(original_name)
                
                # åŒåãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯é€£ç•ªã‚’ä»˜ä¸
                if original_name in saved_files:
                    saved_files[original_name] += 1
                    new_name = f"{base_name}_{saved_files[original_name]}{ext}"
                else:
                    saved_files[original_name] = 0
                    new_name = original_name
                
                file_path = os.path.join(indir, new_name)
                try:
                    # save_upload_toã¯æ­£è¦åŒ–ã•ã‚ŒãŸãƒ‘ã‚¹ã¨ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿”ã™
                    actual_path, debug_info = save_upload_to(file_path, up)
                    
                    if os.path.exists(actual_path):
                        saved_service_files.append(os.path.basename(actual_path))
                        
                except Exception as e:
                    st.error(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {original_name} -> {str(e)}")
            
            attendance_filename = None
            actual_att_path = None
            
            if use_default_attendance:
                if default_attendance_path and default_attendance_path.exists():
                    attendance_filename = default_attendance_path.name
                    actual_att_path = os.path.join(indir, attendance_filename)
                    try:
                        shutil.copyfile(str(default_attendance_path), actual_att_path)
                    except Exception as e:
                        st.error(f"å‹¤æ€ å±¥æ­´CSVã®ã‚³ãƒ”ãƒ¼ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
                        st.stop()
                    attendance_df = None
                    for encoding in ['utf-8-sig', 'cp932', 'utf-8', 'shift_jis']:
                        try:
                            attendance_df = pd.read_csv(str(default_attendance_path), encoding=encoding)
                            break
                        except UnicodeDecodeError:
                            continue
                    if attendance_df is None:
                        attendance_df = build_builtin_attendance_dataframe()
                else:
                    attendance_filename = "å‹¤æ€ å±¥æ­´.csv"
                    actual_att_path = os.path.join(indir, attendance_filename)
                    try:
                        builtin_bytes = get_builtin_attendance_csv_bytes()
                        with open(actual_att_path, "wb") as f:
                            f.write(builtin_bytes)
                        attendance_df = build_builtin_attendance_dataframe()
                    except Exception as e:
                        st.error(f"çµ„ã¿è¾¼ã¿å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
                        st.stop()
                attendance_filename = os.path.basename(actual_att_path)
                st.session_state.attendance_df = attendance_df
                st.session_state.attendance_file_path = actual_att_path
            else:
                attendance_dfs = []
                attendance_source_names = [uploaded.name for uploaded in att_files]
                encodings = ['utf-8-sig', 'cp932', 'utf-8', 'shift_jis']
                
                for uploaded in att_files:
                    file_bytes = uploaded.getvalue()
                    df = None
                    for encoding in encodings:
                        try:
                            df = pd.read_csv(io.BytesIO(file_bytes), encoding=encoding)
                            break
                        except UnicodeDecodeError:
                            continue
                        except Exception as e:
                            st.error(f"å‹¤æ€ å±¥æ­´CSVã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ ({uploaded.name}): {str(e)}")
                            st.stop()
                    if df is None:
                        st.error(f"å‹¤æ€ å±¥æ­´CSVã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ ({uploaded.name}). å¯¾å¿œã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {', '.join(encodings)}")
                        st.stop()
                    attendance_dfs.append(df)
                    if show_debug_logs:
                        st.write(f"å‹¤æ€ CSVèª­è¾¼æˆåŠŸ: {uploaded.name} (è¡Œæ•°: {len(df)})")
                
                if len(attendance_dfs) == 1:
                    single_file = att_files[0]
                    attendance_filename = single_file.name
                    att_file_path = os.path.join(indir, attendance_filename)
                    try:
                        actual_att_path, att_debug_info = save_upload_to(att_file_path, single_file)
                        if not os.path.exists(actual_att_path):
                            st.error("å‹¤æ€ å±¥æ­´CSVä¿å­˜å¤±æ•—")
                            st.stop()
                        attendance_filename = os.path.basename(actual_att_path)
                        attendance_df = attendance_dfs[0]
                        st.session_state.attendance_df = attendance_df
                        st.session_state.attendance_file_path = actual_att_path
                    except Exception as e:
                        st.error(f"å‹¤æ€ å±¥æ­´CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        st.stop()
                else:
                    try:
                        attendance_df = pd.concat(attendance_dfs, ignore_index=True)
                    except Exception as e:
                        st.error(f"å‹¤æ€ å±¥æ­´CSVã®çµåˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
                        st.stop()
                    attendance_filename = "combined_attendance.csv"
                    actual_att_path = os.path.join(indir, attendance_filename)
                    try:
                        attendance_df.to_csv(actual_att_path, index=False, encoding="utf-8-sig")
                    except Exception as e:
                        st.error(f"çµåˆå¾Œã®å‹¤æ€ å±¥æ­´CSVã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
                        st.stop()
                    attendance_filename = os.path.basename(actual_att_path)
                    st.session_state.attendance_df = attendance_df
                    st.session_state.attendance_file_path = actual_att_path
                    if show_debug_logs:
                        st.write(f"å‹¤æ€ CSVã‚’çµåˆ: {', '.join(attendance_source_names)} -> {attendance_filename}")
            
            # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€çµ‚ç¢ºèª
            all_files = os.listdir(indir)
            # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„CSVãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º
            csv_files = [f for f in all_files if f.lower().endswith('.csv')]
            
            actual_service_files = []
            actual_attendance_files = []

            if show_debug_logs:
                st.info("ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€çµ‚ç¢ºèª:")
            
            import unicodedata
            normalized_attendance = unicodedata.normalize('NFC', attendance_filename).casefold() if attendance_filename else None
            
            for csv_file in csv_files:
                file_path = os.path.join(indir, csv_file)
                file_size = os.path.getsize(file_path)
                if show_debug_logs:
                    st.write(f"â€¢ {csv_file} ({file_size} bytes)")
                
                # ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ•ã‚¡ã‚¤ãƒ«ã¨å‹¤æ€ å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†é¡
                normalized_csv = unicodedata.normalize('NFC', csv_file).casefold()
                if normalized_attendance and normalized_csv == normalized_attendance:
                    actual_attendance_files.append(csv_file)
                else:
                    actual_service_files.append(csv_file)
            
            # ã‚µãƒ¼ãƒ“ã‚¹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
            service_data_list = []
            for service_file in actual_service_files:
                try:
                    file_path = os.path.join(indir, service_file)
                    # è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
                    df = None
                    for encoding in ['utf-8-sig', 'cp932', 'utf-8', 'shift_jis']:
                        try:
                            df = pd.read_csv(file_path, encoding=encoding)
                            break
                        except UnicodeDecodeError:
                            continue
                    
                    if df is not None:
                        service_data_list.append(df)
                        
                except Exception as e:
                    st.error(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {service_file}")
            
            st.session_state.service_data_list = service_data_list
            
            if len(actual_service_files) == 0:
                st.error("ã‚µãƒ¼ãƒ“ã‚¹å®Ÿæ…‹CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                st.stop()
            
            if len(actual_attendance_files) == 0:
                st.error("å‹¤æ€ å±¥æ­´CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                st.stop()

            # ã‚³ãƒãƒ³ãƒ‰çµ„ã¿ç«‹ã¦
            cmd = [sys.executable, src_target, "--input", indir, "--identical-prefer", identical_prefer, "--alt-delim", alt_delim]
            # å‹¤æ€ å±¥æ­´CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
            cmd += ["--attendance-file", attendance_filename]
            if use_schedule_when_missing:
                cmd.append("--use-schedule-when-missing")
            if not generate_diagnostics:
                cmd.append("--no-diagnostics")
            # åˆ—åã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆæ—¢å®šã¨é•ã†å ´åˆã®ã¿æ¸¡ã™ï¼‰
            if service_staff_col and service_staff_col != "æ‹…å½“æ‰€å“¡":
                cmd += ["--service-staff-col", service_staff_col]
            if att_name_col and att_name_col != "åå‰":
                cmd += ["--att-name-col", att_name_col]

            # å®Ÿè¡Œ
            proc = subprocess.run(cmd, capture_output=True, text=True)
            if proc.returncode != 0:
                st.error("å‡¦ç†ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                with st.expander("ã‚¨ãƒ©ãƒ¼è©³ç´°", expanded=True):
                    if proc.stderr:
                        st.code(proc.stderr)
                    if proc.stdout:
                        st.code(proc.stdout)
                st.stop()

            # å‡ºåŠ›ï¼ˆresult_*.csv ã¨ diagnosticsï¼‰ã‚’åé›†
            result_paths = []
            diagnostic_paths = []
            for root, dirs, files in os.walk(indir):
                for fn in files:
                    if fn.startswith("result_") and fn.endswith(".csv"):
                        result_paths.append(os.path.join(root, fn))
            diag_dir = os.path.join(indir, "diagnostics")
            if os.path.isdir(diag_dir):
                for fn in os.listdir(diag_dir):
                    diagnostic_paths.append(os.path.join(diag_dir, fn))

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«çµæœã‚’ä¿å­˜
            st.session_state.result_paths = result_paths
            st.session_state.diagnostic_paths = diagnostic_paths
            st.session_state.workdir = workdir
            st.session_state.processing_complete = True
            
            if result_paths:
                st.session_state.summary_df = collect_summary(result_paths)
            else:
                st.session_state.summary_df = None
            
            st.success("å‡¦ç†å®Œäº†")

# ã‚¿ãƒ–2: â‘¡ã‚¨ãƒ©ãƒ¼ç¢ºèªï¼ˆå…ƒã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚¿ãƒ–ï¼‰
with tab2:
    st.header("â‘¡ã‚¨ãƒ©ãƒ¼ç¢ºèª")
    
    if st.session_state.processing_complete and st.session_state.result_paths:
        try:
            grid_df = prepare_grid_data(st.session_state.result_paths)
            
            if not grid_df.empty:
                col_filter1, col_filter2 = st.columns(2)
                
                with col_filter1:
                    error_options = ["ã™ã¹ã¦", "ã‚¨ãƒ©ãƒ¼ã®ã¿", "æ­£å¸¸ã®ã¿"]
                    st.session_state.setdefault("error_filter", "ã‚¨ãƒ©ãƒ¼ã®ã¿")
                    default_error_index = error_options.index(st.session_state.error_filter) if st.session_state.error_filter in error_options else error_options.index("ã‚¨ãƒ©ãƒ¼ã®ã¿")
                    error_filter = st.selectbox("ã‚¨ãƒ©ãƒ¼", error_options, index=default_error_index, key="error_filter")
                
                with col_filter2:
                    category_filter = st.selectbox("ã‚«ãƒ†ã‚´ãƒª", ["ã™ã¹ã¦"] + [cat for cat in grid_df['ã‚«ãƒ†ã‚´ãƒª'].unique() if pd.notna(cat) and cat != ''], key="category_filter")
                
                col_staff, col_user = st.columns(2)
                
                with col_staff:
                    available_staff = [staff for staff in grid_df['æ‹…å½“æ‰€å“¡'].dropna().unique() if staff != '']
                    selected_staff = st.multiselect("æ‹…å½“æ‰€å“¡", sorted(available_staff), key="staff_filter_main")
                
                with col_user:
                    available_users = [user for user in grid_df['åˆ©ç”¨è€…å'].dropna().unique() if user != '']
                    selected_users = st.multiselect("åˆ©ç”¨è€…", sorted(available_users), key="user_filter_main")
                
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†
                filtered_df = grid_df.copy()
                
                if error_filter == "ã‚¨ãƒ©ãƒ¼ã®ã¿":
                    filtered_df = filtered_df[filtered_df['ã‚¨ãƒ©ãƒ¼'] == 'â—¯']
                elif error_filter == "æ­£å¸¸ã®ã¿":
                    filtered_df = filtered_df[filtered_df['ã‚¨ãƒ©ãƒ¼'] != 'â—¯']
                
                if category_filter != "ã™ã¹ã¦":
                    filtered_df = filtered_df[filtered_df['ã‚«ãƒ†ã‚´ãƒª'] == category_filter]
                
                if selected_staff:
                    filtered_df = filtered_df[filtered_df['æ‹…å½“æ‰€å“¡'].isin(selected_staff)]
                
                if selected_users:
                    filtered_df = filtered_df[filtered_df['åˆ©ç”¨è€…å'].isin(selected_users)]
                
                total_records = len(grid_df)
                error_records = len(grid_df[grid_df['ã‚¨ãƒ©ãƒ¼'] == 'â—¯'])
                filtered_records = len(filtered_df)
                
                col_stat1, col_stat2, col_stat3 = st.columns(3)
                with col_stat1:
                    st.metric("ç·ä»¶æ•°", total_records)
                with col_stat2:
                    st.metric("ã‚¨ãƒ©ãƒ¼ä»¶æ•°", error_records)
                with col_stat3:
                    st.metric("è¡¨ç¤ºä»¶æ•°", filtered_records)
                
                # æ¡ä»¶ä»˜ãã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°ã‚’é©ç”¨ã—ãŸAgGridã¾ãŸã¯ã‚«ãƒ¼ãƒ‰ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤º
                if not filtered_df.empty:
                    view_mode = st.radio(
                        "è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰",
                        ("ã‚«ãƒ¼ãƒ‰", "ã‚°ãƒªãƒƒãƒ‰"),
                        horizontal=True,
                        key="view_mode_main"
                    )

                    if view_mode == "ã‚°ãƒªãƒƒãƒ‰":
                        gridOptions = create_styled_grid(filtered_df)
                        
                        # AgGridã‚’è¡¨ç¤º
                        grid_response = AgGrid(
                            filtered_df,
                            gridOptions=gridOptions,
                            data_return_mode=DataReturnMode.FILTERED_AND_SORTED,
                            update_mode=GridUpdateMode.SELECTION_CHANGED,
                            fit_columns_on_grid_load=False,
                            enable_enterprise_modules=False,
                            height=600,
                            width='100%',
                            reload_data=False,
                            allow_unsafe_jscode=True
                        )
                    else:
                        show_card_view(filtered_df)
                else:
                    st.info("ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                
            else:
                st.info("ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
    else:
        st.info("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")

# ã‚¿ãƒ–3: â‘¢å‹¤æ€ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå…ƒã®æœ€é©å‹¤æ€ ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›ã‚¿ãƒ–ï¼‰
with tab3:
    st.header("â‘¢å‹¤æ€ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    # ä¿®æ­£æ¸ˆã¿ã®é–¢æ•°ã‚’ä½¿ç”¨
    show_optimal_attendance_export()

# ã‚¿ãƒ–4: â‘£ã‚¨ãƒ©ãƒ¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå…ƒã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¿ãƒ–ï¼‰
with tab4:
    st.header("â‘£ã‚¨ãƒ©ãƒ¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    
    if st.session_state.processing_complete:
        # å€‹åˆ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if st.session_state.result_paths:
            st.subheader("çµæœCSV")
            st.markdown("")
            st.caption("å„äº‹æ¥­æ‰€ã”ã¨ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯çµæœCSVã‚’å€‹åˆ¥ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚")
            for p in sorted(st.session_state.result_paths):
                with open(p, "rb") as f:
                    st.download_button(
                        label=f"Download {os.path.basename(p)}",
                        data=f.read(),
                        file_name=os.path.basename(p),
                        mime="text/csv",
                    )
            st.write("")

        # è¨ºæ–­ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if st.session_state.diagnostic_paths and generate_diagnostics:
            st.subheader("è¨ºæ–­CSV")
            st.markdown("")
            st.caption("ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ç”Ÿæˆã—ãŸè¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆCSVã‚’ç¢ºèªã§ãã¾ã™ã€‚")
            for p in sorted(st.session_state.diagnostic_paths):
                with open(p, "rb") as f:
                    st.download_button(
                        label=f"Download {os.path.basename(p)}",
                        data=f.read(),
                        file_name=os.path.basename(p),
                        mime="text/csv",
                    )
            st.write("")

        # ã¾ã¨ã‚ã¦ZIP
        if st.session_state.result_paths:
            st.subheader("ä¸€æ‹¬ZIP")
            st.markdown("")
            st.caption("çµæœCSVã¨è¨ºæ–­CSVã‚’ã¾ã¨ã‚ã¦å–å¾—ã—ãŸã„å ´åˆã¯ã“ã¡ã‚‰ã‚’ã”åˆ©ç”¨ãã ã•ã„ã€‚")
            buf = io.BytesIO()
            with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as zf:
                for p in st.session_state.result_paths:
                    zf.write(p, arcname=os.path.basename(p))
                if generate_diagnostics and st.session_state.diagnostic_paths:
                    for p in st.session_state.diagnostic_paths:
                        zf.write(p, arcname=os.path.join("diagnostics", os.path.basename(p)))
            buf.seek(0)
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            st.download_button(
                label="ZIPä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=buf,
                file_name=f"results_{ts}.zip",
                mime="application/zip",
            )

    else:
        st.info("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
